{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Oper8","text":"<p>Oper8 is a framework for writing kubernetes operators in python. It implements many common patterns used by large cloud applications that are reusable across many operator design patterns (GitHub).</p> <p>For API details, please refer to the API References.</p>"},{"location":"API%20References/","title":"API References","text":"<p>Package exports</p>"},{"location":"API%20References/#oper8.__main__","title":"<code>__main__</code>","text":"<p>The main module provides the executable entrypoint for oper8</p>"},{"location":"API%20References/#oper8.__main__.add_command","title":"<code>add_command(subparsers, cmd)</code>","text":"<p>Add the subparser and set up the default fun call</p> Source code in <code>oper8/__main__.py</code> <pre><code>def add_command(\n    subparsers: argparse._SubParsersAction,\n    cmd: CmdBase,\n) -&gt; Tuple[argparse.ArgumentParser, Dict[str, str]]:\n    \"\"\"Add the subparser and set up the default fun call\"\"\"\n    parser = cmd.add_subparser(subparsers)\n    parser.set_defaults(func=cmd.cmd)\n    library_args = parser.add_argument_group(\"Library Configuration\")\n    library_config_setters = add_library_config_args(library_args)\n    return parser, library_config_setters\n</code></pre>"},{"location":"API%20References/#oper8.__main__.add_library_config_args","title":"<code>add_library_config_args(parser, config_obj=None, path=None)</code>","text":"<p>Automatically add args for all elements of the library config</p> Source code in <code>oper8/__main__.py</code> <pre><code>def add_library_config_args(parser, config_obj=None, path=None):\n    \"\"\"Automatically add args for all elements of the library config\"\"\"\n    path = path or []\n    setters = {}\n    config_obj = config_obj or config\n    for key, val in config_obj.items():\n        sub_path = path + [key]\n\n        # If this is a nested arg, recurse\n        if isinstance(val, aconfig.AttributeAccessDict):\n            sub_setters = add_library_config_args(parser, config_obj=val, path=sub_path)\n            for dest_name, nested_path in sub_setters.items():\n                setters[dest_name] = nested_path\n\n        # Otherwise, add an argument explicitly\n        else:\n            arg_name = \".\".join(sub_path)\n            dest_name = \"_\".join(sub_path)\n            kwargs = {\n                \"default\": val,\n                \"dest\": dest_name,\n                \"help\": f\"Library config override for {arg_name} (see oper8.config)\",\n            }\n            if isinstance(val, list):\n                kwargs[\"nargs\"] = \"*\"\n            elif isinstance(val, bool):\n                kwargs[\"action\"] = \"store_true\"\n            else:\n                type_name = None\n                if val is not None:\n                    type_name = type(val)\n                kwargs[\"type\"] = type_name\n\n            if (\n                f\"--{arg_name}\"\n                not in parser._option_string_actions  # pylint: disable=protected-access\n            ):\n                parser.add_argument(f\"--{arg_name}\", **kwargs)\n                setters[dest_name] = sub_path\n    return setters\n</code></pre>"},{"location":"API%20References/#oper8.__main__.main","title":"<code>main()</code>","text":"<p>The main module provides the executable entrypoint for oper8</p> Source code in <code>oper8/__main__.py</code> <pre><code>def main():\n    \"\"\"The main module provides the executable entrypoint for oper8\"\"\"\n    parser = argparse.ArgumentParser(description=__doc__)\n\n    # Add the subcommands\n    subparsers = parser.add_subparsers(help=\"Available commands\", dest=\"command\")\n    run_operator_cmd = RunOperatorCmd()\n    run_operator_parser, library_config_setters = add_command(\n        subparsers, run_operator_cmd\n    )\n    run_health_check_cmd = CheckHeartbeatCmd()\n    add_command(subparsers, run_health_check_cmd)\n    setup_vcs_cmd = SetupVCSCmd()\n    add_command(subparsers, setup_vcs_cmd)\n\n    # Use a preliminary parser to check for the presence of a command and fall\n    # back to the default command if not found\n    check_parser = argparse.ArgumentParser(add_help=False)\n    check_parser.add_argument(\"command\", nargs=\"?\")\n    check_args, _ = check_parser.parse_known_args()\n    if check_args.command not in subparsers.choices:\n        args = run_operator_parser.parse_args()\n    else:\n        args = parser.parse_args()\n\n    # Provide overrides to the library configs\n    update_library_config(args, library_config_setters)\n\n    # Reconfigure logging\n    alog.configure(\n        default_level=config.log_level,\n        filters=config.log_filters,\n        formatter=Oper8JsonFormatter() if config.log_json else \"pretty\",\n        thread_id=config.log_thread_id,\n    )\n\n    # Run the command's function\n    args.func(args)\n</code></pre>"},{"location":"API%20References/#oper8.__main__.update_library_config","title":"<code>update_library_config(args, setters)</code>","text":"<p>Update the library config values based on the parsed arguments</p> Source code in <code>oper8/__main__.py</code> <pre><code>def update_library_config(args, setters):\n    \"\"\"Update the library config values based on the parsed arguments\"\"\"\n    for dest_name, config_path in setters.items():\n        config_obj = library_config\n        while len(config_path) &gt; 1:\n            config_obj = config_obj[config_path[0]]\n            config_path = config_path[1:]\n        config_obj[config_path[0]] = getattr(args, dest_name)\n</code></pre>"},{"location":"API%20References/#oper8.cmd","title":"<code>cmd</code>","text":"<p>This module holds all of the command classes for oper8's main entrypoint</p>"},{"location":"API%20References/#oper8.cmd.base","title":"<code>base</code>","text":"<p>Base class for all oper8 commands</p>"},{"location":"API%20References/#oper8.cmd.base.CmdBase","title":"<code>CmdBase</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>oper8/cmd/base.py</code> <pre><code>class CmdBase(abc.ABC):\n    __doc__ = __doc__\n\n    @abc.abstractmethod\n    def add_subparser(\n        self,\n        subparsers: argparse._SubParsersAction,\n    ) -&gt; argparse.ArgumentParser:\n        \"\"\"Add this command's argument parser subcommand\n\n        Args:\n            subparsers (argparse._SubParsersAction): The subparser section for\n                the central main parser\n\n        Returns:\n            subparser (argparse.ArgumentParser): The configured parser for this\n                command\n        \"\"\"\n\n    @abc.abstractmethod\n    def cmd(self, args: argparse.Namespace):\n        \"\"\"Execute the command with the parsed arguments\n\n        Args:\n            args (argparse.Namespace): The parsed command line arguments\n        \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.cmd.base.CmdBase.add_subparser","title":"<code>add_subparser(subparsers)</code>  <code>abstractmethod</code>","text":"<p>Add this command's argument parser subcommand</p> <p>Parameters:</p> Name Type Description Default <code>subparsers</code> <code>_SubParsersAction</code> <p>The subparser section for the central main parser</p> required <p>Returns:</p> Name Type Description <code>subparser</code> <code>ArgumentParser</code> <p>The configured parser for this command</p> Source code in <code>oper8/cmd/base.py</code> <pre><code>@abc.abstractmethod\ndef add_subparser(\n    self,\n    subparsers: argparse._SubParsersAction,\n) -&gt; argparse.ArgumentParser:\n    \"\"\"Add this command's argument parser subcommand\n\n    Args:\n        subparsers (argparse._SubParsersAction): The subparser section for\n            the central main parser\n\n    Returns:\n        subparser (argparse.ArgumentParser): The configured parser for this\n            command\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.cmd.base.CmdBase.cmd","title":"<code>cmd(args)</code>  <code>abstractmethod</code>","text":"<p>Execute the command with the parsed arguments</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>The parsed command line arguments</p> required Source code in <code>oper8/cmd/base.py</code> <pre><code>@abc.abstractmethod\ndef cmd(self, args: argparse.Namespace):\n    \"\"\"Execute the command with the parsed arguments\n\n    Args:\n        args (argparse.Namespace): The parsed command line arguments\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.cmd.check_heartbeat","title":"<code>check_heartbeat</code>","text":"<p>This is the main entrypoint command for running the operator</p>"},{"location":"API%20References/#oper8.cmd.check_heartbeat.CheckHeartbeatCmd","title":"<code>CheckHeartbeatCmd</code>","text":"<p>               Bases: <code>CmdBase</code></p> Source code in <code>oper8/cmd/check_heartbeat.py</code> <pre><code>class CheckHeartbeatCmd(CmdBase):\n    __doc__ = __doc__\n\n    ## Interface ##\n\n    def add_subparser(\n        self,\n        subparsers: argparse._SubParsersAction,\n    ) -&gt; argparse.ArgumentParser:\n        parser = subparsers.add_parser(\"check-heartbeat\", help=__doc__)\n        runtime_args = parser.add_argument_group(\"Check Heartbeat Configuration\")\n        runtime_args.add_argument(\n            \"--delta\",\n            \"-d\",\n            required=True,\n            type=int,\n            help=\"Max time allowed since last check\",\n        )\n        runtime_args.add_argument(\n            \"--file\",\n            \"-f\",\n            default=config.python_watch_manager.heartbeat_file,\n            help=\"Location of health check file. Defaults to config based.\",\n        )\n        return parser\n\n    def cmd(self, args: argparse.Namespace):\n        \"\"\"Run command to validate a health check file\"\"\"\n\n        # Validate args\n        assert args.delta is not None\n        assert args.file is not None\n\n        # Ensure file exists\n        file_path = Path(args.file)\n        if not file_path.exists():\n            log.error(f\"Health Check failed: {file_path} does not exist\")\n            raise FileNotFoundError()\n\n        # Read and the most recent time from the health check\n        last_log_time = file_path.read_text().strip()\n        last_time = datetime.strptime(last_log_time, HeartbeatThread._DATE_FORMAT)\n\n        if last_time + timedelta(seconds=args.delta) &lt; datetime.now():\n            msg = f\"Health Check failed: {last_log_time} is to old\"\n            log.error(msg)\n            raise KeyError(msg)\n</code></pre>"},{"location":"API%20References/#oper8.cmd.check_heartbeat.CheckHeartbeatCmd.cmd","title":"<code>cmd(args)</code>","text":"<p>Run command to validate a health check file</p> Source code in <code>oper8/cmd/check_heartbeat.py</code> <pre><code>def cmd(self, args: argparse.Namespace):\n    \"\"\"Run command to validate a health check file\"\"\"\n\n    # Validate args\n    assert args.delta is not None\n    assert args.file is not None\n\n    # Ensure file exists\n    file_path = Path(args.file)\n    if not file_path.exists():\n        log.error(f\"Health Check failed: {file_path} does not exist\")\n        raise FileNotFoundError()\n\n    # Read and the most recent time from the health check\n    last_log_time = file_path.read_text().strip()\n    last_time = datetime.strptime(last_log_time, HeartbeatThread._DATE_FORMAT)\n\n    if last_time + timedelta(seconds=args.delta) &lt; datetime.now():\n        msg = f\"Health Check failed: {last_log_time} is to old\"\n        log.error(msg)\n        raise KeyError(msg)\n</code></pre>"},{"location":"API%20References/#oper8.cmd.run_operator_cmd","title":"<code>run_operator_cmd</code>","text":"<p>This is the main entrypoint command for running the operator</p>"},{"location":"API%20References/#oper8.cmd.run_operator_cmd.RunOperatorCmd","title":"<code>RunOperatorCmd</code>","text":"<p>               Bases: <code>CmdBase</code></p> Source code in <code>oper8/cmd/run_operator_cmd.py</code> <pre><code>class RunOperatorCmd(CmdBase):\n    __doc__ = __doc__\n\n    ## Interface ##\n\n    def add_subparser(\n        self,\n        subparsers: argparse._SubParsersAction,\n    ) -&gt; argparse.ArgumentParser:\n        parser = subparsers.add_parser(\"run\", help=__doc__)\n        runtime_args = parser.add_argument_group(\"Runtime Configuration\")\n        runtime_args.add_argument(\n            \"--module_name\",\n            \"-m\",\n            required=True,\n            help=\"The module to import that holds the operator code\",\n        )\n        runtime_args.add_argument(\n            \"--cr\",\n            \"-c\",\n            default=None,\n            help=\"(dry run) A CR manifest yaml to apply directly \",\n        )\n        runtime_args.add_argument(\n            \"--resource_dir\",\n            \"-r\",\n            default=None,\n            help=\"(dry run) Path to a directory of yaml files that should exist in the cluster\",\n        )\n        return parser\n\n    def cmd(self, args: argparse.Namespace):\n        # Validate args\n        assert args.cr is None or (\n            config.dry_run and os.path.isfile(args.cr)\n        ), \"Can only specify --cr with dry run and it must point to a valid file\"\n        assert args.resource_dir is None or (\n            config.dry_run and os.path.isdir(args.resource_dir)\n        ), \"Can only specify --resource_dir with dry run and it must point to a valid directory\"\n\n        # Find all controllers in the operator library\n        controller_types = self._get_controller_types(\n            args.module_name, args.controller_name\n        )\n\n        # Parse pre-populated resources if needed\n        resources = self._parse_resource_dir(args.resource_dir)\n\n        # Create the watch managers\n        deploy_manager = self._setup_watches(controller_types, resources)\n\n        # Register the signal handler to stop the watches\n        def do_stop(*_, **__):  # pragma: no cover\n            watch_manager.stop_all()\n\n        signal.signal(signal.SIGINT, do_stop)\n\n        # Run the watch manager\n        log.info(\"Starting Watches\")\n        watch_manager.start_all()\n\n        # If given, apply the CR directly\n        if args.cr:\n            log.info(\"Applying CR [%s]\", args.cr)\n            with open(args.cr, encoding=\"utf-8\") as handle:\n                cr_manifest = yaml.safe_load(handle)\n                cr_manifest.setdefault(\"metadata\", {}).setdefault(\n                    \"namespace\", \"default\"\n                )\n                log.debug3(cr_manifest)\n                deploy_manager.deploy([cr_manifest])\n\n        # All done!\n        log.info(\"SHUTTING DOWN\")\n\n    ## Impl ##\n\n    @staticmethod\n    def _is_controller_type(attr_val: str):\n        \"\"\"Determine if a given attribute value is a controller type\"\"\"\n        return (\n            isinstance(attr_val, type)\n            and issubclass(attr_val, Controller)\n            and attr_val is not Controller\n        )\n\n    @classmethod\n    def _get_controller_types(cls, module_name: str, controller_name=\"\"):\n        \"\"\"Import the operator library and either extract all Controllers,\n        or just extract the provided Controller\"\"\"\n        module = importlib.import_module(module_name)\n        log.debug4(dir(module))\n        controller_types = []\n\n        if controller_name:\n            # Confirm that the class exists and that it is a controller type\n            try:\n                controller_attr_val = getattr(module, controller_name)\n                is_valid_controller = cls._is_controller_type(controller_attr_val)\n            except AttributeError:\n                is_valid_controller = False\n\n            if is_valid_controller:\n                log.debug3(\"Provided controller, %s, is valid\", controller_name)\n                controller_types.append(controller_attr_val)\n            else:\n                raise AttributeError(\n                    f\"Provided controller, {controller_name}, is invalid\"\n                )\n        else:\n            log.debug3(\"Searching for all controllers...\")\n            for attr in dir(module):\n                attr_val = getattr(module, attr)\n                if cls._is_controller_type(attr_val):\n                    log.debug2(\"Found Controller: %s\", attr_val)\n                    controller_types.append(attr_val)\n\n        assert controller_types, f\"No Controllers found in [{module_name}]\"\n        return controller_types\n\n    @staticmethod\n    def _parse_resource_dir(resource_dir: Optional[str]):\n        \"\"\"If given, this will parse all yaml files found in the given directory\"\"\"\n        all_resources = []\n        if resource_dir is not None:\n            for fname in os.listdir(resource_dir):\n                if fname.endswith(\".yaml\") or fname.endswith(\".yml\"):\n                    resource_path = os.path.join(resource_dir, fname)\n                    log.debug3(\"Reading resource file [%s]\", resource_path)\n                    with open(resource_path, encoding=\"utf-8\") as handle:\n                        all_resources.extend(yaml.safe_load_all(handle))\n        return all_resources\n\n    @staticmethod\n    def _setup_watches(\n        controller_types: List[Type[Controller]],\n        resources: List[dict],\n    ) -&gt; Optional[DryRunDeployManager]:\n        \"\"\"Set up watches for all controllers. If in dry run mode, the\n        DryRunDeployManager will be returned.\n        \"\"\"\n        deploy_manager = None\n        extra_kwargs = {}\n        if config.dry_run:\n            log.info(\"Running DRY RUN\")\n            deploy_manager = DryRunDeployManager(resources=resources)\n            wm_type = watch_manager.DryRunWatchManager\n            extra_kwargs[\"deploy_manager\"] = deploy_manager\n        elif config.watch_manager == \"ansible\":  # pragma: no cover\n            log.info(\"Running Ansible Operator\")\n            wm_type = watch_manager.AnsibleWatchManager\n        elif config.watch_manager == \"python\":  # pragma: no cover\n            log.info(\"Running Python Operator\")\n            wm_type = watch_manager.PythonWatchManager\n        else:\n            raise ConfigError(f\"Unknown watch manager {config.watch_manager}\")\n\n        for controller_type in controller_types:\n            log.debug(\"Registering watch for %s\", controller_type)\n            wm_type(controller_type=controller_type, **extra_kwargs)\n        return deploy_manager\n</code></pre>"},{"location":"API%20References/#oper8.cmd.setup_vcs_cmd","title":"<code>setup_vcs_cmd</code>","text":"<p>CLI command for setting up a VCS version repo</p>"},{"location":"API%20References/#oper8.cmd.setup_vcs_cmd.SetupVCSCmd","title":"<code>SetupVCSCmd</code>","text":"<p>               Bases: <code>CmdBase</code></p> Source code in <code>oper8/cmd/setup_vcs_cmd.py</code> <pre><code>class SetupVCSCmd(CmdBase):\n    __doc__ = __doc__\n\n    def add_subparser(\n        self,\n        subparsers: argparse._SubParsersAction,\n    ) -&gt; argparse.ArgumentParser:\n        \"\"\"Add the subparser for this command\"\"\"\n        parser = subparsers.add_parser(\n            \"setup-vcs\",\n            help=\"Initialize a clean git repo to use with VCS versioning\",\n        )\n        command_args = parser.add_argument_group(\"Command Arguments\")\n        command_args.add_argument(\n            \"--source\",\n            \"-s\",\n            required=True,\n            help=\"Source repo to seed the clean git history\",\n        )\n        command_args.add_argument(\n            \"--destination\",\n            \"-d\",\n            default=DEFAULT_DEST,\n            help=\"Destination directory in which to place the clean git history\",\n        )\n        command_args.add_argument(\n            \"--branch-expr\",\n            \"-b\",\n            nargs=\"*\",\n            default=None,\n            help=\"Regular expression(s) to use to identify branches\",\n        )\n        command_args.add_argument(\n            \"--tag-expr\",\n            \"-te\",\n            nargs=\"*\",\n            default=DEFAULT_TAG_EXPR,\n            help=\"Regular expression(s) to use to identify tags\",\n        )\n        command_args.add_argument(\n            \"--force\",\n            \"-f\",\n            action=\"store_true\",\n            default=False,\n            help=\"Force overwrite existing destination\",\n        )\n        return parser\n\n    def cmd(self, args: argparse.Namespace):\n        setup_vcs(\n            source=args.source,\n            destination=args.destination,\n            branch_expr=args.branch_expr,\n            tag_expr=args.tag_expr,\n            force=args.force,\n        )\n</code></pre>"},{"location":"API%20References/#oper8.cmd.setup_vcs_cmd.SetupVCSCmd.add_subparser","title":"<code>add_subparser(subparsers)</code>","text":"<p>Add the subparser for this command</p> Source code in <code>oper8/cmd/setup_vcs_cmd.py</code> <pre><code>def add_subparser(\n    self,\n    subparsers: argparse._SubParsersAction,\n) -&gt; argparse.ArgumentParser:\n    \"\"\"Add the subparser for this command\"\"\"\n    parser = subparsers.add_parser(\n        \"setup-vcs\",\n        help=\"Initialize a clean git repo to use with VCS versioning\",\n    )\n    command_args = parser.add_argument_group(\"Command Arguments\")\n    command_args.add_argument(\n        \"--source\",\n        \"-s\",\n        required=True,\n        help=\"Source repo to seed the clean git history\",\n    )\n    command_args.add_argument(\n        \"--destination\",\n        \"-d\",\n        default=DEFAULT_DEST,\n        help=\"Destination directory in which to place the clean git history\",\n    )\n    command_args.add_argument(\n        \"--branch-expr\",\n        \"-b\",\n        nargs=\"*\",\n        default=None,\n        help=\"Regular expression(s) to use to identify branches\",\n    )\n    command_args.add_argument(\n        \"--tag-expr\",\n        \"-te\",\n        nargs=\"*\",\n        default=DEFAULT_TAG_EXPR,\n        help=\"Regular expression(s) to use to identify tags\",\n    )\n    command_args.add_argument(\n        \"--force\",\n        \"-f\",\n        action=\"store_true\",\n        default=False,\n        help=\"Force overwrite existing destination\",\n    )\n    return parser\n</code></pre>"},{"location":"API%20References/#oper8.component","title":"<code>component</code>","text":"<p>Component base class for building larger abstractions off of</p>"},{"location":"API%20References/#oper8.component.Component","title":"<code>Component</code>","text":"<p>               Bases: <code>Node</code>, <code>ABC</code></p> <p>This file defines the top-level interface for a \"Component\" in the deployment ecosystem. Each Component will ultimately resolve to a Node in the deployment execution graph which can be atomically rendered, deployed, verified, and if needed reverted.</p> Source code in <code>oper8/component.py</code> <pre><code>class Component(Node, abc.ABC):\n    \"\"\"\n    This file defines the top-level interface for a \"Component\" in the\n    deployment ecosystem. Each Component will ultimately resolve to a Node in\n    the deployment execution graph which can be atomically rendered, deployed,\n    verified, and if needed reverted.\n    \"\"\"\n\n    @abstractclassproperty\n    def name(self):\n        \"\"\"All Components must implement a name class attribute\"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        disabled: bool = False,\n    ):\n        \"\"\"Construct with the session for this deployment\n\n        Args:\n            session:  Session\n                The session that this component will belong to\n            disabled:  bool\n                Whether or not this component is disabled\n        \"\"\"\n        # Ensure that the name property is defined by accessing it and that\n        # namespace is inherited from session.\n        self.name  # noqa: B018\n        self.session_namespace = session.namespace\n        self.disabled = disabled\n\n        # Initialize Node with name\n        super().__init__(self.name)\n\n        # Register with the session\n        # NOTE: This is done before the parent initialization so duplicates can\n        #   be caught by the session with a nice error rather than Graph\n        log.debug2(\"[%s] Auto-registering %s\", session.id, self)\n        session.add_component(self)\n\n        # Initialize the Graph that'll control component rendering\n        self.graph = Graph()\n\n        # The list of all managed objects owned by this component\n        self._managed_objects = None\n\n    def __str__(self):\n        return f\"Component({self.name})\"\n\n    @property\n    def managed_objects(self) -&gt; List[ManagedObject]:\n        \"\"\"The list of managed objects that this Component currently knows\n        about. If called before rending, this will be an empty list, so it will\n        always be iterable.\n\n        Returns:\n            managed_objects:  List[ManagedObject]\n                The list of known managed objects\n        \"\"\"\n        return self._managed_objects or []\n\n    ## Base Class Interface ####################################################\n    #\n    # These methods MAY be implemented by children, but contain default\n    # implementations that are appropriate for simple cases.\n    #\n    # NOTE: We liberally use pylint disables here to make the base interface\n    #   clear to deriving classes.\n    ##\n\n    def build_chart(self, session: Session):  # pylint: disable=unused-argument\n        \"\"\"The build_chart function allows the derived class to add child Charts\n        lazily so that they can take advantage of post-initialization\n        information.\n\n        Args:\n            session:  Session\n                The current deploy session\n        \"\"\"\n\n    def verify(self, session):\n        \"\"\"The verify function will run any necessary testing and validation\n        that the component needs to ensure that rollout was successfully\n        completed.\n\n        Args:\n            session:  Session\n                The current reconciliation session\n\n        Returns:\n            success:  bool\n                True on successful deployment verification, False on failure\n                conditions\n        \"\"\"\n        return self._default_verify(session, is_subsystem=False)\n\n    @alog.logged_function(log.debug2)\n    @alog.timed_function(log.debug2)\n    def render_chart(self, session):\n        \"\"\"This will be invoked by the parent Application to build and render\n        the individual component's chart\n\n        Args:\n            session:  Session\n                The session for this reconciliation\n        \"\"\"\n\n        # Do the rendering\n        self.__render(session)\n\n        # If a working directory is configured, use it\n        if config.working_dir:\n            rendered_file = self.to_file(session)\n            log.debug(\"Rendered %s to %s\", self, rendered_file)\n\n    def update_object_definition(\n        self,\n        session: Session,  # pylint: disable=unused-argument\n        internal_name: str,  # pylint: disable=unused-argument\n        resource_definition: dict,\n    ):\n        \"\"\"Allow children to inject arbitrary object mutation logic for\n        individual managed objects\n\n        The base implementation simply returns the given definition as a\n        passthrough\n\n        Args:\n            session:  Session\n                The session for this reconciliation\n            internal_name:  str\n                The internal name of the object to update\n            resource_definition:  dict\n                The dict representation of the resource to modify\n\n        Returns:\n            resource_definition:  dict\n                The dict representation of the resource with any modifications\n                applied\n        \"\"\"\n        return resource_definition\n\n    @alog.logged_function(log.debug2)\n    @alog.timed_function(log.debug2)\n    def deploy(self, session):\n        \"\"\"Deploy the component\n\n        Args:\n            session:  Session\n                The current reconciliation session\n\n        Returns:\n            success:  bool\n                True on successful application of the kub state (not\n                programmatic verification), False otherwise\n        \"\"\"\n        assert (\n            self._managed_objects is not None\n        ), \"Cannot call deploy() before render_chart()\"\n\n        # Deploy all managed objects\n        for obj in self.managed_objects:\n            success, _ = session.deploy_manager.deploy(\n                resource_definitions=[obj.definition],\n                method=obj.deploy_method,\n            )\n            if not success:\n                log.warning(\"Failed to deploy [%s]\", self)\n                return False\n        return True\n\n    def disable(self, session):\n        \"\"\"Disable the component\n\n        Args:\n            session:  Session\n                The current reconciliation session\n\n        Returns:\n            success:  bool\n                True on successful application of the kub state (not\n                programmatic verification), False otherwise\n        \"\"\"\n        assert (\n            self._managed_objects is not None\n        ), \"Cannot call disable() before render_chart()\"\n\n        # Disable all managed objects\n        success, _ = session.deploy_manager.disable(\n            [obj.definition for obj in self._managed_objects]\n        )\n        if not success:\n            log.warning(\"Failed to disable [%s]\", self)\n            return False\n        return True\n\n    ## Resource Interface ####################################################\n    #\n    # These methods offer functionality that children can use to add resources to\n    # a components graph\n    ##\n\n    def add_resource(\n        self,\n        name: str,  # pylint: disable=redefined-builtin\n        obj: Any,\n        verify_function: Optional[RESOURCE_VERIFY_FUNCTION] = None,\n        deploy_method: Optional[DeployMethod] = DeployMethod.DEFAULT,\n    ) -&gt; Optional[\n        ResourceNode\n    ]:  # pylint: disable=unused-argument, redefined-builtin, invalid-name\n        \"\"\"The add_resource function allows the derived class to add resources\n        to this component to later be rendered\n\n        Args:\n            name:  str\n                The name of the resource in the Graph\n            obj: Any\n                An object or dict which can be manipulated into a dict\n                representation of the kubernetes resource\n        \"\"\"\n        # Sanitize object to enable native support for openapi objects\n        obj = sanitize_for_serialization(obj)\n\n        # Add namespace to obj if not present\n        obj.setdefault(\"metadata\", {}).setdefault(\"namespace\", self.session_namespace)\n\n        node = ResourceNode(name, obj, verify_function, deploy_method)\n        self.graph.add_node(node)\n        return node\n\n    def add_dependency(\n        self,\n        session: Session,\n        *components: \"Component\",\n        verify_function: Optional[COMPONENT_VERIFY_FUNCTION] = None,\n    ):\n        \"\"\"This add_dependency function sets up a dependency between this component\n        and a list of other components. To add a dependency between resources inside\n        this component use resource.add_dependency\n        Args:\n            session:  Session\n                The current resource session\n            *components:  Components\n                Any number of components to be added as a dependency\n            verify_function: Optional[verify_function]\n                An Optional callable function of the form `def verify(session) -&gt; bool:`\n                to use to verify that the dependency has been satisfied. This\n                will be used to block deployment of the component beyond\n                requiring that the upstream has been deployed successfully.\n        \"\"\"\n        for component in components:\n            session.add_component_dependency(self, component, verify_function)\n\n    ## Base Class Utilities ####################################################\n    #\n    # These methods offer shared functionality that children can (and should)\n    # use in their implementations\n    ##\n\n    @alog.logged_function(log.debug2)\n    def to_dict(self, session):\n        \"\"\"\n        Render the component and return it as a Dictionary, mainly useful for testing\n        :return: Dictionary of the rendered component\n        \"\"\"\n        self.__render(session)\n        return [obj.definition for obj in self.managed_objects]\n\n    def to_config(self, session):\n        \"\"\"\n        Render the component and return it as an AttrDict, mainly useful for testing\n        :return: AttrDict of the rendered component\n        \"\"\"\n\n        return [\n            aconfig.Config(obj, override_env_vars=False)\n            for obj in self.to_dict(session)\n        ]\n\n    def to_file(self, session):\n        \"\"\"\n        Render the component to disk and return the rendered file path\n        :return: str path to rendered file\n        \"\"\"\n        assert config.working_dir is not None, \"Config must have a working_dir set\"\n\n        # If disabled and not dumping disabled components, nothing to do\n        if self.disabled and not config.dump_disabled:\n            log.debug(\"Not dumping disabled component: %s\", self)\n            return None\n\n        # Get the in-memory representation\n        objects = self.to_dict(session)\n\n        # Get the output file name and make sure the directory structure exists\n        path_parts = [\n            config.working_dir,\n            \".\".join([session.api_version.replace(\"/\", \".\"), session.kind]).lower(),\n            session.name,\n        ]\n        if self.disabled:\n            path_parts.append(\"DISABLED\")\n        path_parts.append(self.name)\n        output_dir = os.path.join(*path_parts)\n        if not os.path.exists(output_dir):\n            log.debug2(\"Creating output dir: %s\", output_dir)\n            os.makedirs(output_dir)\n\n        # Serialize to a yaml file\n        instance_name = session.name\n        output_file = os.path.join(output_dir, f\"{instance_name}-{self.name}.k8s.yaml\")\n        log.debug2(\"Saving %s to %s\", self, output_file)\n        with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n            outfile.write(\"---\\n\" + yaml.safe_dump_all(objects))\n\n        return output_file\n\n    ## Base Class Implementation Details #######################################\n    #\n    # These methods provide shared functionality to the base class function\n    # implementations and should not be used directly by children\n    ##\n\n    @classmethod\n    def get_name(cls):  # pylint: disable=arguments-differ\n        \"\"\"Override get_name to support class attribute\"\"\"\n        return cls.name\n\n    def _default_verify(self, session, is_subsystem=False):\n        \"\"\"The verify function will run any necessary testing and validation\n        that the component needs to ensure that rollout was successfully\n        completed.\n\n        Args:\n            session:  Session\n                The current reconciliation session\n\n        Returns:\n            success:  bool\n                True on successful deployment verification, False on failure\n                conditions\n        \"\"\"\n        log.debug2(\"Using default verification for [%s]\", self)\n\n        # If this is in dry run mode, we skip verification since this relies on\n        # checking for changes in the cluster which won't ever happen\n        if config.dry_run:\n            log.debug2(\"No verification to perform in dry_run\")\n            return True\n\n        # Verify all managed resources\n        for resource in self.managed_objects:\n            log.debug2(\"Verifying [%s/%s]\", resource.kind, resource.name)\n            if not verify_resource(\n                kind=resource.kind,\n                name=resource.name,\n                api_version=resource.api_version,\n                session=session,\n                is_subsystem=is_subsystem,\n                namespace=resource.namespace,\n                verify_function=resource.verify_function,\n            ):\n                log.debug(\"[%s/%s] not verified\", resource.kind, resource.name)\n                return False\n        log.debug(\"All managed resources verified for [%s]\", self)\n        return True\n\n    @staticmethod\n    def _preserve_patch_annotation(session, internal_name, resource_definition):\n        \"\"\"This implementation helper checks the current state of the given\n        resource and patches the desired state to preserve any temporary patch\n        annotations found. This is done so that temporary patches can be applied\n        to subsystem CRs managed by a top-level controller.\n        \"\"\"\n\n        # Get the current state of the object\n        kind = resource_definition.get(\"kind\")\n        api_version = resource_definition.get(\"apiVersion\")\n        metadata = resource_definition.get(\"metadata\", {})\n        name = metadata.get(\"name\")\n        namespace = metadata.get(\"namespace\")\n        assert (\n            kind is not None and api_version is not None and name is not None\n        ), f\"Resource {internal_name} missing critical metadata!\"\n        success, content = session.get_object_current_state(\n            kind=kind, name=name, api_version=api_version, namespace=namespace\n        )\n        assert_cluster(\n            success,\n            f\"Failed to look for current state for [{kind}/{api_version}/{namespace}/{name}]\",\n        )\n\n        # Look for existing patch annotations\n        if content is not None:\n            content_meta = content.get(\"metadata\", {})\n            patch_anno = content_meta.get(\"annotations\", {}).get(\n                TEMPORARY_PATCHES_ANNOTATION_NAME\n            )\n\n            # If found, update the resource\n            if patch_anno:\n                resource_definition.setdefault(\"metadata\", {}).setdefault(\n                    \"annotations\", {}\n                )[TEMPORARY_PATCHES_ANNOTATION_NAME] = patch_anno\n\n            # Any time we have metadata changes, we need to include the\n            # resourceVersion. It can't hurt to do so, so we will just always do\n            # it here if found.\n            resource_version = content_meta.get(\"resourceVersion\")\n            if resource_version is not None:\n                resource_definition[\"metadata\"][\"resourceVersion\"] = resource_version\n\n            # Make sure any ownerReferences are persisted as well\n            owner_refs = content_meta.get(\"ownerReferences\")\n            if owner_refs:\n                resource_definition[\"metadata\"][\"ownerReferences\"] = owner_refs\n\n        return resource_definition\n\n    def __build_lazy_charts(self, session):\n        \"\"\"Delegate to the child implementation of build_chart for lazy chart\n        construction.\n        \"\"\"\n        self.build_chart(session)\n\n    @alog.logged_function(log.debug3)\n    def __render(self, session):\n        \"\"\"This is the primary implementation for rendering objects into\n        self.managed_objects\n        \"\"\"\n\n        # Short-circuit if already rendered\n        if self._managed_objects is not None:\n            log.debug2(\n                \"%s returning %d pre-rendered objects\", self, len(self._managed_objects)\n            )\n            return self.managed_objects\n\n        # Generate name and dict representation of objects\n        resource_list = self.__gather_resources(session)\n\n        # Iterate all ApiObject children in dependency order and perform the\n        # rendering, including patches and backend modifications.\n        self._managed_objects = []\n        for name, obj, verify_func, deploy_method in resource_list:\n            # Apply any patches to this object\n            log.debug2(\"Applying patches to managed object: %s\", name)\n            log.debug4(\"Before Patching: %s\", obj)\n            obj = apply_patches(name, obj, session.temporary_patches)\n\n            # Make sure any temporary patch annotations that exist already\n            # on this resource in the cluster are preserved\n            log.debug2(\"Checking for existing subsystem patches on: %s\", name)\n            obj = self._preserve_patch_annotation(session, name, obj)\n\n            # Add the internal name annotation if enabled\n            if config.internal_name_annotation:\n                log.debug2(\n                    \"Adding internal name annotation [%s: %s]\",\n                    INTERNAL_NAME_ANNOTATION_NAME,\n                    name,\n                )\n                obj.setdefault(\"metadata\", {}).setdefault(\"annotations\", {})[\n                    INTERNAL_NAME_ANNOTATION_NAME\n                ] = name\n\n            # Allow children to inject additional modification logic\n            log.debug4(\"Before Object Updates: %s\", obj)\n            obj = self.update_object_definition(session, name, obj)\n\n            # Add the resource to the set managed by the is component\n            managed_obj = ManagedObject(obj, verify_func, deploy_method)\n            log.debug2(\"Adding managed object: %s\", managed_obj)\n            log.debug4(\"Final Definition: %s\", obj)\n            self._managed_objects.append(managed_obj)\n\n        return self.managed_objects\n\n    def __gather_resources(\n        self, session\n    ) -&gt; List[Tuple[str, dict, Callable, DeployMethod]]:\n        \"\"\"This is a helper for __render which handles converting resource objects\n        into a list of dictionaries.\n        \"\"\"\n        # Perform lazy chart creation before finishing rendering\n        self.__build_lazy_charts(session)\n\n        # Determine the flattened set of ApiObject children.\n        log.debug2(\"%s populating managed_objects\", self)\n        topology = self.graph.topology()\n        log.debug3(\"%s topology has %d elements\", self, len(topology))\n        log.debug4([type(obj) for obj in topology])\n        children = [node for node in topology if isinstance(node, ResourceNode)]\n        log.debug2(\"%s found %d ResourceNode children\", self, len(children))\n\n        resource_list = []\n        for child in children:\n            # Construct the managed object with its internal name\n            child_name = \".\".join([self.name, child.get_name()])\n            resource_list.append(\n                (child_name, child.manifest, child.verify_function, child.deploy_method)\n            )\n\n        return resource_list\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.managed_objects","title":"<code>managed_objects</code>  <code>property</code>","text":"<p>The list of managed objects that this Component currently knows about. If called before rending, this will be an empty list, so it will always be iterable.</p> <p>Returns:</p> Name Type Description <code>managed_objects</code> <code>List[ManagedObject]</code> <p>List[ManagedObject] The list of known managed objects</p>"},{"location":"API%20References/#oper8.component.Component.__build_lazy_charts","title":"<code>__build_lazy_charts(session)</code>","text":"<p>Delegate to the child implementation of build_chart for lazy chart construction.</p> Source code in <code>oper8/component.py</code> <pre><code>def __build_lazy_charts(self, session):\n    \"\"\"Delegate to the child implementation of build_chart for lazy chart\n    construction.\n    \"\"\"\n    self.build_chart(session)\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.__gather_resources","title":"<code>__gather_resources(session)</code>","text":"<p>This is a helper for __render which handles converting resource objects into a list of dictionaries.</p> Source code in <code>oper8/component.py</code> <pre><code>def __gather_resources(\n    self, session\n) -&gt; List[Tuple[str, dict, Callable, DeployMethod]]:\n    \"\"\"This is a helper for __render which handles converting resource objects\n    into a list of dictionaries.\n    \"\"\"\n    # Perform lazy chart creation before finishing rendering\n    self.__build_lazy_charts(session)\n\n    # Determine the flattened set of ApiObject children.\n    log.debug2(\"%s populating managed_objects\", self)\n    topology = self.graph.topology()\n    log.debug3(\"%s topology has %d elements\", self, len(topology))\n    log.debug4([type(obj) for obj in topology])\n    children = [node for node in topology if isinstance(node, ResourceNode)]\n    log.debug2(\"%s found %d ResourceNode children\", self, len(children))\n\n    resource_list = []\n    for child in children:\n        # Construct the managed object with its internal name\n        child_name = \".\".join([self.name, child.get_name()])\n        resource_list.append(\n            (child_name, child.manifest, child.verify_function, child.deploy_method)\n        )\n\n    return resource_list\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.__init__","title":"<code>__init__(session, disabled=False)</code>","text":"<p>Construct with the session for this deployment</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The session that this component will belong to</p> required <code>disabled</code> <code>bool</code> <p>bool Whether or not this component is disabled</p> <code>False</code> Source code in <code>oper8/component.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    disabled: bool = False,\n):\n    \"\"\"Construct with the session for this deployment\n\n    Args:\n        session:  Session\n            The session that this component will belong to\n        disabled:  bool\n            Whether or not this component is disabled\n    \"\"\"\n    # Ensure that the name property is defined by accessing it and that\n    # namespace is inherited from session.\n    self.name  # noqa: B018\n    self.session_namespace = session.namespace\n    self.disabled = disabled\n\n    # Initialize Node with name\n    super().__init__(self.name)\n\n    # Register with the session\n    # NOTE: This is done before the parent initialization so duplicates can\n    #   be caught by the session with a nice error rather than Graph\n    log.debug2(\"[%s] Auto-registering %s\", session.id, self)\n    session.add_component(self)\n\n    # Initialize the Graph that'll control component rendering\n    self.graph = Graph()\n\n    # The list of all managed objects owned by this component\n    self._managed_objects = None\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.__render","title":"<code>__render(session)</code>","text":"<p>This is the primary implementation for rendering objects into self.managed_objects</p> Source code in <code>oper8/component.py</code> <pre><code>@alog.logged_function(log.debug3)\ndef __render(self, session):\n    \"\"\"This is the primary implementation for rendering objects into\n    self.managed_objects\n    \"\"\"\n\n    # Short-circuit if already rendered\n    if self._managed_objects is not None:\n        log.debug2(\n            \"%s returning %d pre-rendered objects\", self, len(self._managed_objects)\n        )\n        return self.managed_objects\n\n    # Generate name and dict representation of objects\n    resource_list = self.__gather_resources(session)\n\n    # Iterate all ApiObject children in dependency order and perform the\n    # rendering, including patches and backend modifications.\n    self._managed_objects = []\n    for name, obj, verify_func, deploy_method in resource_list:\n        # Apply any patches to this object\n        log.debug2(\"Applying patches to managed object: %s\", name)\n        log.debug4(\"Before Patching: %s\", obj)\n        obj = apply_patches(name, obj, session.temporary_patches)\n\n        # Make sure any temporary patch annotations that exist already\n        # on this resource in the cluster are preserved\n        log.debug2(\"Checking for existing subsystem patches on: %s\", name)\n        obj = self._preserve_patch_annotation(session, name, obj)\n\n        # Add the internal name annotation if enabled\n        if config.internal_name_annotation:\n            log.debug2(\n                \"Adding internal name annotation [%s: %s]\",\n                INTERNAL_NAME_ANNOTATION_NAME,\n                name,\n            )\n            obj.setdefault(\"metadata\", {}).setdefault(\"annotations\", {})[\n                INTERNAL_NAME_ANNOTATION_NAME\n            ] = name\n\n        # Allow children to inject additional modification logic\n        log.debug4(\"Before Object Updates: %s\", obj)\n        obj = self.update_object_definition(session, name, obj)\n\n        # Add the resource to the set managed by the is component\n        managed_obj = ManagedObject(obj, verify_func, deploy_method)\n        log.debug2(\"Adding managed object: %s\", managed_obj)\n        log.debug4(\"Final Definition: %s\", obj)\n        self._managed_objects.append(managed_obj)\n\n    return self.managed_objects\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.add_dependency","title":"<code>add_dependency(session, *components, verify_function=None)</code>","text":"<p>This add_dependency function sets up a dependency between this component and a list of other components. To add a dependency between resources inside this component use resource.add_dependency Args:     session:  Session         The current resource session     *components:  Components         Any number of components to be added as a dependency     verify_function: Optional[verify_function]         An Optional callable function of the form <code>def verify(session) -&gt; bool:</code>         to use to verify that the dependency has been satisfied. This         will be used to block deployment of the component beyond         requiring that the upstream has been deployed successfully.</p> Source code in <code>oper8/component.py</code> <pre><code>def add_dependency(\n    self,\n    session: Session,\n    *components: \"Component\",\n    verify_function: Optional[COMPONENT_VERIFY_FUNCTION] = None,\n):\n    \"\"\"This add_dependency function sets up a dependency between this component\n    and a list of other components. To add a dependency between resources inside\n    this component use resource.add_dependency\n    Args:\n        session:  Session\n            The current resource session\n        *components:  Components\n            Any number of components to be added as a dependency\n        verify_function: Optional[verify_function]\n            An Optional callable function of the form `def verify(session) -&gt; bool:`\n            to use to verify that the dependency has been satisfied. This\n            will be used to block deployment of the component beyond\n            requiring that the upstream has been deployed successfully.\n    \"\"\"\n    for component in components:\n        session.add_component_dependency(self, component, verify_function)\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.add_resource","title":"<code>add_resource(name, obj, verify_function=None, deploy_method=DeployMethod.DEFAULT)</code>","text":"<p>The add_resource function allows the derived class to add resources to this component to later be rendered</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>str The name of the resource in the Graph</p> required <code>obj</code> <code>Any</code> <p>Any An object or dict which can be manipulated into a dict representation of the kubernetes resource</p> required Source code in <code>oper8/component.py</code> <pre><code>def add_resource(\n    self,\n    name: str,  # pylint: disable=redefined-builtin\n    obj: Any,\n    verify_function: Optional[RESOURCE_VERIFY_FUNCTION] = None,\n    deploy_method: Optional[DeployMethod] = DeployMethod.DEFAULT,\n) -&gt; Optional[\n    ResourceNode\n]:  # pylint: disable=unused-argument, redefined-builtin, invalid-name\n    \"\"\"The add_resource function allows the derived class to add resources\n    to this component to later be rendered\n\n    Args:\n        name:  str\n            The name of the resource in the Graph\n        obj: Any\n            An object or dict which can be manipulated into a dict\n            representation of the kubernetes resource\n    \"\"\"\n    # Sanitize object to enable native support for openapi objects\n    obj = sanitize_for_serialization(obj)\n\n    # Add namespace to obj if not present\n    obj.setdefault(\"metadata\", {}).setdefault(\"namespace\", self.session_namespace)\n\n    node = ResourceNode(name, obj, verify_function, deploy_method)\n    self.graph.add_node(node)\n    return node\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.build_chart","title":"<code>build_chart(session)</code>","text":"<p>The build_chart function allows the derived class to add child Charts lazily so that they can take advantage of post-initialization information.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current deploy session</p> required Source code in <code>oper8/component.py</code> <pre><code>def build_chart(self, session: Session):  # pylint: disable=unused-argument\n    \"\"\"The build_chart function allows the derived class to add child Charts\n    lazily so that they can take advantage of post-initialization\n    information.\n\n    Args:\n        session:  Session\n            The current deploy session\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.deploy","title":"<code>deploy(session)</code>","text":"<p>Deploy the component</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <p>Session The current reconciliation session</p> required <p>Returns:</p> Name Type Description <code>success</code> <p>bool True on successful application of the kub state (not programmatic verification), False otherwise</p> Source code in <code>oper8/component.py</code> <pre><code>@alog.logged_function(log.debug2)\n@alog.timed_function(log.debug2)\ndef deploy(self, session):\n    \"\"\"Deploy the component\n\n    Args:\n        session:  Session\n            The current reconciliation session\n\n    Returns:\n        success:  bool\n            True on successful application of the kub state (not\n            programmatic verification), False otherwise\n    \"\"\"\n    assert (\n        self._managed_objects is not None\n    ), \"Cannot call deploy() before render_chart()\"\n\n    # Deploy all managed objects\n    for obj in self.managed_objects:\n        success, _ = session.deploy_manager.deploy(\n            resource_definitions=[obj.definition],\n            method=obj.deploy_method,\n        )\n        if not success:\n            log.warning(\"Failed to deploy [%s]\", self)\n            return False\n    return True\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.disable","title":"<code>disable(session)</code>","text":"<p>Disable the component</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <p>Session The current reconciliation session</p> required <p>Returns:</p> Name Type Description <code>success</code> <p>bool True on successful application of the kub state (not programmatic verification), False otherwise</p> Source code in <code>oper8/component.py</code> <pre><code>def disable(self, session):\n    \"\"\"Disable the component\n\n    Args:\n        session:  Session\n            The current reconciliation session\n\n    Returns:\n        success:  bool\n            True on successful application of the kub state (not\n            programmatic verification), False otherwise\n    \"\"\"\n    assert (\n        self._managed_objects is not None\n    ), \"Cannot call disable() before render_chart()\"\n\n    # Disable all managed objects\n    success, _ = session.deploy_manager.disable(\n        [obj.definition for obj in self._managed_objects]\n    )\n    if not success:\n        log.warning(\"Failed to disable [%s]\", self)\n        return False\n    return True\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.get_name","title":"<code>get_name()</code>  <code>classmethod</code>","text":"<p>Override get_name to support class attribute</p> Source code in <code>oper8/component.py</code> <pre><code>@classmethod\ndef get_name(cls):  # pylint: disable=arguments-differ\n    \"\"\"Override get_name to support class attribute\"\"\"\n    return cls.name\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.name","title":"<code>name()</code>","text":"<p>All Components must implement a name class attribute</p> Source code in <code>oper8/component.py</code> <pre><code>@abstractclassproperty\ndef name(self):\n    \"\"\"All Components must implement a name class attribute\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.render_chart","title":"<code>render_chart(session)</code>","text":"<p>This will be invoked by the parent Application to build and render the individual component's chart</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <p>Session The session for this reconciliation</p> required Source code in <code>oper8/component.py</code> <pre><code>@alog.logged_function(log.debug2)\n@alog.timed_function(log.debug2)\ndef render_chart(self, session):\n    \"\"\"This will be invoked by the parent Application to build and render\n    the individual component's chart\n\n    Args:\n        session:  Session\n            The session for this reconciliation\n    \"\"\"\n\n    # Do the rendering\n    self.__render(session)\n\n    # If a working directory is configured, use it\n    if config.working_dir:\n        rendered_file = self.to_file(session)\n        log.debug(\"Rendered %s to %s\", self, rendered_file)\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.to_config","title":"<code>to_config(session)</code>","text":"<p>Render the component and return it as an AttrDict, mainly useful for testing :return: AttrDict of the rendered component</p> Source code in <code>oper8/component.py</code> <pre><code>def to_config(self, session):\n    \"\"\"\n    Render the component and return it as an AttrDict, mainly useful for testing\n    :return: AttrDict of the rendered component\n    \"\"\"\n\n    return [\n        aconfig.Config(obj, override_env_vars=False)\n        for obj in self.to_dict(session)\n    ]\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.to_dict","title":"<code>to_dict(session)</code>","text":"<p>Render the component and return it as a Dictionary, mainly useful for testing :return: Dictionary of the rendered component</p> Source code in <code>oper8/component.py</code> <pre><code>@alog.logged_function(log.debug2)\ndef to_dict(self, session):\n    \"\"\"\n    Render the component and return it as a Dictionary, mainly useful for testing\n    :return: Dictionary of the rendered component\n    \"\"\"\n    self.__render(session)\n    return [obj.definition for obj in self.managed_objects]\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.to_file","title":"<code>to_file(session)</code>","text":"<p>Render the component to disk and return the rendered file path :return: str path to rendered file</p> Source code in <code>oper8/component.py</code> <pre><code>def to_file(self, session):\n    \"\"\"\n    Render the component to disk and return the rendered file path\n    :return: str path to rendered file\n    \"\"\"\n    assert config.working_dir is not None, \"Config must have a working_dir set\"\n\n    # If disabled and not dumping disabled components, nothing to do\n    if self.disabled and not config.dump_disabled:\n        log.debug(\"Not dumping disabled component: %s\", self)\n        return None\n\n    # Get the in-memory representation\n    objects = self.to_dict(session)\n\n    # Get the output file name and make sure the directory structure exists\n    path_parts = [\n        config.working_dir,\n        \".\".join([session.api_version.replace(\"/\", \".\"), session.kind]).lower(),\n        session.name,\n    ]\n    if self.disabled:\n        path_parts.append(\"DISABLED\")\n    path_parts.append(self.name)\n    output_dir = os.path.join(*path_parts)\n    if not os.path.exists(output_dir):\n        log.debug2(\"Creating output dir: %s\", output_dir)\n        os.makedirs(output_dir)\n\n    # Serialize to a yaml file\n    instance_name = session.name\n    output_file = os.path.join(output_dir, f\"{instance_name}-{self.name}.k8s.yaml\")\n    log.debug2(\"Saving %s to %s\", self, output_file)\n    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n        outfile.write(\"---\\n\" + yaml.safe_dump_all(objects))\n\n    return output_file\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.update_object_definition","title":"<code>update_object_definition(session, internal_name, resource_definition)</code>","text":"<p>Allow children to inject arbitrary object mutation logic for individual managed objects</p> <p>The base implementation simply returns the given definition as a passthrough</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The session for this reconciliation</p> required <code>internal_name</code> <code>str</code> <p>str The internal name of the object to update</p> required <code>resource_definition</code> <code>dict</code> <p>dict The dict representation of the resource to modify</p> required <p>Returns:</p> Name Type Description <code>resource_definition</code> <p>dict The dict representation of the resource with any modifications applied</p> Source code in <code>oper8/component.py</code> <pre><code>def update_object_definition(\n    self,\n    session: Session,  # pylint: disable=unused-argument\n    internal_name: str,  # pylint: disable=unused-argument\n    resource_definition: dict,\n):\n    \"\"\"Allow children to inject arbitrary object mutation logic for\n    individual managed objects\n\n    The base implementation simply returns the given definition as a\n    passthrough\n\n    Args:\n        session:  Session\n            The session for this reconciliation\n        internal_name:  str\n            The internal name of the object to update\n        resource_definition:  dict\n            The dict representation of the resource to modify\n\n    Returns:\n        resource_definition:  dict\n            The dict representation of the resource with any modifications\n            applied\n    \"\"\"\n    return resource_definition\n</code></pre>"},{"location":"API%20References/#oper8.component.Component.verify","title":"<code>verify(session)</code>","text":"<p>The verify function will run any necessary testing and validation that the component needs to ensure that rollout was successfully completed.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <p>Session The current reconciliation session</p> required <p>Returns:</p> Name Type Description <code>success</code> <p>bool True on successful deployment verification, False on failure conditions</p> Source code in <code>oper8/component.py</code> <pre><code>def verify(self, session):\n    \"\"\"The verify function will run any necessary testing and validation\n    that the component needs to ensure that rollout was successfully\n    completed.\n\n    Args:\n        session:  Session\n            The current reconciliation session\n\n    Returns:\n        success:  bool\n            True on successful deployment verification, False on failure\n            conditions\n    \"\"\"\n    return self._default_verify(session, is_subsystem=False)\n</code></pre>"},{"location":"API%20References/#oper8.config","title":"<code>config</code>","text":"<p>Base operator config module. The config here is only used as a baseline bootup config. All application-specific config must come from the app_config.</p>"},{"location":"API%20References/#oper8.config.config","title":"<code>config</code>","text":"<p>This module just loads config at import time and does the initial log config</p>"},{"location":"API%20References/#oper8.config.validation","title":"<code>validation</code>","text":"<p>Module to validate values in a loaded config</p>"},{"location":"API%20References/#oper8.config.validation.get_invalid_params","title":"<code>get_invalid_params(config, validation_config)</code>","text":"<p>Get a list of any params that are invalid</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>aconfig.Config The parsed config with any override values</p> required <code>validation_config</code> <code>Config</code> <p>aconfig.Config The parallel config holding validation setup</p> required <p>Returns:</p> Name Type Description <code>invalid_params</code> <code>List[str]</code> <p>List[str] A list of all string keys for parameters that fail validation</p> Source code in <code>oper8/config/validation.py</code> <pre><code>def get_invalid_params(\n    config: aconfig.Config,\n    validation_config: aconfig.Config,\n) -&gt; List[str]:\n    \"\"\"Get a list of any params that are invalid\n\n    Args:\n        config:  aconfig.Config\n            The parsed config with any override values\n        validation_config:  aconfig.Config\n            The parallel config holding validation setup\n\n    Returns:\n        invalid_params:  List[str]\n            A list of all string keys for parameters that fail validation\n    \"\"\"\n\n    # For each validation element, perform the validation\n    invalid_params = []\n    for val_key, validator in _parse_validation_config(validation_config).items():\n        if not validator.validate(nested_get(config, val_key)):\n            log.warning(\"Found invalid config key [%s]\", val_key)\n            invalid_params.append(val_key)\n\n    # Return the list of invalid params\n    return invalid_params\n</code></pre>"},{"location":"API%20References/#oper8.constants","title":"<code>constants</code>","text":"<p>Shared module to hold constant values for the library</p>"},{"location":"API%20References/#oper8.controller","title":"<code>controller</code>","text":"<p>The Controller class manages a collection of Components and associates them with a CustomResource in the cluster.</p>"},{"location":"API%20References/#oper8.controller.Controller","title":"<code>Controller</code>","text":"<p>               Bases: <code>ABC</code></p> <p>This class represents a controller for a single kubernetes custom resource kind. Its primary functionality is to perform a reconciliation of a given CR manifest for an instance of the resource kind against the current state of the cluster. To accomplish this, its reconciliation logic is:</p> <ol> <li>Construct a Directed Acyclic Graph of all Components that this kind     needs to manage.</li> <li>Execute the Graph in dependency order where each node of the graph first     renders the manifests for all kubernetes resources managed by the     Component, then applies them to the cluster.</li> <li>Execute a secondary Graph with verification logic for each Component,     terminating verification for downstream nodes if any node is not yet     verified.</li> </ol> <p>To do this, the main operations of the class are to construct a DAG of Components, then walk them through the primary lifecycle phases:</p> <ol> <li>Run the Component's deploy() function to completion and verify that the     actual deployment operations succeeded</li> <li>Run the Component's verify() function to run component-specific tests     that will verify if the deployment is rolled out in a successful state</li> </ol> Source code in <code>oper8/controller.py</code> <pre><code>class Controller(abc.ABC):\n    \"\"\"This class represents a controller for a single kubernetes custom\n    resource kind. Its primary functionality is to perform a reconciliation of a\n    given CR manifest for an instance of the resource kind against the current\n    state of the cluster. To accomplish this, its reconciliation logic is:\n\n    1. Construct a Directed Acyclic Graph of all Components that this kind\n        needs to manage.\n    2. Execute the Graph in dependency order where each node of the graph first\n        renders the manifests for all kubernetes resources managed by the\n        Component, then applies them to the cluster.\n    3. Execute a secondary Graph with verification logic for each Component,\n        terminating verification for downstream nodes if any node is not yet\n        verified.\n\n    To do this, the main operations of the class are to construct a DAG of\n    Components, then walk them through the primary lifecycle phases:\n\n    1. Run the Component's deploy() function to completion and verify that the\n        actual deployment operations succeeded\n    2. Run the Component's verify() function to run component-specific tests\n        that will verify if the deployment is rolled out in a successful state\n    \"\"\"\n\n    ## Class Properties ########################################################\n\n    # Derived classes must have class properties for group, version, and kind.\n    # To enforce this, we set defaults for all of these and then validate that\n    # they are present, we define them as classproperty and raise when accessed\n    # from the base implementation.\n\n    # NOTE: pylint is very confused by the use of these property decorators, so\n    #   we need to liberally ignore warnings.\n\n    @abstractclassproperty  # noqa: B027\n    def group(cls) -&gt; str:\n        \"\"\"The apiVersion group for the resource this controller manages\"\"\"\n\n    @abstractclassproperty  # noqa: B027\n    def version(cls) -&gt; str:\n        \"\"\"The apiVersion version for the resource this controller manages\"\"\"\n\n    @abstractclassproperty  # noqa: B027\n    def kind(cls) -&gt; str:\n        \"\"\"The kind for the resource this controller manages\"\"\"\n\n    @classproperty\n    def finalizer(cls) -&gt; Optional[str]:  # pylint: disable=no-self-argument\n        \"\"\"The finalizer used by this Controller\"\"\"\n        if cls.has_finalizer:  # pylint: disable=using-constant-test\n            return f\"finalizers.{cls.kind.lower()}.{cls.group}\"  # pylint: disable=no-member\n        return None\n\n    @classproperty\n    def has_finalizer(cls) -&gt; bool:  # pylint: disable=no-self-argument\n        \"\"\"If the derived class has an implementation of finalize_components, it\n        has a finalizer and can be registered for finalize events\n        \"\"\"\n        return cls.finalize_components is not Controller.finalize_components\n\n    ## Construction ############################################################\n\n    def __init__(self, config_defaults: Optional[aconfig.Config] = None):\n        \"\"\"The constructor sets up all of the properties of the controller which\n        are constant across reconciliations.\n\n        Args:\n            config_defaults:  Optional[aconfig.Config]\n                Default values for the backend controller config\n\n        \"\"\"\n        # Make sure the class properties are present and not empty\n        assert self.group, \"Controller.group must be a non-empty string\"\n        assert self.version, \"Controller.version must be a non-empty string\"\n        assert self.kind, \"Controller.kind must be a non-empty string\"\n        self.config_defaults = config_defaults or aconfig.Config({})\n\n    @classmethod\n    def __str__(cls):\n        \"\"\"Stringify with the GVK\"\"\"\n        return f\"Controller({cls.group}/{cls.version}/{cls.kind})\"\n\n    ## Abstract Interface ######################################################\n    #\n    # These functions must be implemented by child classes\n    ##\n\n    @abc.abstractmethod\n    def setup_components(self, session: Session):\n        \"\"\"Given the session for an individual reconciliation, construct the set\n        of Components that will be deployed.\n\n        Error Semantics: Child classes should throw ConfigError if config is\n        not valid and include the portion of config where the problem occurred.\n\n        Args:\n            session:  Session\n                The current session containing the per-event configs\n        \"\"\"\n\n    ## Base Class Interface ####################################################\n    #\n    # These methods MAY be implemented by children, but contain default\n    # implementations that are appropriate for simple cases.\n    #\n    # NOTE: We liberally use pylint disables here to make the base interface\n    #   clear to deriving classes.\n    ##\n\n    def finalize_components(self, session: Session):  # noqa: B027\n        \"\"\"When performing a finalizer operation, this function will be called\n        to perform custom finalizer logic for this Controller.\n\n        Error Semantics: Child classes should throw ConfigError if config is\n        not valid and include the portion of config where the problem occurred.\n\n        NOTE: This method is not abstract since the standard controller usecase\n            does not require finalizing\n\n        Args:\n            session:  Session\n                The current session containing the per-event configs\n        \"\"\"\n\n    def after_deploy(\n        self, session: Session, deploy_completion_state: CompletionState\n    ) -&gt; bool:\n        \"\"\"This allows children to inject logic that will run when the\n        controller has finished deploying all components, but not necessarily\n        verifying all of them. The default behavior is a no-op.\n\n        Args:\n            session:  Session\n                The current reconciliation session\n            deploy_completion_state: CompletionState\n                Completion state of the last deployment\n\n        Returns:\n            success:  bool\n                True if custom hook code executed successfully and lifecycle\n                should continue\n        \"\"\"\n        return True\n\n    def after_deploy_unsuccessful(\n        self, session: Session, failed: bool, deploy_completion_state: CompletionState\n    ) -&gt; bool:\n        \"\"\"This allows children to inject logic that will run when the\n        controller has failed or could not finish deploying all components.\n        The default behavior is a no-op.\n\n        Args:\n            session:  Session\n                The current reconciliation session\n            failed:  bool\n                Indicator of whether or not the termination was a failure\n            deploy_completion_state: CompletionState\n                Completion state of the last deployment\n\n        Returns:\n            success:  bool\n                True if custom hook code executed successfully and lifecycle\n                should continue\n        \"\"\"\n        return True\n\n    def after_verify(\n        self,\n        session: Session,  # pylint: disable=unused-argument\n        verify_completion_state: CompletionState,\n        deploy_completion_state: CompletionState,\n    ) -&gt; bool:\n        \"\"\"This allows children to inject logic that will run when the\n        controller has finished verifying all components. The default behavior\n        is a no-op.\n\n        Args:\n            session:  Session\n                The current reconciliation session\n            verify_completion_state: CompletionState\n                Completion state of the last verification\n            deploy_completion_state: CompletionState\n                Completion state of the last deployment\n\n        Returns:\n            success:  bool\n                True if custom hook code executed successfully and lifecycle\n                should continue\n        \"\"\"\n        return True\n\n    def after_verify_unsuccessful(\n        self,\n        session: Session,\n        failed: bool,\n        verify_completion_state: CompletionState,\n        deploy_completion_state: CompletionState,\n    ) -&gt; bool:\n        \"\"\"This allows children to inject logic that will run when the\n        controller has finished deploying all components but failed to verify.\n        The default behavior is a no-op.\n\n        Args:\n            session:  Session\n                The current reconciliation session\n            failed:  bool\n                Indicator of whether or not the termination was a failure\n            verify_completion_state: CompletionState\n                Completion state of the last verification\n            deploy_completion_state: CompletionState\n                Completion state of the last deployment\n\n        Returns:\n            success:  bool\n                True if custom hook code executed successfully and lifecycle\n                should continue\n        \"\"\"\n        return True\n\n    def should_requeue(self, session: Session) -&gt; Tuple[bool, Optional[RequeueParams]]:\n        \"\"\"should_requeue determines if current reconcile request should be re-queued.\n\n        Children can override default implementation to provide custom logic.\n        Default implementation re-queues the request if the reconciling CR status\n        hasn't been reached stable state.\n\n        Args:\n            session: Session\n                The current reconciliation session\n\n        Returns:\n            requeue: bool\n                True if the reconciliation request should be re-queued\n            config: RequeueParams\n                 Parameters of requeue request. Can be None if requeue is False.\n        \"\"\"\n        api_version = session.api_version\n        kind = session.kind\n        name = session.name\n        namespace = session.namespace\n        requeue_params = RequeueParams()\n        # Fetch the current status from the cluster\n        success, current_state = session.deploy_manager.get_object_current_state(\n            api_version=api_version,\n            kind=kind,\n            name=name,\n            namespace=namespace,\n        )\n        if not success:\n            log.warning(\n                \"Failed to fetch current state for %s/%s/%s\", namespace, kind, name\n            )\n            return True, requeue_params\n        # Do not requeue if resource was deleted\n        if not current_state:\n            log.warning(\"Resource not found: %s/%s/%s\", namespace, kind, name)\n            return False, requeue_params\n\n        log.debug3(\"Current CR manifest for requeue check: %s\", current_state)\n\n        verified = verify_subsystem(current_state, session.version)\n        return not verified, requeue_params\n\n    def get_cr_manifest_defaults(\n        self,\n    ) -&gt; Union[dict, aconfig.Config]:\n        \"\"\"This allows children to provide default values for their cr_manifest\n        that will be injected where no override is provided in the user-provided\n        cr_manifest.\n\n        Returns:\n            cr_manifest_defaults:  Union[dict, aconfig.Config]\n                The cr defaults. Raw dicts will be converted to Config objects.\n        \"\"\"\n        return aconfig.Config({})\n\n    def get_config_defaults(self):\n        \"\"\"This function allows children to override the default values for the session\n        config. This value can also be set via the controllers __init__ function.\n        \"\"\"\n        return self.config_defaults\n\n    ## Public Interface ########################################################\n    #\n    # These functions should be used by the reconciliation manager or in\n    # tests\n    ##\n\n    def run_reconcile(\n        self, session: Session, is_finalizer: bool = False\n    ) -&gt; CompletionState:\n        \"\"\"Perform a reconciliation iteration for this controller on given a session.\n        This function should only be called once per session. The general logic for a\n        controller reconcile is as follows:\n\n        1. Set up the set of Components and their dependencies that will be\n            managed in this reconciliation based on the CR and config\n        2. Invoke the rollout to render each component and apply it to the\n            cluster (if not in dry-run), then verify the DAG of components\n\n        Args:\n            session:  Session\n                The full structured content of the CR manifest for this operand\n            is_finalizer:  bool\n                If true, the logic in finalize_components is run, otherwise the\n                logic in setup_components is called\n\n        Returns:\n            result: ReconciliationResult\n                The result of reconcile\n        \"\"\"\n        # Check if session has already been reconciled\n        if not session.graph.empty():\n            raise RolloutError(\"Session has already been reconciled\")\n\n        self._manage_components(session, is_finalizer)\n        completion_state = self._rollout_components(session)\n        return completion_state\n\n    ## Implementation Details ##################################################\n\n    def _manage_components(self, session: Session, is_finalizer: bool):\n        \"\"\"Delegate logic to child's finalize_components or setup_components\n\n        Args:\n            session: Session\n                The current session being reconciled\n            is_finalizer: bool\n                Weather the current CR is being deleted\n\n        \"\"\"\n\n        # If this is a finalizer, run finalize_components\n        if is_finalizer:\n            log.debug(\"[%s] Running as finalizer\", session.id)\n            self.finalize_components(session)\n\n        # Otherwise run setup_components\n        else:\n            self.setup_components(session)\n\n    @alog.logged_function(log.debug)\n    def _rollout_components(self, session: Session):\n        \"\"\"Deploy all dependent components according to the configured\n        dependencies between them\n        \"\"\"\n        log.debug(\"Rolling out %s\", str(self))\n        log.debug3(\"Session dependency DAG: %s\", str(session.graph))\n\n        # Set up the deployment manager and run the rollout\n        rollout_manager = RolloutManager(\n            session=session,\n            after_deploy=self.after_deploy,\n            after_deploy_unsuccessful=self.after_deploy_unsuccessful,\n            after_verify=self.after_verify,\n            after_verify_unsuccessful=self.after_verify_unsuccessful,\n        )\n        completion_state = rollout_manager.rollout()\n        rollout_failed = completion_state.failed()\n        log.info(\"Final rollout state: %s\", completion_state)\n\n        # Get Rollout Status\n        deploy_completed = completion_state.deploy_completed()\n        verify_completed = completion_state.verify_completed()\n        log.debug2(\n            \"Deploy Completed: %s, Verify Completed: %s, Deploy Failed: %s\",\n            deploy_completed,\n            verify_completed,\n            rollout_failed,\n        )\n\n        # If an oper8 error occurred in the rollout, decorate it with a reference\n        # to the completion state itself and then raise it to be handled by the\n        # top-level ReconcileManager handler.\n        if isinstance(completion_state.exception, Oper8Error):\n            log.debug(\"Handling Oper8Error from rollout\")\n            completion_state.exception.completion_state = completion_state\n            raise completion_state.exception\n\n        # If the deploy failed but didn't trigger an Oper8Error, we'll make one\n        # ourselves\n        if rollout_failed:\n            raise RolloutError(\n                \"Deploy phase failed\", completion_state=completion_state\n            ) from completion_state.exception\n\n        return completion_state\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.__init__","title":"<code>__init__(config_defaults=None)</code>","text":"<p>The constructor sets up all of the properties of the controller which are constant across reconciliations.</p> <p>Parameters:</p> Name Type Description Default <code>config_defaults</code> <code>Optional[Config]</code> <p>Optional[aconfig.Config] Default values for the backend controller config</p> <code>None</code> Source code in <code>oper8/controller.py</code> <pre><code>def __init__(self, config_defaults: Optional[aconfig.Config] = None):\n    \"\"\"The constructor sets up all of the properties of the controller which\n    are constant across reconciliations.\n\n    Args:\n        config_defaults:  Optional[aconfig.Config]\n            Default values for the backend controller config\n\n    \"\"\"\n    # Make sure the class properties are present and not empty\n    assert self.group, \"Controller.group must be a non-empty string\"\n    assert self.version, \"Controller.version must be a non-empty string\"\n    assert self.kind, \"Controller.kind must be a non-empty string\"\n    self.config_defaults = config_defaults or aconfig.Config({})\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.__str__","title":"<code>__str__()</code>  <code>classmethod</code>","text":"<p>Stringify with the GVK</p> Source code in <code>oper8/controller.py</code> <pre><code>@classmethod\ndef __str__(cls):\n    \"\"\"Stringify with the GVK\"\"\"\n    return f\"Controller({cls.group}/{cls.version}/{cls.kind})\"\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.after_deploy","title":"<code>after_deploy(session, deploy_completion_state)</code>","text":"<p>This allows children to inject logic that will run when the controller has finished deploying all components, but not necessarily verifying all of them. The default behavior is a no-op.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current reconciliation session</p> required <code>deploy_completion_state</code> <code>CompletionState</code> <p>CompletionState Completion state of the last deployment</p> required <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool True if custom hook code executed successfully and lifecycle should continue</p> Source code in <code>oper8/controller.py</code> <pre><code>def after_deploy(\n    self, session: Session, deploy_completion_state: CompletionState\n) -&gt; bool:\n    \"\"\"This allows children to inject logic that will run when the\n    controller has finished deploying all components, but not necessarily\n    verifying all of them. The default behavior is a no-op.\n\n    Args:\n        session:  Session\n            The current reconciliation session\n        deploy_completion_state: CompletionState\n            Completion state of the last deployment\n\n    Returns:\n        success:  bool\n            True if custom hook code executed successfully and lifecycle\n            should continue\n    \"\"\"\n    return True\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.after_deploy_unsuccessful","title":"<code>after_deploy_unsuccessful(session, failed, deploy_completion_state)</code>","text":"<p>This allows children to inject logic that will run when the controller has failed or could not finish deploying all components. The default behavior is a no-op.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current reconciliation session</p> required <code>failed</code> <code>bool</code> <p>bool Indicator of whether or not the termination was a failure</p> required <code>deploy_completion_state</code> <code>CompletionState</code> <p>CompletionState Completion state of the last deployment</p> required <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool True if custom hook code executed successfully and lifecycle should continue</p> Source code in <code>oper8/controller.py</code> <pre><code>def after_deploy_unsuccessful(\n    self, session: Session, failed: bool, deploy_completion_state: CompletionState\n) -&gt; bool:\n    \"\"\"This allows children to inject logic that will run when the\n    controller has failed or could not finish deploying all components.\n    The default behavior is a no-op.\n\n    Args:\n        session:  Session\n            The current reconciliation session\n        failed:  bool\n            Indicator of whether or not the termination was a failure\n        deploy_completion_state: CompletionState\n            Completion state of the last deployment\n\n    Returns:\n        success:  bool\n            True if custom hook code executed successfully and lifecycle\n            should continue\n    \"\"\"\n    return True\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.after_verify","title":"<code>after_verify(session, verify_completion_state, deploy_completion_state)</code>","text":"<p>This allows children to inject logic that will run when the controller has finished verifying all components. The default behavior is a no-op.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current reconciliation session</p> required <code>verify_completion_state</code> <code>CompletionState</code> <p>CompletionState Completion state of the last verification</p> required <code>deploy_completion_state</code> <code>CompletionState</code> <p>CompletionState Completion state of the last deployment</p> required <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool True if custom hook code executed successfully and lifecycle should continue</p> Source code in <code>oper8/controller.py</code> <pre><code>def after_verify(\n    self,\n    session: Session,  # pylint: disable=unused-argument\n    verify_completion_state: CompletionState,\n    deploy_completion_state: CompletionState,\n) -&gt; bool:\n    \"\"\"This allows children to inject logic that will run when the\n    controller has finished verifying all components. The default behavior\n    is a no-op.\n\n    Args:\n        session:  Session\n            The current reconciliation session\n        verify_completion_state: CompletionState\n            Completion state of the last verification\n        deploy_completion_state: CompletionState\n            Completion state of the last deployment\n\n    Returns:\n        success:  bool\n            True if custom hook code executed successfully and lifecycle\n            should continue\n    \"\"\"\n    return True\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.after_verify_unsuccessful","title":"<code>after_verify_unsuccessful(session, failed, verify_completion_state, deploy_completion_state)</code>","text":"<p>This allows children to inject logic that will run when the controller has finished deploying all components but failed to verify. The default behavior is a no-op.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current reconciliation session</p> required <code>failed</code> <code>bool</code> <p>bool Indicator of whether or not the termination was a failure</p> required <code>verify_completion_state</code> <code>CompletionState</code> <p>CompletionState Completion state of the last verification</p> required <code>deploy_completion_state</code> <code>CompletionState</code> <p>CompletionState Completion state of the last deployment</p> required <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool True if custom hook code executed successfully and lifecycle should continue</p> Source code in <code>oper8/controller.py</code> <pre><code>def after_verify_unsuccessful(\n    self,\n    session: Session,\n    failed: bool,\n    verify_completion_state: CompletionState,\n    deploy_completion_state: CompletionState,\n) -&gt; bool:\n    \"\"\"This allows children to inject logic that will run when the\n    controller has finished deploying all components but failed to verify.\n    The default behavior is a no-op.\n\n    Args:\n        session:  Session\n            The current reconciliation session\n        failed:  bool\n            Indicator of whether or not the termination was a failure\n        verify_completion_state: CompletionState\n            Completion state of the last verification\n        deploy_completion_state: CompletionState\n            Completion state of the last deployment\n\n    Returns:\n        success:  bool\n            True if custom hook code executed successfully and lifecycle\n            should continue\n    \"\"\"\n    return True\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.finalize_components","title":"<code>finalize_components(session)</code>","text":"<p>When performing a finalizer operation, this function will be called to perform custom finalizer logic for this Controller.</p> <p>Error Semantics: Child classes should throw ConfigError if config is not valid and include the portion of config where the problem occurred.</p> This method is not abstract since the standard controller usecase <p>does not require finalizing</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current session containing the per-event configs</p> required Source code in <code>oper8/controller.py</code> <pre><code>def finalize_components(self, session: Session):  # noqa: B027\n    \"\"\"When performing a finalizer operation, this function will be called\n    to perform custom finalizer logic for this Controller.\n\n    Error Semantics: Child classes should throw ConfigError if config is\n    not valid and include the portion of config where the problem occurred.\n\n    NOTE: This method is not abstract since the standard controller usecase\n        does not require finalizing\n\n    Args:\n        session:  Session\n            The current session containing the per-event configs\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.finalizer","title":"<code>finalizer()</code>","text":"<p>The finalizer used by this Controller</p> Source code in <code>oper8/controller.py</code> <pre><code>@classproperty\ndef finalizer(cls) -&gt; Optional[str]:  # pylint: disable=no-self-argument\n    \"\"\"The finalizer used by this Controller\"\"\"\n    if cls.has_finalizer:  # pylint: disable=using-constant-test\n        return f\"finalizers.{cls.kind.lower()}.{cls.group}\"  # pylint: disable=no-member\n    return None\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.get_config_defaults","title":"<code>get_config_defaults()</code>","text":"<p>This function allows children to override the default values for the session config. This value can also be set via the controllers init function.</p> Source code in <code>oper8/controller.py</code> <pre><code>def get_config_defaults(self):\n    \"\"\"This function allows children to override the default values for the session\n    config. This value can also be set via the controllers __init__ function.\n    \"\"\"\n    return self.config_defaults\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.get_cr_manifest_defaults","title":"<code>get_cr_manifest_defaults()</code>","text":"<p>This allows children to provide default values for their cr_manifest that will be injected where no override is provided in the user-provided cr_manifest.</p> <p>Returns:</p> Name Type Description <code>cr_manifest_defaults</code> <code>Union[dict, Config]</code> <p>Union[dict, aconfig.Config] The cr defaults. Raw dicts will be converted to Config objects.</p> Source code in <code>oper8/controller.py</code> <pre><code>def get_cr_manifest_defaults(\n    self,\n) -&gt; Union[dict, aconfig.Config]:\n    \"\"\"This allows children to provide default values for their cr_manifest\n    that will be injected where no override is provided in the user-provided\n    cr_manifest.\n\n    Returns:\n        cr_manifest_defaults:  Union[dict, aconfig.Config]\n            The cr defaults. Raw dicts will be converted to Config objects.\n    \"\"\"\n    return aconfig.Config({})\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.group","title":"<code>group()</code>","text":"<p>The apiVersion group for the resource this controller manages</p> Source code in <code>oper8/controller.py</code> <pre><code>@abstractclassproperty  # noqa: B027\ndef group(cls) -&gt; str:\n    \"\"\"The apiVersion group for the resource this controller manages\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.has_finalizer","title":"<code>has_finalizer()</code>","text":"<p>If the derived class has an implementation of finalize_components, it has a finalizer and can be registered for finalize events</p> Source code in <code>oper8/controller.py</code> <pre><code>@classproperty\ndef has_finalizer(cls) -&gt; bool:  # pylint: disable=no-self-argument\n    \"\"\"If the derived class has an implementation of finalize_components, it\n    has a finalizer and can be registered for finalize events\n    \"\"\"\n    return cls.finalize_components is not Controller.finalize_components\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.kind","title":"<code>kind()</code>","text":"<p>The kind for the resource this controller manages</p> Source code in <code>oper8/controller.py</code> <pre><code>@abstractclassproperty  # noqa: B027\ndef kind(cls) -&gt; str:\n    \"\"\"The kind for the resource this controller manages\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.run_reconcile","title":"<code>run_reconcile(session, is_finalizer=False)</code>","text":"<p>Perform a reconciliation iteration for this controller on given a session. This function should only be called once per session. The general logic for a controller reconcile is as follows:</p> <ol> <li>Set up the set of Components and their dependencies that will be     managed in this reconciliation based on the CR and config</li> <li>Invoke the rollout to render each component and apply it to the     cluster (if not in dry-run), then verify the DAG of components</li> </ol> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The full structured content of the CR manifest for this operand</p> required <code>is_finalizer</code> <code>bool</code> <p>bool If true, the logic in finalize_components is run, otherwise the logic in setup_components is called</p> <code>False</code> <p>Returns:</p> Name Type Description <code>result</code> <code>CompletionState</code> <p>ReconciliationResult The result of reconcile</p> Source code in <code>oper8/controller.py</code> <pre><code>def run_reconcile(\n    self, session: Session, is_finalizer: bool = False\n) -&gt; CompletionState:\n    \"\"\"Perform a reconciliation iteration for this controller on given a session.\n    This function should only be called once per session. The general logic for a\n    controller reconcile is as follows:\n\n    1. Set up the set of Components and their dependencies that will be\n        managed in this reconciliation based on the CR and config\n    2. Invoke the rollout to render each component and apply it to the\n        cluster (if not in dry-run), then verify the DAG of components\n\n    Args:\n        session:  Session\n            The full structured content of the CR manifest for this operand\n        is_finalizer:  bool\n            If true, the logic in finalize_components is run, otherwise the\n            logic in setup_components is called\n\n    Returns:\n        result: ReconciliationResult\n            The result of reconcile\n    \"\"\"\n    # Check if session has already been reconciled\n    if not session.graph.empty():\n        raise RolloutError(\"Session has already been reconciled\")\n\n    self._manage_components(session, is_finalizer)\n    completion_state = self._rollout_components(session)\n    return completion_state\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.setup_components","title":"<code>setup_components(session)</code>  <code>abstractmethod</code>","text":"<p>Given the session for an individual reconciliation, construct the set of Components that will be deployed.</p> <p>Error Semantics: Child classes should throw ConfigError if config is not valid and include the portion of config where the problem occurred.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current session containing the per-event configs</p> required Source code in <code>oper8/controller.py</code> <pre><code>@abc.abstractmethod\ndef setup_components(self, session: Session):\n    \"\"\"Given the session for an individual reconciliation, construct the set\n    of Components that will be deployed.\n\n    Error Semantics: Child classes should throw ConfigError if config is\n    not valid and include the portion of config where the problem occurred.\n\n    Args:\n        session:  Session\n            The current session containing the per-event configs\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.should_requeue","title":"<code>should_requeue(session)</code>","text":"<p>should_requeue determines if current reconcile request should be re-queued.</p> <p>Children can override default implementation to provide custom logic. Default implementation re-queues the request if the reconciling CR status hasn't been reached stable state.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current reconciliation session</p> required <p>Returns:</p> Name Type Description <code>requeue</code> <code>bool</code> <p>bool True if the reconciliation request should be re-queued</p> <code>config</code> <code>Optional[RequeueParams]</code> <p>RequeueParams  Parameters of requeue request. Can be None if requeue is False.</p> Source code in <code>oper8/controller.py</code> <pre><code>def should_requeue(self, session: Session) -&gt; Tuple[bool, Optional[RequeueParams]]:\n    \"\"\"should_requeue determines if current reconcile request should be re-queued.\n\n    Children can override default implementation to provide custom logic.\n    Default implementation re-queues the request if the reconciling CR status\n    hasn't been reached stable state.\n\n    Args:\n        session: Session\n            The current reconciliation session\n\n    Returns:\n        requeue: bool\n            True if the reconciliation request should be re-queued\n        config: RequeueParams\n             Parameters of requeue request. Can be None if requeue is False.\n    \"\"\"\n    api_version = session.api_version\n    kind = session.kind\n    name = session.name\n    namespace = session.namespace\n    requeue_params = RequeueParams()\n    # Fetch the current status from the cluster\n    success, current_state = session.deploy_manager.get_object_current_state(\n        api_version=api_version,\n        kind=kind,\n        name=name,\n        namespace=namespace,\n    )\n    if not success:\n        log.warning(\n            \"Failed to fetch current state for %s/%s/%s\", namespace, kind, name\n        )\n        return True, requeue_params\n    # Do not requeue if resource was deleted\n    if not current_state:\n        log.warning(\"Resource not found: %s/%s/%s\", namespace, kind, name)\n        return False, requeue_params\n\n    log.debug3(\"Current CR manifest for requeue check: %s\", current_state)\n\n    verified = verify_subsystem(current_state, session.version)\n    return not verified, requeue_params\n</code></pre>"},{"location":"API%20References/#oper8.controller.Controller.version","title":"<code>version()</code>","text":"<p>The apiVersion version for the resource this controller manages</p> Source code in <code>oper8/controller.py</code> <pre><code>@abstractclassproperty  # noqa: B027\ndef version(cls) -&gt; str:\n    \"\"\"The apiVersion version for the resource this controller manages\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.dag","title":"<code>dag</code>","text":"<p>Package exports</p>"},{"location":"API%20References/#oper8.dag.completion_state","title":"<code>completion_state</code>","text":"<p>CompletionState holds info about how a DAG Runner completes</p>"},{"location":"API%20References/#oper8.dag.completion_state.CompletionState","title":"<code>CompletionState</code>","text":"<p>This class holds the definition of a CompletionState which manages all the information about how the nodes in a rollout Runner terminated</p> Source code in <code>oper8/dag/completion_state.py</code> <pre><code>class CompletionState:\n    \"\"\"\n    This class holds the definition of a CompletionState which manages all\n    the information about how the nodes in a rollout Runner terminated\n    \"\"\"\n\n    def __init__(  # pylint: disable=too-many-arguments\n        self,\n        verified_nodes: Optional[List[Node]] = None,\n        unverified_nodes: Optional[List[Node]] = None,\n        failed_nodes: Optional[List[Node]] = None,\n        unstarted_nodes: Optional[List[Node]] = None,\n        exception: Optional[Exception] = None,\n    ):\n        \"\"\"Construct with each node set\"\"\"\n        self.verified_nodes = set(verified_nodes or [])\n        self.unverified_nodes = set(unverified_nodes or [])\n        self.failed_nodes = set(failed_nodes or [])\n        self.unstarted_nodes = set(unstarted_nodes or [])\n        self.all_nodes = (\n            self.verified_nodes.union(self.unverified_nodes)\n            .union(self.failed_nodes)\n            .union(self.unstarted_nodes)\n        )\n        self.exception = exception\n\n        # Make sure the sets are not overlapping\n        sets = [\n            self.verified_nodes,\n            self.unverified_nodes,\n            self.failed_nodes,\n            self.unstarted_nodes,\n        ]\n        for i, node_set_a in enumerate(sets):\n            for j, node_set_b in enumerate(sets):\n                if i != j:\n                    assert not node_set_a.intersection(node_set_b), (\n                        \"Programming Error: \"\n                        + f\"CompletionState constructed with overlapping sets: {str(self)}\"\n                    )\n\n    def __str__(self):\n        return \"\\n\".join(\n            [\n                f\"[NODES] {key}: {list(sorted(nodes))}\"\n                for key, nodes in [\n                    (\"Verified\", [node.get_name() for node in self.verified_nodes]),\n                    (\"Unverified\", [node.get_name() for node in self.unverified_nodes]),\n                    (\"Failed\", [node.get_name() for node in self.failed_nodes]),\n                    (\"Unstarted\", [node.get_name() for node in self.unstarted_nodes]),\n                ]\n            ]\n            + [\n                f\"Exception: {self.exception}\",\n            ]\n        )\n\n    def __eq__(self, other: \"CompletionState\"):\n        return (\n            self.verified_nodes == other.verified_nodes\n            and self.unverified_nodes == other.unverified_nodes\n            and self.failed_nodes == other.failed_nodes\n            and self.unstarted_nodes == other.unstarted_nodes\n        )\n\n    def deploy_completed(self) -&gt; bool:\n        \"\"\"Determine if the dag completed all nodes through to the deploy\n        step\n\n        NOTE: An empty node set is considered completed\n\n        Returns:\n            completed:  bool\n                True if there are no failed nodes and no unstarted nodes\n        \"\"\"\n        return not self.failed_nodes and not self.unstarted_nodes\n\n    def verify_completed(self) -&gt; bool:\n        \"\"\"Determine if the dag completed all nodes through to the verification\n        step\n\n        NOTE: An empty node set is considered verified\n\n        Returns:\n            completed:  bool\n                True if there are no nodes found outside of the verified_nodes\n                and there is no exception in the termination state\n        \"\"\"\n        return (\n            not self.unverified_nodes\n            and not self.failed_nodes\n            and not self.unstarted_nodes\n            and not self.exception\n        )\n\n    def failed(self) -&gt; bool:\n        \"\"\"Determine if any of the nodes failed\n\n        Returns:\n            failed:  bool\n                True if there are any nodes in the failed state or there is a\n                fatal error\n        \"\"\"\n        return bool(self.failed_nodes) or self._fatal_exception()\n\n    def _fatal_exception(self):\n        \"\"\"Helper to determine if there is a fatal exception in the state\"\"\"\n        return self.exception is not None and getattr(\n            self.exception, \"is_fatal_error\", True\n        )\n</code></pre>"},{"location":"API%20References/#oper8.dag.completion_state.CompletionState.__init__","title":"<code>__init__(verified_nodes=None, unverified_nodes=None, failed_nodes=None, unstarted_nodes=None, exception=None)</code>","text":"<p>Construct with each node set</p> Source code in <code>oper8/dag/completion_state.py</code> <pre><code>def __init__(  # pylint: disable=too-many-arguments\n    self,\n    verified_nodes: Optional[List[Node]] = None,\n    unverified_nodes: Optional[List[Node]] = None,\n    failed_nodes: Optional[List[Node]] = None,\n    unstarted_nodes: Optional[List[Node]] = None,\n    exception: Optional[Exception] = None,\n):\n    \"\"\"Construct with each node set\"\"\"\n    self.verified_nodes = set(verified_nodes or [])\n    self.unverified_nodes = set(unverified_nodes or [])\n    self.failed_nodes = set(failed_nodes or [])\n    self.unstarted_nodes = set(unstarted_nodes or [])\n    self.all_nodes = (\n        self.verified_nodes.union(self.unverified_nodes)\n        .union(self.failed_nodes)\n        .union(self.unstarted_nodes)\n    )\n    self.exception = exception\n\n    # Make sure the sets are not overlapping\n    sets = [\n        self.verified_nodes,\n        self.unverified_nodes,\n        self.failed_nodes,\n        self.unstarted_nodes,\n    ]\n    for i, node_set_a in enumerate(sets):\n        for j, node_set_b in enumerate(sets):\n            if i != j:\n                assert not node_set_a.intersection(node_set_b), (\n                    \"Programming Error: \"\n                    + f\"CompletionState constructed with overlapping sets: {str(self)}\"\n                )\n</code></pre>"},{"location":"API%20References/#oper8.dag.completion_state.CompletionState.deploy_completed","title":"<code>deploy_completed()</code>","text":"<p>Determine if the dag completed all nodes through to the deploy step</p> <p>NOTE: An empty node set is considered completed</p> <p>Returns:</p> Name Type Description <code>completed</code> <code>bool</code> <p>bool True if there are no failed nodes and no unstarted nodes</p> Source code in <code>oper8/dag/completion_state.py</code> <pre><code>def deploy_completed(self) -&gt; bool:\n    \"\"\"Determine if the dag completed all nodes through to the deploy\n    step\n\n    NOTE: An empty node set is considered completed\n\n    Returns:\n        completed:  bool\n            True if there are no failed nodes and no unstarted nodes\n    \"\"\"\n    return not self.failed_nodes and not self.unstarted_nodes\n</code></pre>"},{"location":"API%20References/#oper8.dag.completion_state.CompletionState.failed","title":"<code>failed()</code>","text":"<p>Determine if any of the nodes failed</p> <p>Returns:</p> Name Type Description <code>failed</code> <code>bool</code> <p>bool True if there are any nodes in the failed state or there is a fatal error</p> Source code in <code>oper8/dag/completion_state.py</code> <pre><code>def failed(self) -&gt; bool:\n    \"\"\"Determine if any of the nodes failed\n\n    Returns:\n        failed:  bool\n            True if there are any nodes in the failed state or there is a\n            fatal error\n    \"\"\"\n    return bool(self.failed_nodes) or self._fatal_exception()\n</code></pre>"},{"location":"API%20References/#oper8.dag.completion_state.CompletionState.verify_completed","title":"<code>verify_completed()</code>","text":"<p>Determine if the dag completed all nodes through to the verification step</p> <p>NOTE: An empty node set is considered verified</p> <p>Returns:</p> Name Type Description <code>completed</code> <code>bool</code> <p>bool True if there are no nodes found outside of the verified_nodes and there is no exception in the termination state</p> Source code in <code>oper8/dag/completion_state.py</code> <pre><code>def verify_completed(self) -&gt; bool:\n    \"\"\"Determine if the dag completed all nodes through to the verification\n    step\n\n    NOTE: An empty node set is considered verified\n\n    Returns:\n        completed:  bool\n            True if there are no nodes found outside of the verified_nodes\n            and there is no exception in the termination state\n    \"\"\"\n    return (\n        not self.unverified_nodes\n        and not self.failed_nodes\n        and not self.unstarted_nodes\n        and not self.exception\n    )\n</code></pre>"},{"location":"API%20References/#oper8.dag.graph","title":"<code>graph</code>","text":"<p>Graph holds information about a Directed Acyclic Graph</p>"},{"location":"API%20References/#oper8.dag.graph.Graph","title":"<code>Graph</code>","text":"<p>Class for representing an instance of a Graph. Handles adding and removing nodes as well as graph functions like flattening</p> Source code in <code>oper8/dag/graph.py</code> <pre><code>class Graph:\n    \"\"\"Class for representing an instance of a Graph. Handles adding and removing nodes\n    as well as graph functions like flattening\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.__node_dict = {}\n\n        # Add the root node of the Graph. Every member of this graph is also a child\n        # of the root node\n        self.__root_node = Node()\n        self.__node_dict[self.__root_node.get_name()] = self.__root_node\n\n    ## Properties ##############################################################\n\n    @property\n    def root(self) -&gt; Node:  # pylint: disable=invalid-name\n        \"\"\"The root node of the Graph\"\"\"\n        return self.__root_node\n\n    @property\n    def node_dict(self) -&gt; dict:\n        \"\"\"Dictionary of all node names and their nodes\"\"\"\n        return self.__node_dict\n\n    ## Modifiers ##############################################################\n\n    def add_node(self, node: Node):\n        \"\"\"Add node to graph\n        Args:\n            node:  Node\n                The node to be added to the Dag.\n        \"\"\"\n        if not node.get_name():\n            raise ValueError(\"None is reserved for the root node of the dag Graph\")\n\n        if node.get_name() in self.node_dict:\n            raise ValueError(\n                f\"Only one node with id {node.get_name()} can be added to a Graph\"\n            )\n\n        self.node_dict[node.get_name()] = node\n        self.root.add_child(node)\n\n    def add_node_dependency(\n        self, parent_node: Node, child_node: Node, edge_fn: Optional[Callable] = None\n    ):\n        \"\"\"Add dependency or \"edge\" to graph between two nodes. This is the same\n        as doing parent_node.add_dependency(child_node)\n        Args:\n            parent_node:  Node\n                The parent or dependent node aka the node that must wait\n            child_node: Node\n                The child or dependency node aka the node that must be deployed first\n            edge_fn:\n        \"\"\"\n        if not self.get_node(parent_node.get_name()):\n            raise ValueError(f\"Parent node {parent_node} is not present in Graph\")\n\n        if not self.get_node(child_node.get_name()):\n            raise ValueError(f\"Child node {child_node} is not present in Graph\")\n\n        # Make sure edits are applied to the nodes already present in the graph\n        parent_node = self.get_node(parent_node.get_name())\n        child_node = self.get_node(child_node.get_name())\n\n        parent_node.add_child(child_node, edge_fn)\n\n    ## Accessors ##############################################################\n\n    def get_node(self, name: str):  # pylint: disable=invalid-name\n        \"\"\"Get the node with name\"\"\"\n        return self.node_dict.get(name)\n\n    def get_all_nodes(self):\n        \"\"\"Get list of all nodes\"\"\"\n        return [node for node, _ in self.root.get_children()]\n\n    def has_node(self, node: Node):  # pylint: disable=invalid-name\n        \"\"\"Check if node is in graph\"\"\"\n        return self.root.has_child(node)\n\n    def empty(self):\n        \"\"\"Check if a graph is empty\"\"\"\n        return len(self.root.get_children()) == 0\n\n    ## Graph Functions ##############################################################\n\n    def topology(self) -&gt; List[\"Node\"]:\n        \"\"\"Get a list of nodes in deployment order\"\"\"\n        topology = self.root.topology()\n        topology.remove(self.root)\n        return topology\n\n    ## Internal Functions ##############################################################\n\n    def __repr__(self):\n        str_list = []\n        for child, _ in self.root.get_children():\n            child_str_list = [node.get_name() for node, _ in child.get_children()]\n            str_list.append(f\"{child.get_name()}:[{','.join(child_str_list)}]\")\n\n        return f\"Graph({{{','.join(str_list)}}})\"\n\n    def __contains__(self, item: Node):\n        return self.has_node(item)\n\n    def __iter__(self):\n        \"\"\"Iterate over all child nodes\"\"\"\n        return self.get_all_nodes().__iter__()\n</code></pre>"},{"location":"API%20References/#oper8.dag.graph.Graph.node_dict","title":"<code>node_dict</code>  <code>property</code>","text":"<p>Dictionary of all node names and their nodes</p>"},{"location":"API%20References/#oper8.dag.graph.Graph.root","title":"<code>root</code>  <code>property</code>","text":"<p>The root node of the Graph</p>"},{"location":"API%20References/#oper8.dag.graph.Graph.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over all child nodes</p> Source code in <code>oper8/dag/graph.py</code> <pre><code>def __iter__(self):\n    \"\"\"Iterate over all child nodes\"\"\"\n    return self.get_all_nodes().__iter__()\n</code></pre>"},{"location":"API%20References/#oper8.dag.graph.Graph.add_node","title":"<code>add_node(node)</code>","text":"<p>Add node to graph Args:     node:  Node         The node to be added to the Dag.</p> Source code in <code>oper8/dag/graph.py</code> <pre><code>def add_node(self, node: Node):\n    \"\"\"Add node to graph\n    Args:\n        node:  Node\n            The node to be added to the Dag.\n    \"\"\"\n    if not node.get_name():\n        raise ValueError(\"None is reserved for the root node of the dag Graph\")\n\n    if node.get_name() in self.node_dict:\n        raise ValueError(\n            f\"Only one node with id {node.get_name()} can be added to a Graph\"\n        )\n\n    self.node_dict[node.get_name()] = node\n    self.root.add_child(node)\n</code></pre>"},{"location":"API%20References/#oper8.dag.graph.Graph.add_node_dependency","title":"<code>add_node_dependency(parent_node, child_node, edge_fn=None)</code>","text":"<p>Add dependency or \"edge\" to graph between two nodes. This is the same as doing parent_node.add_dependency(child_node) Args:     parent_node:  Node         The parent or dependent node aka the node that must wait     child_node: Node         The child or dependency node aka the node that must be deployed first     edge_fn:</p> Source code in <code>oper8/dag/graph.py</code> <pre><code>def add_node_dependency(\n    self, parent_node: Node, child_node: Node, edge_fn: Optional[Callable] = None\n):\n    \"\"\"Add dependency or \"edge\" to graph between two nodes. This is the same\n    as doing parent_node.add_dependency(child_node)\n    Args:\n        parent_node:  Node\n            The parent or dependent node aka the node that must wait\n        child_node: Node\n            The child or dependency node aka the node that must be deployed first\n        edge_fn:\n    \"\"\"\n    if not self.get_node(parent_node.get_name()):\n        raise ValueError(f\"Parent node {parent_node} is not present in Graph\")\n\n    if not self.get_node(child_node.get_name()):\n        raise ValueError(f\"Child node {child_node} is not present in Graph\")\n\n    # Make sure edits are applied to the nodes already present in the graph\n    parent_node = self.get_node(parent_node.get_name())\n    child_node = self.get_node(child_node.get_name())\n\n    parent_node.add_child(child_node, edge_fn)\n</code></pre>"},{"location":"API%20References/#oper8.dag.graph.Graph.empty","title":"<code>empty()</code>","text":"<p>Check if a graph is empty</p> Source code in <code>oper8/dag/graph.py</code> <pre><code>def empty(self):\n    \"\"\"Check if a graph is empty\"\"\"\n    return len(self.root.get_children()) == 0\n</code></pre>"},{"location":"API%20References/#oper8.dag.graph.Graph.get_all_nodes","title":"<code>get_all_nodes()</code>","text":"<p>Get list of all nodes</p> Source code in <code>oper8/dag/graph.py</code> <pre><code>def get_all_nodes(self):\n    \"\"\"Get list of all nodes\"\"\"\n    return [node for node, _ in self.root.get_children()]\n</code></pre>"},{"location":"API%20References/#oper8.dag.graph.Graph.get_node","title":"<code>get_node(name)</code>","text":"<p>Get the node with name</p> Source code in <code>oper8/dag/graph.py</code> <pre><code>def get_node(self, name: str):  # pylint: disable=invalid-name\n    \"\"\"Get the node with name\"\"\"\n    return self.node_dict.get(name)\n</code></pre>"},{"location":"API%20References/#oper8.dag.graph.Graph.has_node","title":"<code>has_node(node)</code>","text":"<p>Check if node is in graph</p> Source code in <code>oper8/dag/graph.py</code> <pre><code>def has_node(self, node: Node):  # pylint: disable=invalid-name\n    \"\"\"Check if node is in graph\"\"\"\n    return self.root.has_child(node)\n</code></pre>"},{"location":"API%20References/#oper8.dag.graph.Graph.topology","title":"<code>topology()</code>","text":"<p>Get a list of nodes in deployment order</p> Source code in <code>oper8/dag/graph.py</code> <pre><code>def topology(self) -&gt; List[\"Node\"]:\n    \"\"\"Get a list of nodes in deployment order\"\"\"\n    topology = self.root.topology()\n    topology.remove(self.root)\n    return topology\n</code></pre>"},{"location":"API%20References/#oper8.dag.node","title":"<code>node</code>","text":"<p>This module contains a collection of classes for implementing nodes of a Graph</p>"},{"location":"API%20References/#oper8.dag.node.Node","title":"<code>Node</code>","text":"<p>Class for representing a node in the Graph</p> Source code in <code>oper8/dag/node.py</code> <pre><code>class Node:\n    \"\"\"Class for representing a node in the Graph\"\"\"\n\n    def __init__(\n        self,\n        name: Optional[str] = None,\n        data: Optional[Any] = None,\n    ) -&gt; None:\n        \"\"\"Construct a new Node\n\n        Args:\n            name:  Optional[str]\n                The name of the node\n            data:  Optional[Any]\n                Any data that should be stored with the node\n        \"\"\"\n        self._name = name\n        self._data = data\n        self.children = {}\n\n    ## Modifiers ##############################################################\n    def add_child(self, node: \"Node\", edge_data: Optional[Any] = None):\n        \"\"\"Add edge from  self to node with optional edge data\"\"\"\n        if node.dfs(self):\n            raise ValueError(\"Unable to add cyclic dependency\")\n        self.children[node] = edge_data\n\n    def remove_child(self, node: \"Node\"):\n        \"\"\"Remove child node from self\"\"\"\n        if node in self.children:\n            self.children.pop(node)\n\n    def set_data(self, data: Any):\n        \"\"\"Mutator for node data\"\"\"\n        self._data = data\n\n    ## Accessors ##############################################################\n\n    def get_data(self):\n        \"\"\"Accessor for specific child\"\"\"\n        return self._data\n\n    def get_name(self):\n        \"\"\"Accessor for specific child\"\"\"\n        return self._name\n\n    def has_child(self, node: \"Node\"):\n        \"\"\"Accessor for specific child\"\"\"\n        return node in self.children\n\n    def get_children(self) -&gt; set:\n        \"\"\"Accessor for all children\"\"\"\n        return list(self.children.items())\n\n    ## Graph Functions ##############################################################\n    def topology(self) -&gt; List[\"Node\"]:\n        \"\"\"Function to get an ordered topology of a node's children\"\"\"\n        found = set()\n        topology = []\n\n        def visit(node):\n            for child, _ in sorted(node.get_children()):\n                visit(child)\n\n            if node not in found:\n                topology.append(node)\n                found.add(node)\n\n        visit(self)\n\n        return topology\n\n    def dfs(self, node: \"Node\", visited: List[\"Node\"] = None) -&gt; bool:\n        \"\"\"Function to determine if their is a path between two nodes. Used in acyclic check\"\"\"\n        if not visited:\n            visited = []\n        if node == self:\n            return True\n        visited.append(self)\n        for child, _ in self.get_children():\n            if child not in visited:\n                if child.dfs(node, visited):\n                    return True\n                visited.append(child)\n        return False\n\n    ## Internal ##\n    def __eq__(self, obj):\n        \"\"\"Compare and sort nodes by name\"\"\"\n        if not isinstance(obj, Node):\n            return False\n        return (self.get_name()) == (obj.get_name())\n\n    def __lt__(self, obj):\n        if not isinstance(obj, Node):\n            return False\n        return (self.get_name()) &lt; (obj.get_name())\n\n    def __repr__(self) -&gt; str:\n        # __repr__ may be called before __init__ thus _name is not present\n        if hasattr(self, \"_name\"):\n            return f\"{self.__class__.__name__}('{self.get_name()}', {self.get_data()})\"\n        return super().__repr__()\n\n    def __hash__(self) -&gt; str:\n        return self.get_name().__hash__()\n</code></pre>"},{"location":"API%20References/#oper8.dag.node.Node.__eq__","title":"<code>__eq__(obj)</code>","text":"<p>Compare and sort nodes by name</p> Source code in <code>oper8/dag/node.py</code> <pre><code>def __eq__(self, obj):\n    \"\"\"Compare and sort nodes by name\"\"\"\n    if not isinstance(obj, Node):\n        return False\n    return (self.get_name()) == (obj.get_name())\n</code></pre>"},{"location":"API%20References/#oper8.dag.node.Node.__init__","title":"<code>__init__(name=None, data=None)</code>","text":"<p>Construct a new Node</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Optional[str] The name of the node</p> <code>None</code> <code>data</code> <code>Optional[Any]</code> <p>Optional[Any] Any data that should be stored with the node</p> <code>None</code> Source code in <code>oper8/dag/node.py</code> <pre><code>def __init__(\n    self,\n    name: Optional[str] = None,\n    data: Optional[Any] = None,\n) -&gt; None:\n    \"\"\"Construct a new Node\n\n    Args:\n        name:  Optional[str]\n            The name of the node\n        data:  Optional[Any]\n            Any data that should be stored with the node\n    \"\"\"\n    self._name = name\n    self._data = data\n    self.children = {}\n</code></pre>"},{"location":"API%20References/#oper8.dag.node.Node.add_child","title":"<code>add_child(node, edge_data=None)</code>","text":"<p>Add edge from  self to node with optional edge data</p> Source code in <code>oper8/dag/node.py</code> <pre><code>def add_child(self, node: \"Node\", edge_data: Optional[Any] = None):\n    \"\"\"Add edge from  self to node with optional edge data\"\"\"\n    if node.dfs(self):\n        raise ValueError(\"Unable to add cyclic dependency\")\n    self.children[node] = edge_data\n</code></pre>"},{"location":"API%20References/#oper8.dag.node.Node.dfs","title":"<code>dfs(node, visited=None)</code>","text":"<p>Function to determine if their is a path between two nodes. Used in acyclic check</p> Source code in <code>oper8/dag/node.py</code> <pre><code>def dfs(self, node: \"Node\", visited: List[\"Node\"] = None) -&gt; bool:\n    \"\"\"Function to determine if their is a path between two nodes. Used in acyclic check\"\"\"\n    if not visited:\n        visited = []\n    if node == self:\n        return True\n    visited.append(self)\n    for child, _ in self.get_children():\n        if child not in visited:\n            if child.dfs(node, visited):\n                return True\n            visited.append(child)\n    return False\n</code></pre>"},{"location":"API%20References/#oper8.dag.node.Node.get_children","title":"<code>get_children()</code>","text":"<p>Accessor for all children</p> Source code in <code>oper8/dag/node.py</code> <pre><code>def get_children(self) -&gt; set:\n    \"\"\"Accessor for all children\"\"\"\n    return list(self.children.items())\n</code></pre>"},{"location":"API%20References/#oper8.dag.node.Node.get_data","title":"<code>get_data()</code>","text":"<p>Accessor for specific child</p> Source code in <code>oper8/dag/node.py</code> <pre><code>def get_data(self):\n    \"\"\"Accessor for specific child\"\"\"\n    return self._data\n</code></pre>"},{"location":"API%20References/#oper8.dag.node.Node.get_name","title":"<code>get_name()</code>","text":"<p>Accessor for specific child</p> Source code in <code>oper8/dag/node.py</code> <pre><code>def get_name(self):\n    \"\"\"Accessor for specific child\"\"\"\n    return self._name\n</code></pre>"},{"location":"API%20References/#oper8.dag.node.Node.has_child","title":"<code>has_child(node)</code>","text":"<p>Accessor for specific child</p> Source code in <code>oper8/dag/node.py</code> <pre><code>def has_child(self, node: \"Node\"):\n    \"\"\"Accessor for specific child\"\"\"\n    return node in self.children\n</code></pre>"},{"location":"API%20References/#oper8.dag.node.Node.remove_child","title":"<code>remove_child(node)</code>","text":"<p>Remove child node from self</p> Source code in <code>oper8/dag/node.py</code> <pre><code>def remove_child(self, node: \"Node\"):\n    \"\"\"Remove child node from self\"\"\"\n    if node in self.children:\n        self.children.pop(node)\n</code></pre>"},{"location":"API%20References/#oper8.dag.node.Node.set_data","title":"<code>set_data(data)</code>","text":"<p>Mutator for node data</p> Source code in <code>oper8/dag/node.py</code> <pre><code>def set_data(self, data: Any):\n    \"\"\"Mutator for node data\"\"\"\n    self._data = data\n</code></pre>"},{"location":"API%20References/#oper8.dag.node.Node.topology","title":"<code>topology()</code>","text":"<p>Function to get an ordered topology of a node's children</p> Source code in <code>oper8/dag/node.py</code> <pre><code>def topology(self) -&gt; List[\"Node\"]:\n    \"\"\"Function to get an ordered topology of a node's children\"\"\"\n    found = set()\n    topology = []\n\n    def visit(node):\n        for child, _ in sorted(node.get_children()):\n            visit(child)\n\n        if node not in found:\n            topology.append(node)\n            found.add(node)\n\n    visit(self)\n\n    return topology\n</code></pre>"},{"location":"API%20References/#oper8.dag.node.ResourceNode","title":"<code>ResourceNode</code>","text":"<p>               Bases: <code>Node</code></p> <p>Class for representing a kubernetes resource in the Graph with a function for verifying said resource</p> Source code in <code>oper8/dag/node.py</code> <pre><code>class ResourceNode(Node):\n    \"\"\"Class for representing a kubernetes resource in the Graph with\n    a function for verifying said resource\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        manifest: dict,\n        verify_func: Optional[Callable] = None,\n        deploy_method: Optional[\"DeployMethod\"] = None,  # noqa: F821\n    ):\n        # Override init to require name/manifest parameters\n        super().__init__(name, manifest)\n        self._verify_function = verify_func\n        self._deploy_method = deploy_method\n        if not deploy_method:\n            # Local\n            from ..deploy_manager import DeployMethod\n\n            self._deploy_method = DeployMethod.DEFAULT\n\n    ## ApiObject Parameters and Functions ######################################\n    @property\n    def api_group(self) -&gt; str:\n        \"\"\"The kubernetes apiVersion group name without the schema version\"\"\"\n        return self.api_version.split(\"/\")[0]\n\n    @property\n    def api_version(self) -&gt; str:\n        \"\"\"The full kubernetes apiVersion\"\"\"\n        return self.manifest.get(\"apiVersion\")\n\n    @property\n    def kind(self) -&gt; str:\n        \"\"\"The resource kind\"\"\"\n        return self.manifest.get(\"kind\")\n\n    @property\n    def metadata(self) -&gt; dict:\n        \"\"\"The full resource metadata dict\"\"\"\n        return self.manifest.get(\"metadata\", {})\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"The resource metadata.name\"\"\"\n        return self.metadata.get(\"name\")\n\n    @property\n    def manifest(self) -&gt; dict:\n        \"\"\"The resource manifest\"\"\"\n        return self.get_data()\n\n    @property\n    def verify_function(self) -&gt; Optional[Callable]:\n        \"\"\"The resource manifest\"\"\"\n        return self._verify_function\n\n    @property\n    def deploy_method(self) -&gt; Optional[\"DeployMethod\"]:  # noqa: F821\n        \"\"\"The resource manifest\"\"\"\n        return self._deploy_method\n\n    def add_dependency(self, node: \"ResourceNode\"):\n        \"\"\"Add a child dependency to this node\"\"\"\n        self.add_child(node)\n</code></pre>"},{"location":"API%20References/#oper8.dag.node.ResourceNode.api_group","title":"<code>api_group</code>  <code>property</code>","text":"<p>The kubernetes apiVersion group name without the schema version</p>"},{"location":"API%20References/#oper8.dag.node.ResourceNode.api_version","title":"<code>api_version</code>  <code>property</code>","text":"<p>The full kubernetes apiVersion</p>"},{"location":"API%20References/#oper8.dag.node.ResourceNode.deploy_method","title":"<code>deploy_method</code>  <code>property</code>","text":"<p>The resource manifest</p>"},{"location":"API%20References/#oper8.dag.node.ResourceNode.kind","title":"<code>kind</code>  <code>property</code>","text":"<p>The resource kind</p>"},{"location":"API%20References/#oper8.dag.node.ResourceNode.manifest","title":"<code>manifest</code>  <code>property</code>","text":"<p>The resource manifest</p>"},{"location":"API%20References/#oper8.dag.node.ResourceNode.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>The full resource metadata dict</p>"},{"location":"API%20References/#oper8.dag.node.ResourceNode.name","title":"<code>name</code>  <code>property</code>","text":"<p>The resource metadata.name</p>"},{"location":"API%20References/#oper8.dag.node.ResourceNode.verify_function","title":"<code>verify_function</code>  <code>property</code>","text":"<p>The resource manifest</p>"},{"location":"API%20References/#oper8.dag.node.ResourceNode.add_dependency","title":"<code>add_dependency(node)</code>","text":"<p>Add a child dependency to this node</p> Source code in <code>oper8/dag/node.py</code> <pre><code>def add_dependency(self, node: \"ResourceNode\"):\n    \"\"\"Add a child dependency to this node\"\"\"\n    self.add_child(node)\n</code></pre>"},{"location":"API%20References/#oper8.dag.runner","title":"<code>runner</code>","text":"<p>This module contains a collection of classes for executing functions along a DAG</p>"},{"location":"API%20References/#oper8.dag.runner.DagHaltError","title":"<code>DagHaltError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception used to indicate that a Runner execution should halt</p> Source code in <code>oper8/dag/runner.py</code> <pre><code>class DagHaltError(Exception):\n    \"\"\"Custom exception used to indicate that a Runner execution should halt\"\"\"\n\n    def __init__(\n        self,\n        failure: bool,\n        exception: Exception = None,\n    ):\n        super().__init__()\n        self.failure = failure\n        self.exception = exception\n</code></pre>"},{"location":"API%20References/#oper8.dag.runner.NonThreadPoolExecutor","title":"<code>NonThreadPoolExecutor</code>","text":"<p>               Bases: <code>Executor</code></p> <p>This \"pool\" implements the Executor interfaces, but runs without any threads. This is used when running a Runner without cocurrency</p> Source code in <code>oper8/dag/runner.py</code> <pre><code>class NonThreadPoolExecutor(Executor):\n    \"\"\"This \"pool\" implements the Executor interfaces, but runs without any\n    threads. This is used when running a Runner without cocurrency\n    \"\"\"\n\n    def __init__(self, *_, **__):\n        \"\"\"Swallow constructor args so that it can match ThreadPoolExecutor\"\"\"\n        super().__init__()\n\n    @staticmethod\n    def submit(fn: Callable, /, *args, **kwargs):\n        \"\"\"Run the function immediately and return a pre-completed Future\"\"\"\n        fut = Future()\n        fut.set_result(fn(*args, **kwargs))\n        return fut\n\n    @staticmethod\n    def shutdown(*_, **__):\n        \"\"\"Nothing to do since this is not a real pool\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.dag.runner.NonThreadPoolExecutor.__init__","title":"<code>__init__(*_, **__)</code>","text":"<p>Swallow constructor args so that it can match ThreadPoolExecutor</p> Source code in <code>oper8/dag/runner.py</code> <pre><code>def __init__(self, *_, **__):\n    \"\"\"Swallow constructor args so that it can match ThreadPoolExecutor\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"API%20References/#oper8.dag.runner.NonThreadPoolExecutor.shutdown","title":"<code>shutdown(*_, **__)</code>  <code>staticmethod</code>","text":"<p>Nothing to do since this is not a real pool</p> Source code in <code>oper8/dag/runner.py</code> <pre><code>@staticmethod\ndef shutdown(*_, **__):\n    \"\"\"Nothing to do since this is not a real pool\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.dag.runner.NonThreadPoolExecutor.submit","title":"<code>submit(fn, /, *args, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Run the function immediately and return a pre-completed Future</p> Source code in <code>oper8/dag/runner.py</code> <pre><code>@staticmethod\ndef submit(fn: Callable, /, *args, **kwargs):\n    \"\"\"Run the function immediately and return a pre-completed Future\"\"\"\n    fut = Future()\n    fut.set_result(fn(*args, **kwargs))\n    return fut\n</code></pre>"},{"location":"API%20References/#oper8.dag.runner.Runner","title":"<code>Runner</code>","text":"<p>This is a very simple \"keep running until done\" Runner executor which uses a ThreadPoolExecutor to allow non-blocking calls to execute in parallel.</p> Source code in <code>oper8/dag/runner.py</code> <pre><code>class Runner:  # pylint: disable=too-many-instance-attributes\n    \"\"\"This is a very simple \"keep running until done\" Runner executor which uses\n    a ThreadPoolExecutor to allow non-blocking calls to execute in parallel.\n    \"\"\"\n\n    @property\n    def graph(self) -&gt; str:  # pylint: disable=missing-function-docstring\n        return self._graph\n\n    def __init__(  # pylint: disable=too-many-arguments\n        self,\n        name: str = \"\",\n        threads: Optional[int] = None,\n        graph: Optional[Graph] = None,\n        default_function: Optional[Callable[[\"Node\"], bool]] = None,\n        poll_time: float = 0.05,\n        verify_upstream: bool = True,\n    ):\n        \"\"\"Construct a Runner which will manage the execution of a single Graph\n\n        Args:\n            name:  str\n                String name that can be used for logging to differentiate deploy\n                and verify Graph executions\n            threads:  Optional[int]\n                Number of threads to use. If not given, the default behavior of\n                ThreadPoolExecutor is to use the number of available cores.\n            graph:  Optional[Graph]\n                Existing graph to use, if not supplied an empty graph is created\n            default_function: Optional[Callable[[\"Node\"],None]]=None\n                Function that will be called on node run if specific function is not provided\n            poll_time:  float\n                How often to check runner status\n            verify_upstream: bool\n                Conditional to control whether to check a nodes upstream via its edge function\n        \"\"\"\n        self.name = name\n        # If threads are disabled, use the NonThreadPoolExecutor\n        if threads == 0:\n            log.debug(\"Running without threading\")\n            pool_type = NonThreadPoolExecutor\n        else:\n            log.debug(\"Running with %s threads\", threads)\n            pool_type = ThreadPoolExecutor\n        self._pool = pool_type(max_workers=threads)\n        self._graph = graph or Graph()\n        self._default_node_func = default_function or (lambda _: None)\n\n        self._failed = False\n        self._exception = None\n        self._verify_upstream = verify_upstream\n        self._poll_time = poll_time\n        self._started_nodes = []\n        self._disabled_nodes = []\n\n        # Nodes can terminate in one of three states:\n        #   1. Completed and verified\n        #   2. Completed, but not verified\n        #   3. Failed\n        self._verified_nodes = []\n        self._unverified_nodes = []\n        self._failed_nodes = []\n\n    ## Public ##\n\n    def disable_node(\n        self,\n        node: \"Node\",\n    ):\n        \"\"\"Function to disable a node in the graph. This will skip the node in runner without\n        changing the graph\"\"\"\n        graph_node = self.graph.get_node(node.get_name())\n        if graph_node:\n            self._disabled_nodes.append(graph_node)\n\n    def enable_node(\n        self,\n        node: \"Node\",\n    ):\n        \"\"\"Function to reenable a node after it was disabled by Runner.disable_node\"\"\"\n        graph_node = self.graph.get_node(node.get_name())\n        if graph_node in self._disabled_nodes:\n            self._disabled_nodes.remove(graph_node)\n\n    def completion_state(self):\n        \"\"\"Get the state of which nodes completed and which failed\n\n        Returns:\n            completion_state:  CompletionState\n                The state holding the full view of the termination state of each\n                node\n        \"\"\"\n        return CompletionState(\n            verified_nodes=self._verified_nodes,\n            unverified_nodes=self._unverified_nodes,\n            failed_nodes=self._failed_nodes,\n            unstarted_nodes=[\n                node\n                for node in self.graph.get_all_nodes()\n                if node not in self._get_completed_nodes()\n            ],\n            exception=self._exception,\n        )\n\n    def run(self):\n        \"\"\"Run the Runner! This will continue until the graph has run to completion\n        or halted due to an error.\n        \"\"\"\n        node_list = self._get_runnable_nodes()\n        log.debug3(\n            \"Started Nodes: %s, All Nodes: %s\",\n            self._started_nodes,\n            list(node_list),\n        )\n\n        # The \"node start\" loop should terminate if:\n        # 1. All nodes have started\n        # 2. All started nodes have completed in one form or another and there\n        #   are no newly ready nodes\n        while len(self._started_nodes) &lt; len(node_list):\n            # Get the set of nodes that has completed already\n            #\n            # NOTE: It's _critically_ important that this be done before getting\n            #   the ready nodes. The operation of getting ready nodes can\n            #   delegate to user-defined verification functions which may be\n            #   very slow and IO bound. With slow verification functions, a node\n            #   running in a thread may complete and mark itself verified after\n            #   a downstream dependency has checked its completion status, but\n            #   before the full set of _get_ready_nodes() checks has passed. If\n            #   this happens and _get_completed_nodes() is called afterwards,\n            #   the short-circuit logic below will think that all started nodes\n            #   have completed and there are no ready nodes, thus terminating\n            #   the Runner prematurely.\n            completed_nodes = self._get_completed_nodes()\n\n            # Get the currently ready nodes\n            ready_nodes = self._get_ready_nodes()\n\n            # If there are no ready nodes and all started nodes have completed\n            # in one way or another, we're in an early termination case\n            log.debug4(\"Ready Nodes: %s\", ready_nodes)\n            log.debug4(\"Completed Nodes: %s\", completed_nodes)\n            if not ready_nodes and set(self._started_nodes) == set(completed_nodes):\n                log.debug2(\n                    \"[%s] Graph exhausted all available nodes. Terminating early.\",\n                    self.name,\n                )\n                break\n\n            # If there are new ready nodes, start them\n            if ready_nodes:\n                log.debug2(\n                    \"Ready nodes: %s. Remaining nodes: %s\",\n                    ready_nodes,\n                    [\n                        node\n                        for node in node_list\n                        if node not in ready_nodes and node not in completed_nodes\n                    ],\n                )\n            for ready_node in ready_nodes:\n                self._started_nodes.append(ready_node)\n                self._pool.submit(self._run_node, ready_node)\n            time.sleep(self._poll_time)\n\n        # Log out the state of the graph once we've terminated, but before we've\n        # waited for all nodes to terminate\n        log.debug2(\"[NODES] Started: %s\", sorted(self._started_nodes))\n        log.debug2(\"[NODES] Verified: %s\", sorted(self._verified_nodes))\n        log.debug2(\"[NODES] Unverified: %s\", sorted(self._unverified_nodes))\n        log.debug2(\"[NODES] Failed: %s\", sorted(self._failed_nodes))\n        log.debug2(\"[NODES] All: %s\", sorted(list(node_list)))\n\n        # Wait until all started nodes have finished one way or the other\n        while len(self._get_completed_nodes()) != len(self._started_nodes):\n            time.sleep(self._poll_time)\n\n        # Make sure any in-flight nodes complete before terminating\n        log.debug2(\"Waiting for in-flight nodes to complete\")\n        self._pool.shutdown()\n        log.debug2(\"All nodes complete\")\n        log.debug2(self.completion_state())\n\n    ## Implementation Details ##\n\n    def _run_node(self, node: \"Node\"):\n        node_name = node.get_name()\n        log.debug2(\"Starting node: %s\", node_name)\n\n        try:\n            # Call node function or default\n            node_func = node.get_data()\n            if callable(node_func):\n                node_func()\n            else:\n                self._default_node_func(node)\n\n        except DagHaltError as err:\n            log.debug(\"[%s] DagHaltError caught. Stopping Execution\", self.name)\n            self._failed = err.failure\n            self._exception = err.exception\n            if err.failure:\n                self._failed_nodes.append(node)\n            else:\n                self._unverified_nodes.append(node)\n        except Exception as err:  # pylint: disable=broad-except\n            log.warning(\n                \"Unexpected exception caught in Runner node: %s\", err, exc_info=True\n            )\n            self._failed = True\n            self._failed_nodes.append(node)\n        else:\n            log.debug2(\"Node complete: %s\", node_name)\n            self._verified_nodes.append(node)\n\n    def _dependency_satisfied(self, dep: \"Node\", verify_fn: Callable = None) -&gt; bool:\n        \"\"\"\n        Check if the given dependency is satisfied. A dependency is satisfied if:\n        - a) The upstream has been deployed and no verification function `verify_fn` is\n           given for the dependency\n        - b) The upstream has been deployed and the given verification\n           function `verify_fn` passes\n\n        Parameters:\n        dep (Node): The dependency to check.\n        verify_fn (Callable, optional): A verification function to call. Defaults to None.\n\n        Returns:\n        bool: True if the dependency is satisfied, False otherwise.\n        \"\"\"\n        dep_name = dep.get_name()\n        if dep not in self._verified_nodes:\n            log.debug4(\"%s not yet verified\", dep_name)\n            return False\n\n        # Runner initialized with upstream verification disabled.\n        if not self._verify_upstream:\n            log.debug3(\"%s verified without checking\", dep_name)\n            return True\n\n        # Run the custom verification function if provided and return the result.\n        if verify_fn is None:\n            log.debug4(\"%s verified with no verify_fn\", dep_name)\n            return True\n        log.debug4(\"%s calling verify_fn\", dep_name)\n        satisfied = verify_fn()\n        log.debug4(\"%s verify_fn() -&gt; %s\", dep_name, satisfied)\n        return satisfied\n\n    def _get_ready_nodes(self) -&gt; List[str]:\n        \"\"\"\n        Get the list of nodes whose dependencies (upstream) are satisfied and ready to be executed,\n\n        Returns:\n            List[str]: A list of node names that are ready to be executed.\n        \"\"\"\n        ready_nodes = []\n        for node in [\n            n for n in self._get_runnable_nodes() if n not in self._started_nodes\n        ]:\n            node_name = node.get_name()\n            log.debug4(\"Checking if %s is ready\", node_name)\n            node_deps = node.get_children()\n            satisfied_dependencies = [\n                (self._dependency_satisfied(dep, verify_fn), dep)\n                for dep, verify_fn in node_deps\n            ]\n\n            # If all dependencies are satisfied, the node is ready.\n            if all(res[0] for res in satisfied_dependencies):\n                ready_nodes.append(node)\n\n            # Otherwise wait for the dependencies (upstream) to be satisfied.\n            else:\n                log.debug3(\n                    \"[%s] waiting on upstreams: %s\",\n                    node_name,\n                    [res[0] for res in satisfied_dependencies if not res[0]],\n                )\n        return ready_nodes\n\n    def _get_completed_nodes(self) -&gt; List[str]:\n        return self._verified_nodes + self._unverified_nodes + self._failed_nodes\n\n    def _get_runnable_nodes(self) -&gt; List[Node]:\n        return set(self.graph.get_all_nodes()) - set(self._disabled_nodes)\n</code></pre>"},{"location":"API%20References/#oper8.dag.runner.Runner.__init__","title":"<code>__init__(name='', threads=None, graph=None, default_function=None, poll_time=0.05, verify_upstream=True)</code>","text":"<p>Construct a Runner which will manage the execution of a single Graph</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>str String name that can be used for logging to differentiate deploy and verify Graph executions</p> <code>''</code> <code>threads</code> <code>Optional[int]</code> <p>Optional[int] Number of threads to use. If not given, the default behavior of ThreadPoolExecutor is to use the number of available cores.</p> <code>None</code> <code>graph</code> <code>Optional[Graph]</code> <p>Optional[Graph] Existing graph to use, if not supplied an empty graph is created</p> <code>None</code> <code>default_function</code> <code>Optional[Callable[[Node], bool]]</code> <p>Optional[Callable[[\"Node\"],None]]=None Function that will be called on node run if specific function is not provided</p> <code>None</code> <code>poll_time</code> <code>float</code> <p>float How often to check runner status</p> <code>0.05</code> <code>verify_upstream</code> <code>bool</code> <p>bool Conditional to control whether to check a nodes upstream via its edge function</p> <code>True</code> Source code in <code>oper8/dag/runner.py</code> <pre><code>def __init__(  # pylint: disable=too-many-arguments\n    self,\n    name: str = \"\",\n    threads: Optional[int] = None,\n    graph: Optional[Graph] = None,\n    default_function: Optional[Callable[[\"Node\"], bool]] = None,\n    poll_time: float = 0.05,\n    verify_upstream: bool = True,\n):\n    \"\"\"Construct a Runner which will manage the execution of a single Graph\n\n    Args:\n        name:  str\n            String name that can be used for logging to differentiate deploy\n            and verify Graph executions\n        threads:  Optional[int]\n            Number of threads to use. If not given, the default behavior of\n            ThreadPoolExecutor is to use the number of available cores.\n        graph:  Optional[Graph]\n            Existing graph to use, if not supplied an empty graph is created\n        default_function: Optional[Callable[[\"Node\"],None]]=None\n            Function that will be called on node run if specific function is not provided\n        poll_time:  float\n            How often to check runner status\n        verify_upstream: bool\n            Conditional to control whether to check a nodes upstream via its edge function\n    \"\"\"\n    self.name = name\n    # If threads are disabled, use the NonThreadPoolExecutor\n    if threads == 0:\n        log.debug(\"Running without threading\")\n        pool_type = NonThreadPoolExecutor\n    else:\n        log.debug(\"Running with %s threads\", threads)\n        pool_type = ThreadPoolExecutor\n    self._pool = pool_type(max_workers=threads)\n    self._graph = graph or Graph()\n    self._default_node_func = default_function or (lambda _: None)\n\n    self._failed = False\n    self._exception = None\n    self._verify_upstream = verify_upstream\n    self._poll_time = poll_time\n    self._started_nodes = []\n    self._disabled_nodes = []\n\n    # Nodes can terminate in one of three states:\n    #   1. Completed and verified\n    #   2. Completed, but not verified\n    #   3. Failed\n    self._verified_nodes = []\n    self._unverified_nodes = []\n    self._failed_nodes = []\n</code></pre>"},{"location":"API%20References/#oper8.dag.runner.Runner.completion_state","title":"<code>completion_state()</code>","text":"<p>Get the state of which nodes completed and which failed</p> <p>Returns:</p> Name Type Description <code>completion_state</code> <p>CompletionState The state holding the full view of the termination state of each node</p> Source code in <code>oper8/dag/runner.py</code> <pre><code>def completion_state(self):\n    \"\"\"Get the state of which nodes completed and which failed\n\n    Returns:\n        completion_state:  CompletionState\n            The state holding the full view of the termination state of each\n            node\n    \"\"\"\n    return CompletionState(\n        verified_nodes=self._verified_nodes,\n        unverified_nodes=self._unverified_nodes,\n        failed_nodes=self._failed_nodes,\n        unstarted_nodes=[\n            node\n            for node in self.graph.get_all_nodes()\n            if node not in self._get_completed_nodes()\n        ],\n        exception=self._exception,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.dag.runner.Runner.disable_node","title":"<code>disable_node(node)</code>","text":"<p>Function to disable a node in the graph. This will skip the node in runner without changing the graph</p> Source code in <code>oper8/dag/runner.py</code> <pre><code>def disable_node(\n    self,\n    node: \"Node\",\n):\n    \"\"\"Function to disable a node in the graph. This will skip the node in runner without\n    changing the graph\"\"\"\n    graph_node = self.graph.get_node(node.get_name())\n    if graph_node:\n        self._disabled_nodes.append(graph_node)\n</code></pre>"},{"location":"API%20References/#oper8.dag.runner.Runner.enable_node","title":"<code>enable_node(node)</code>","text":"<p>Function to reenable a node after it was disabled by Runner.disable_node</p> Source code in <code>oper8/dag/runner.py</code> <pre><code>def enable_node(\n    self,\n    node: \"Node\",\n):\n    \"\"\"Function to reenable a node after it was disabled by Runner.disable_node\"\"\"\n    graph_node = self.graph.get_node(node.get_name())\n    if graph_node in self._disabled_nodes:\n        self._disabled_nodes.remove(graph_node)\n</code></pre>"},{"location":"API%20References/#oper8.dag.runner.Runner.run","title":"<code>run()</code>","text":"<p>Run the Runner! This will continue until the graph has run to completion or halted due to an error.</p> Source code in <code>oper8/dag/runner.py</code> <pre><code>def run(self):\n    \"\"\"Run the Runner! This will continue until the graph has run to completion\n    or halted due to an error.\n    \"\"\"\n    node_list = self._get_runnable_nodes()\n    log.debug3(\n        \"Started Nodes: %s, All Nodes: %s\",\n        self._started_nodes,\n        list(node_list),\n    )\n\n    # The \"node start\" loop should terminate if:\n    # 1. All nodes have started\n    # 2. All started nodes have completed in one form or another and there\n    #   are no newly ready nodes\n    while len(self._started_nodes) &lt; len(node_list):\n        # Get the set of nodes that has completed already\n        #\n        # NOTE: It's _critically_ important that this be done before getting\n        #   the ready nodes. The operation of getting ready nodes can\n        #   delegate to user-defined verification functions which may be\n        #   very slow and IO bound. With slow verification functions, a node\n        #   running in a thread may complete and mark itself verified after\n        #   a downstream dependency has checked its completion status, but\n        #   before the full set of _get_ready_nodes() checks has passed. If\n        #   this happens and _get_completed_nodes() is called afterwards,\n        #   the short-circuit logic below will think that all started nodes\n        #   have completed and there are no ready nodes, thus terminating\n        #   the Runner prematurely.\n        completed_nodes = self._get_completed_nodes()\n\n        # Get the currently ready nodes\n        ready_nodes = self._get_ready_nodes()\n\n        # If there are no ready nodes and all started nodes have completed\n        # in one way or another, we're in an early termination case\n        log.debug4(\"Ready Nodes: %s\", ready_nodes)\n        log.debug4(\"Completed Nodes: %s\", completed_nodes)\n        if not ready_nodes and set(self._started_nodes) == set(completed_nodes):\n            log.debug2(\n                \"[%s] Graph exhausted all available nodes. Terminating early.\",\n                self.name,\n            )\n            break\n\n        # If there are new ready nodes, start them\n        if ready_nodes:\n            log.debug2(\n                \"Ready nodes: %s. Remaining nodes: %s\",\n                ready_nodes,\n                [\n                    node\n                    for node in node_list\n                    if node not in ready_nodes and node not in completed_nodes\n                ],\n            )\n        for ready_node in ready_nodes:\n            self._started_nodes.append(ready_node)\n            self._pool.submit(self._run_node, ready_node)\n        time.sleep(self._poll_time)\n\n    # Log out the state of the graph once we've terminated, but before we've\n    # waited for all nodes to terminate\n    log.debug2(\"[NODES] Started: %s\", sorted(self._started_nodes))\n    log.debug2(\"[NODES] Verified: %s\", sorted(self._verified_nodes))\n    log.debug2(\"[NODES] Unverified: %s\", sorted(self._unverified_nodes))\n    log.debug2(\"[NODES] Failed: %s\", sorted(self._failed_nodes))\n    log.debug2(\"[NODES] All: %s\", sorted(list(node_list)))\n\n    # Wait until all started nodes have finished one way or the other\n    while len(self._get_completed_nodes()) != len(self._started_nodes):\n        time.sleep(self._poll_time)\n\n    # Make sure any in-flight nodes complete before terminating\n    log.debug2(\"Waiting for in-flight nodes to complete\")\n    self._pool.shutdown()\n    log.debug2(\"All nodes complete\")\n    log.debug2(self.completion_state())\n</code></pre>"},{"location":"API%20References/#oper8.decorator","title":"<code>decorator</code>","text":"<p>Decorator for making the authoring of \"pure\" components easier</p>"},{"location":"API%20References/#oper8.decorator.component","title":"<code>component(name)</code>","text":"<p>The @component decorator is the primary entrypoint for creating an oper8.Component. It ensures the wrapped type's interface matches the expected Component interface, including the \"name\" class attribute.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>str The name string will be set as the class property for the wrapped class</p> required <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable[[Type], Type]</code> <p>Callable[[Type[Component]], Type[Component]] The decorator function that will be invoked on construction of decorated classes</p> Source code in <code>oper8/decorator.py</code> <pre><code>def component(name: str) -&gt; Callable[[Type], Type]:\n    \"\"\"The @component decorator is the primary entrypoint for creating an\n    oper8.Component. It ensures the wrapped type's interface matches the expected\n    Component interface, including the \"name\" class attribute.\n\n    Args:\n        name:  str\n            The name string will be set as the class property for the wrapped\n            class\n\n    Returns:\n        decorator:  Callable[[Type[Component]], Type[Component]]\n            The decorator function that will be invoked on construction of\n            decorated classes\n    \"\"\"\n\n    def decorator(cls: Type[Component]) -&gt; Type[Component]:\n        cls.name = name\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"API%20References/#oper8.decorator.controller","title":"<code>controller(group, version, kind, finalizer=None, extra_properties=None)</code>","text":"<p>The @controller decorator is the primary entrypoint for creating an oper8.Controller. It ensures the wrapped type's interface matches the required Controller interface, including class properties.</p> The <code>extra_properties</code> argument is an entrypoint for loosely coupled <p>Controller-specific configuration that is tied to the specific WatchManager implementation being used. The current list of useful properties is:</p> <ul> <li>disable_vcs: This can be used to tell the AnsibleWatchManager     that the Controller will not use ansible-vcs, even if other     Controllers managed by the same operator do.</li> <li>pwm_filters: This can be used to tell the PythonWatchManager of any     additional watch filters. If value is a list then the filters are added     to all watches including dependent watches. If value is a dict than     it expects the keys to be the resource global id with the values being a list     of filters for that resource</li> <li>pwm_subsystems: This can be used to tell the PythonWatchManager of any     subsystem relations. This allows a \"subsystem\" controller to be ran during     the reconciliation of another similar to the DryRunWatchManager</li> </ul> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>str</code> <p>str The apiVersion group for the resource this controller manages</p> required <code>version</code> <code>str</code> <p>str The apiVersion version for the resource this controller manages</p> required <code>kind</code> <code>str</code> <p>str The kind for the resource this controller manages</p> required <code>extra_properties</code> <code>Optional[Dict[str, any]]</code> <p>Optional[Dict[str, any]] Extra properties that should be defined as class-properties for this controller</p> <code>None</code> <p>Returns:</p> Name Type Description <code>decorator</code> <code>Callable[[Type[Controller]], Type[Controller]]</code> <p>Callable[[Type[Controller]], Type[Controller]] The decorator function that will be invoked on construction of decorated classes</p> Source code in <code>oper8/decorator.py</code> <pre><code>def controller(  # pylint: disable=too-many-arguments\n    group: str,\n    version: str,\n    kind: str,\n    finalizer: str = None,\n    extra_properties: Optional[Dict[str, any]] = None,\n) -&gt; Callable[[Type[Controller]], Type[Controller]]:\n    \"\"\"The @controller decorator is the primary entrypoint for creating an\n    oper8.Controller. It ensures the wrapped type's interface matches the\n    required Controller interface, including class properties.\n\n    NOTE: The `extra_properties` argument is an entrypoint for loosely coupled\n        Controller-specific configuration that is tied to the specific\n        WatchManager implementation being used. The current list of useful\n        properties is:\n\n        * disable_vcs: This can be used to tell the AnsibleWatchManager\n            that the Controller will not use ansible-vcs, even if other\n            Controllers managed by the same operator do.\n        * pwm_filters: This can be used to tell the PythonWatchManager of any\n            additional watch filters. If value is a list then the filters are added\n            to all watches including dependent watches. If value is a dict than\n            it expects the keys to be the resource global id with the values being a list\n            of filters for that resource\n        * pwm_subsystems: This can be used to tell the PythonWatchManager of any\n            subsystem relations. This allows a \"subsystem\" controller to be ran during\n            the reconciliation of another similar to the DryRunWatchManager\n\n    Args:\n        group:  str\n            The apiVersion group for the resource this controller manages\n        version:  str\n            The apiVersion version for the resource this controller manages\n        kind:  str\n            The kind for the resource this controller manages\n        extra_properties:  Optional[Dict[str, any]]\n            Extra properties that should be defined as class-properties for this\n            controller\n\n    Returns:\n        decorator:  Callable[[Type[Controller]], Type[Controller]]\n            The decorator function that will be invoked on construction of\n            decorated classes\n    \"\"\"\n\n    def decorator(cls: Type[Controller]) -&gt; Type[Controller]:\n        cls.group = group\n        cls.version = version\n        cls.kind = kind\n        for key, val in (extra_properties or {}).items():\n            setattr(cls, key, val)\n        if finalizer is not None:\n            cls.finalizer = finalizer\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager","title":"<code>deploy_manager</code>","text":"<p>The DeployManager is the abstraction in charge of interacting with the kubernetes cluster to deploy, look up, and delete resources.</p>"},{"location":"API%20References/#oper8.deploy_manager.base","title":"<code>base</code>","text":"<p>This defines the base class for all DeployManager types.</p>"},{"location":"API%20References/#oper8.deploy_manager.base.DeployManagerBase","title":"<code>DeployManagerBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for deploy managers which will be responsible for carrying out the actual deploy of an Application/Component.</p> Source code in <code>oper8/deploy_manager/base.py</code> <pre><code>class DeployManagerBase(abc.ABC):\n    \"\"\"\n    Base class for deploy managers which will be responsible for carrying out\n    the actual deploy of an Application/Component.\n    \"\"\"\n\n    @abc.abstractmethod\n    def deploy(\n        self,\n        resource_definitions: List[dict],\n        manage_owner_references: bool = True,\n        method: DeployMethod = DeployMethod.DEFAULT,\n    ) -&gt; Tuple[bool, bool]:\n        \"\"\"The deploy function ensures that the resources defined in the list of\n        definitions are deployed in the cluster.\n\n        Args:\n            resource_definitions:  list(dict)\n                List of resource object dicts to apply to the cluster\n            manage_owner_references:  bool\n                If true, ownerReferences for the parent CR will be applied to\n                the deployed object\n\n        Returns:\n            success:  bool\n                Whether or not the deploy succeeded\n            changed:  bool\n                Whether or not the deployment resulted in changes\n        \"\"\"\n\n    @abc.abstractmethod\n    def disable(self, resource_definitions: List[dict]) -&gt; Tuple[bool, bool]:\n        \"\"\"The disable function ensures that the resources defined in the list of\n        definitions are deleted from the cluster\n\n        Args:\n            resource_definitions:  list(dict)\n                List of resource object dicts to apply to the cluster\n\n        Returns:\n            success:  bool\n                Whether or not the delete succeeded\n            changed:  bool\n                Whether or not the delete resulted in changes\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_object_current_state(\n        self,\n        kind: str,\n        name: str,\n        namespace: Optional[str] = None,\n        api_version: Optional[str] = None,\n    ) -&gt; Tuple[bool, dict]:\n        \"\"\"The get_current_objects function fetches the current state of a given\n        object by name\n\n        Args:\n            kind:  str\n                The kind of the object to fetch\n            name:  str\n                The full name of the object to fetch\n            namespace:  str\n                The namespace to search for the object\n            api_version:  str\n                The api_version of the resource kind to fetch\n\n        Returns:\n            success:  bool\n                Whether or not the state fetch operation succeeded\n            current_state:  dict or None\n                The dict representation of the current object's configuration,\n                or None if not present\n        \"\"\"\n\n    @abc.abstractmethod\n    def watch_objects(  # pylint: disable=too-many-arguments\n        self,\n        kind: str,\n        api_version: Optional[str] = None,\n        namespace: Optional[str] = None,\n        name: Optional[str] = None,\n        label_selector: Optional[str] = None,\n        field_selector: Optional[str] = None,\n        resource_version: Optional[str] = None,\n    ) -&gt; Iterator[KubeWatchEvent]:\n        \"\"\"The watch_objects function listens for changes in the cluster and returns a\n        stream of KubeWatchEvents\n\n        Args:\n            kind:  str\n                The kind of the object to fetch\n            namespace:  str\n                The namespace to search for the object\n            name:  str\n                The name to search for the object\n            api_version:  str\n                The api_version of the resource kind to fetch\n            label_selector:  str\n                The label_selector to filter the resources\n            field_selector:  str\n                The field_selector to filter the resources\n            resource_version:  str\n                The resource_version the resource must be newer than\n\n        Returns:\n            watch_stream: Generator[KubeWatchEvent]\n                A stream of KubeWatchEvents generated while watching\n        \"\"\"\n\n    @abc.abstractmethod\n    def filter_objects_current_state(  # pylint: disable=too-many-arguments\n        self,\n        kind: str,\n        namespace: Optional[str] = None,\n        api_version: Optional[str] = None,\n        label_selector: Optional[str] = None,\n        field_selector: Optional[str] = None,\n    ) -&gt; Tuple[bool, List[dict]]:\n        \"\"\"The filter_objects_current_state function fetches a list of objects\n        that match either/both the label or field selector\n        Args:\n            kind:  str\n                The kind of the object to fetch\n            namespace:  str\n                The namespace to search for the object\n            api_version:  str\n                The api_version of the resource kind to fetch\n            label_selector:  str\n                The label_selector to filter the resources\n            field_selector:  str\n                The field_selector to filter the resources\n\n        Returns:\n            success:  bool\n                Whether or not the state fetch operation succeeded\n            current_state:  List[dict]\n                A list of  dict representations for the objects configuration,\n                or an empty list if no objects match\n        \"\"\"\n\n    @abc.abstractmethod\n    def set_status(  # pylint: disable=too-many-arguments\n        self,\n        kind: str,\n        name: str,\n        namespace: Optional[str],\n        status: dict,\n        api_version: Optional[str] = None,\n    ) -&gt; Tuple[bool, bool]:\n        \"\"\"Set the status for an object managed by oper8\n\n        Args:\n            kind:  str\n                The kind of the object ot fetch\n            name:  str\n                The full name of the object to fetch\n            namespace:  Optional[str]\n                The namespace to search for the object. If None search cluster wide\n            status:  dict\n                The status object to set onto the given object\n            api_version:  str\n                The api_version of the resource to update\n\n        Returns:\n            success:  bool\n                Whether or not the state fetch operation succeeded\n            changed:  bool\n                Whether or not the status update resulted in a change\n        \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.base.DeployManagerBase.deploy","title":"<code>deploy(resource_definitions, manage_owner_references=True, method=DeployMethod.DEFAULT)</code>  <code>abstractmethod</code>","text":"<p>The deploy function ensures that the resources defined in the list of definitions are deployed in the cluster.</p> <p>Parameters:</p> Name Type Description Default <code>resource_definitions</code> <code>List[dict]</code> <p>list(dict) List of resource object dicts to apply to the cluster</p> required <code>manage_owner_references</code> <code>bool</code> <p>bool If true, ownerReferences for the parent CR will be applied to the deployed object</p> <code>True</code> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool Whether or not the deploy succeeded</p> <code>changed</code> <code>bool</code> <p>bool Whether or not the deployment resulted in changes</p> Source code in <code>oper8/deploy_manager/base.py</code> <pre><code>@abc.abstractmethod\ndef deploy(\n    self,\n    resource_definitions: List[dict],\n    manage_owner_references: bool = True,\n    method: DeployMethod = DeployMethod.DEFAULT,\n) -&gt; Tuple[bool, bool]:\n    \"\"\"The deploy function ensures that the resources defined in the list of\n    definitions are deployed in the cluster.\n\n    Args:\n        resource_definitions:  list(dict)\n            List of resource object dicts to apply to the cluster\n        manage_owner_references:  bool\n            If true, ownerReferences for the parent CR will be applied to\n            the deployed object\n\n    Returns:\n        success:  bool\n            Whether or not the deploy succeeded\n        changed:  bool\n            Whether or not the deployment resulted in changes\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.base.DeployManagerBase.disable","title":"<code>disable(resource_definitions)</code>  <code>abstractmethod</code>","text":"<p>The disable function ensures that the resources defined in the list of definitions are deleted from the cluster</p> <p>Parameters:</p> Name Type Description Default <code>resource_definitions</code> <code>List[dict]</code> <p>list(dict) List of resource object dicts to apply to the cluster</p> required <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool Whether or not the delete succeeded</p> <code>changed</code> <code>bool</code> <p>bool Whether or not the delete resulted in changes</p> Source code in <code>oper8/deploy_manager/base.py</code> <pre><code>@abc.abstractmethod\ndef disable(self, resource_definitions: List[dict]) -&gt; Tuple[bool, bool]:\n    \"\"\"The disable function ensures that the resources defined in the list of\n    definitions are deleted from the cluster\n\n    Args:\n        resource_definitions:  list(dict)\n            List of resource object dicts to apply to the cluster\n\n    Returns:\n        success:  bool\n            Whether or not the delete succeeded\n        changed:  bool\n            Whether or not the delete resulted in changes\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.base.DeployManagerBase.filter_objects_current_state","title":"<code>filter_objects_current_state(kind, namespace=None, api_version=None, label_selector=None, field_selector=None)</code>  <code>abstractmethod</code>","text":"<p>The filter_objects_current_state function fetches a list of objects that match either/both the label or field selector Args:     kind:  str         The kind of the object to fetch     namespace:  str         The namespace to search for the object     api_version:  str         The api_version of the resource kind to fetch     label_selector:  str         The label_selector to filter the resources     field_selector:  str         The field_selector to filter the resources</p> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool Whether or not the state fetch operation succeeded</p> <code>current_state</code> <code>List[dict]</code> <p>List[dict] A list of  dict representations for the objects configuration, or an empty list if no objects match</p> Source code in <code>oper8/deploy_manager/base.py</code> <pre><code>@abc.abstractmethod\ndef filter_objects_current_state(  # pylint: disable=too-many-arguments\n    self,\n    kind: str,\n    namespace: Optional[str] = None,\n    api_version: Optional[str] = None,\n    label_selector: Optional[str] = None,\n    field_selector: Optional[str] = None,\n) -&gt; Tuple[bool, List[dict]]:\n    \"\"\"The filter_objects_current_state function fetches a list of objects\n    that match either/both the label or field selector\n    Args:\n        kind:  str\n            The kind of the object to fetch\n        namespace:  str\n            The namespace to search for the object\n        api_version:  str\n            The api_version of the resource kind to fetch\n        label_selector:  str\n            The label_selector to filter the resources\n        field_selector:  str\n            The field_selector to filter the resources\n\n    Returns:\n        success:  bool\n            Whether or not the state fetch operation succeeded\n        current_state:  List[dict]\n            A list of  dict representations for the objects configuration,\n            or an empty list if no objects match\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.base.DeployManagerBase.get_object_current_state","title":"<code>get_object_current_state(kind, name, namespace=None, api_version=None)</code>  <code>abstractmethod</code>","text":"<p>The get_current_objects function fetches the current state of a given object by name</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>str The kind of the object to fetch</p> required <code>name</code> <code>str</code> <p>str The full name of the object to fetch</p> required <code>namespace</code> <code>Optional[str]</code> <p>str The namespace to search for the object</p> <code>None</code> <code>api_version</code> <code>Optional[str]</code> <p>str The api_version of the resource kind to fetch</p> <code>None</code> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool Whether or not the state fetch operation succeeded</p> <code>current_state</code> <code>dict</code> <p>dict or None The dict representation of the current object's configuration, or None if not present</p> Source code in <code>oper8/deploy_manager/base.py</code> <pre><code>@abc.abstractmethod\ndef get_object_current_state(\n    self,\n    kind: str,\n    name: str,\n    namespace: Optional[str] = None,\n    api_version: Optional[str] = None,\n) -&gt; Tuple[bool, dict]:\n    \"\"\"The get_current_objects function fetches the current state of a given\n    object by name\n\n    Args:\n        kind:  str\n            The kind of the object to fetch\n        name:  str\n            The full name of the object to fetch\n        namespace:  str\n            The namespace to search for the object\n        api_version:  str\n            The api_version of the resource kind to fetch\n\n    Returns:\n        success:  bool\n            Whether or not the state fetch operation succeeded\n        current_state:  dict or None\n            The dict representation of the current object's configuration,\n            or None if not present\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.base.DeployManagerBase.set_status","title":"<code>set_status(kind, name, namespace, status, api_version=None)</code>  <code>abstractmethod</code>","text":"<p>Set the status for an object managed by oper8</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>str The kind of the object ot fetch</p> required <code>name</code> <code>str</code> <p>str The full name of the object to fetch</p> required <code>namespace</code> <code>Optional[str]</code> <p>Optional[str] The namespace to search for the object. If None search cluster wide</p> required <code>status</code> <code>dict</code> <p>dict The status object to set onto the given object</p> required <code>api_version</code> <code>Optional[str]</code> <p>str The api_version of the resource to update</p> <code>None</code> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool Whether or not the state fetch operation succeeded</p> <code>changed</code> <code>bool</code> <p>bool Whether or not the status update resulted in a change</p> Source code in <code>oper8/deploy_manager/base.py</code> <pre><code>@abc.abstractmethod\ndef set_status(  # pylint: disable=too-many-arguments\n    self,\n    kind: str,\n    name: str,\n    namespace: Optional[str],\n    status: dict,\n    api_version: Optional[str] = None,\n) -&gt; Tuple[bool, bool]:\n    \"\"\"Set the status for an object managed by oper8\n\n    Args:\n        kind:  str\n            The kind of the object ot fetch\n        name:  str\n            The full name of the object to fetch\n        namespace:  Optional[str]\n            The namespace to search for the object. If None search cluster wide\n        status:  dict\n            The status object to set onto the given object\n        api_version:  str\n            The api_version of the resource to update\n\n    Returns:\n        success:  bool\n            Whether or not the state fetch operation succeeded\n        changed:  bool\n            Whether or not the status update resulted in a change\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.base.DeployManagerBase.watch_objects","title":"<code>watch_objects(kind, api_version=None, namespace=None, name=None, label_selector=None, field_selector=None, resource_version=None)</code>  <code>abstractmethod</code>","text":"<p>The watch_objects function listens for changes in the cluster and returns a stream of KubeWatchEvents</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>str The kind of the object to fetch</p> required <code>namespace</code> <code>Optional[str]</code> <p>str The namespace to search for the object</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>str The name to search for the object</p> <code>None</code> <code>api_version</code> <code>Optional[str]</code> <p>str The api_version of the resource kind to fetch</p> <code>None</code> <code>label_selector</code> <code>Optional[str]</code> <p>str The label_selector to filter the resources</p> <code>None</code> <code>field_selector</code> <code>Optional[str]</code> <p>str The field_selector to filter the resources</p> <code>None</code> <code>resource_version</code> <code>Optional[str]</code> <p>str The resource_version the resource must be newer than</p> <code>None</code> <p>Returns:</p> Name Type Description <code>watch_stream</code> <code>Iterator[KubeWatchEvent]</code> <p>Generator[KubeWatchEvent] A stream of KubeWatchEvents generated while watching</p> Source code in <code>oper8/deploy_manager/base.py</code> <pre><code>@abc.abstractmethod\ndef watch_objects(  # pylint: disable=too-many-arguments\n    self,\n    kind: str,\n    api_version: Optional[str] = None,\n    namespace: Optional[str] = None,\n    name: Optional[str] = None,\n    label_selector: Optional[str] = None,\n    field_selector: Optional[str] = None,\n    resource_version: Optional[str] = None,\n) -&gt; Iterator[KubeWatchEvent]:\n    \"\"\"The watch_objects function listens for changes in the cluster and returns a\n    stream of KubeWatchEvents\n\n    Args:\n        kind:  str\n            The kind of the object to fetch\n        namespace:  str\n            The namespace to search for the object\n        name:  str\n            The name to search for the object\n        api_version:  str\n            The api_version of the resource kind to fetch\n        label_selector:  str\n            The label_selector to filter the resources\n        field_selector:  str\n            The field_selector to filter the resources\n        resource_version:  str\n            The resource_version the resource must be newer than\n\n    Returns:\n        watch_stream: Generator[KubeWatchEvent]\n            A stream of KubeWatchEvents generated while watching\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.dry_run_deploy_manager","title":"<code>dry_run_deploy_manager</code>","text":"<p>The DryRunDeployManager implements the DeployManager interface but does not actually interact with the cluster and instead holds the state of the cluster in a local map.</p>"},{"location":"API%20References/#oper8.deploy_manager.dry_run_deploy_manager.DryRunDeployManager","title":"<code>DryRunDeployManager</code>","text":"<p>               Bases: <code>DeployManagerBase</code></p> <p>Deploy manager which doesn't actually deploy!</p> Source code in <code>oper8/deploy_manager/dry_run_deploy_manager.py</code> <pre><code>class DryRunDeployManager(DeployManagerBase):\n    \"\"\"\n    Deploy manager which doesn't actually deploy!\n    \"\"\"\n\n    def __init__(\n        self,\n        resources=None,\n        owner_cr=None,\n        strict_resource_version=False,\n        generate_resource_version=True,\n    ):\n        \"\"\"Construct with a static value to use for whether or not the functions\n        should report change.\n        \"\"\"\n        self._owner_cr = owner_cr\n        self._cluster_content = {}\n        self.strict_resource_version = strict_resource_version\n        self.generate_resource_version = generate_resource_version\n\n        # Dicts of registered watches and watchers\n        self._watches = {}\n        self._finalizers = {}\n\n        # Deploy provided resources\n        self._deploy(resources or [], call_watches=False, manage_owner_references=False)\n\n    ## Interface ###############################################################\n\n    def deploy(\n        self,\n        resource_definitions,\n        manage_owner_references=True,\n        method: DeployMethod = DeployMethod.DEFAULT,\n        **_,\n    ):\n        log.info(\"DRY RUN deploy\")\n        return self._deploy(\n            resource_definitions,\n            manage_owner_references=manage_owner_references,\n            method=method,\n        )\n\n    def disable(self, resource_definitions):\n        log.info(\"DRY RUN disable\")\n        changed = False\n        for resource in resource_definitions:\n            api_version = resource.get(\"apiVersion\")\n            kind = resource.get(\"kind\")\n            name = resource.get(\"metadata\", {}).get(\"name\")\n            namespace = resource.get(\"metadata\", {}).get(\"namespace\")\n            _, content = self.get_object_current_state(\n                kind=kind, api_version=api_version, namespace=namespace, name=name\n            )\n            if content is not None:\n                changed = True\n\n                # Set resource finalizers\n                with DRY_RUN_CLUSTER_LOCK:\n                    self._cluster_content[namespace][kind][api_version][name][\n                        \"metadata\"\n                    ][\"deletionTimestamp\"] = datetime.now().strftime(\n                        \"%Y-%m-%dT%H:%M:%SZ\"\n                    )\n                    self._cluster_content[namespace][kind][api_version][name][\n                        \"metadata\"\n                    ][\"deletionGracePeriodSeconds\"] = 0\n\n                # Call any registered finalizers\n                for key, callback in self._get_registered_watches(\n                    api_version, kind, namespace, name, finalizer=True\n                ):\n                    log.debug2(\n                        \"Calling registered finalizer [%s] for [%s]\", callback, key\n                    )\n                    callback(self._cluster_content[namespace][kind][api_version][name])\n\n                # If finalizers have been cleared and object hasn't already been deleted then\n                # remove the key\n                current_obj = (\n                    self._cluster_content.get(namespace, {})\n                    .get(kind, {})\n                    .get(api_version, {})\n                    .get(name, {})\n                )\n                if current_obj and not current_obj.get(\"metadata\", {}).get(\n                    \"finalizers\", []\n                ):\n                    with DRY_RUN_CLUSTER_LOCK:\n                        self._delete_key(namespace, kind, api_version, name)\n\n        return True, changed\n\n    def get_object_current_state(self, kind, name, namespace=None, api_version=None):\n        log.info(\n            \"DRY RUN get_object_current_state of [%s/%s] in [%s]\", kind, name, namespace\n        )\n\n        # Look in the cluster state\n        matches = []\n        kind_entries = self._cluster_content.get(namespace, {}).get(kind, {})\n        log.debug3(\"Kind entries: %s\", kind_entries)\n        for api_ver, entries in kind_entries.items():\n            log.debug3(\"Checking api_version [%s // %s]\", api_ver, api_version)\n            if name in entries and (api_ver == api_version or api_version is None):\n                matches.append(entries[name])\n        log.debug(\n            \"Found %d matches for [%s/%s] in %s\", len(matches), kind, name, namespace\n        )\n        if len(matches) == 1:\n            return True, copy.deepcopy(matches[0])\n        return True, None\n\n    def filter_objects_current_state(\n        self,\n        kind,\n        namespace=None,\n        api_version=None,\n        label_selector=None,\n        field_selector=None,\n    ):  # pylint: disable=too-many-arguments\n        log.info(\n            \"DRY RUN filter_objects_current_state of [%s] in [%s]\", kind, namespace\n        )\n        # Look in the cluster state\n        matches = []\n        kind_entries = self._cluster_content.get(namespace, {}).get(kind, {})\n        log.debug3(\"Kind entries: %s\", kind_entries)\n        for api_ver, entries in kind_entries.items():\n            # Make sure api version matches\n            log.debug3(\"Checking api_version [%s // %s]\", api_ver, api_version)\n            if api_ver != api_version and api_version is not None:\n                continue\n\n            for resource in entries.values():\n                # Make sure Labels Match\n                log.debug3(\"Resource: %s\", resource)\n\n                labels = resource.get(\"metadata\", {}).get(\"labels\", {})\n                log.debug3(\"Checking label_selector [%s // %s]\", labels, label_selector)\n                if label_selector is not None and not _match_selector(\n                    labels, label_selector\n                ):\n                    continue\n\n                # Only do the work for field selector if one exists\n                log.debug3(\"Checking field_selector [%s]\", field_selector)\n                if field_selector is not None and not _match_selector(\n                    _convert_dict_to_dot(resource),\n                    field_selector,\n                ):\n                    continue\n\n                # Add deep copy of entry to matches list\n                matches.append(copy.deepcopy(resource))\n\n        return True, matches\n\n    def set_status(\n        self,\n        kind,\n        name,\n        namespace,\n        status,\n        api_version=None,\n    ):  # pylint: disable=too-many-arguments\n        log.info(\n            \"DRY RUN set_status of [%s.%s/%s] in %s: %s\",\n            api_version,\n            kind,\n            name,\n            namespace,\n            status,\n        )\n        object_content = self.get_object_current_state(\n            kind, name, namespace, api_version\n        )[1]\n        if object_content is None:\n            log.debug(\"Did not find [%s/%s] in %s\", kind, name, namespace)\n            return False, False\n        prev_status = object_content.get(\"status\")\n        object_content[\"status\"] = status\n        self._deploy([object_content], call_watches=False)\n        return True, prev_status != status\n\n    def watch_objects(  # pylint: disable=too-many-arguments,too-many-locals,unused-argument\n        self,\n        kind: str,\n        api_version: Optional[str] = None,\n        namespace: Optional[str] = None,\n        name: Optional[str] = None,\n        label_selector: Optional[str] = None,\n        field_selector: Optional[str] = None,\n        resource_version: Optional[str] = None,\n        timeout: Optional[int] = 15,\n        **kwargs,\n    ) -&gt; Iterator[KubeWatchEvent]:\n        \"\"\"Watch the DryRunDeployManager for resource changes by registering\n        callbacks\"\"\"\n\n        event_queue = Queue()\n        resource_map = {}\n\n        def add_event(resource_map: dict, manifest: dict):\n            \"\"\"Callback triggered when resources are deployed\"\"\"\n            resource = ManagedObject(manifest)\n            event_type = KubeEventType.ADDED\n\n            watch_key = self._watch_key(\n                api_version=resource.api_version,\n                kind=resource.kind,\n                namespace=resource.namespace,\n                name=resource.name,\n            )\n            if watch_key in resource_map:\n                log.debug4(\"Watch key detected, setting Modified event type\")\n                event_type = KubeEventType.MODIFIED\n\n            resource_map[watch_key] = resource\n            event = KubeWatchEvent(\n                type=event_type,\n                resource=resource,\n            )\n            event_queue.put(event)\n\n        def delete_event(resource_map: dict, manifest: dict):\n            \"\"\"Callback triggered when resources are disabled\"\"\"\n            resource = ManagedObject(manifest)\n            watch_key = self._watch_key(\n                api_version=resource.api_version,\n                kind=resource.kind,\n                namespace=resource.namespace,\n                name=resource.name,\n            )\n            if watch_key in resource_map:\n                del resource_map[watch_key]\n\n            event = KubeWatchEvent(\n                type=KubeEventType.DELETED,\n                resource=resource,\n            )\n            event_queue.put(event)\n\n        # Get initial resources\n        _, manifests = self.filter_objects_current_state(\n            kind=kind,\n            api_version=api_version,\n            namespace=namespace,\n            label_selector=label_selector,\n            field_selector=field_selector,\n        )\n        for manifest in manifests:\n            resource = ManagedObject(manifest)\n            watch_key = self._watch_key(\n                kind=resource.kind,\n                api_version=resource.api_version,\n                name=resource.name,\n                namespace=resource.namespace,\n            )\n            resource_map[watch_key] = resource\n\n            event = KubeWatchEvent(type=KubeEventType.ADDED, resource=resource)\n            log.debug2(\"Yielding initial event %s\", event)\n            yield event\n\n        end_time = datetime.max\n        if timeout:\n            end_time = datetime.now() + timedelta(seconds=timeout)\n\n        # Register callbacks\n        self.register_watch(\n            api_version=api_version,\n            kind=kind,\n            namespace=namespace,\n            name=name,\n            callback=partial(add_event, resource_map),\n        )\n        self.register_finalizer(\n            api_version=api_version,\n            kind=kind,\n            namespace=namespace,\n            name=name,\n            callback=partial(delete_event, resource_map),\n        )\n\n        # Yield any events from the callback queue\n        log.debug2(\"Waiting till %s\", end_time)\n        while True:\n            sec_till_end = (end_time - datetime.now()).seconds or 1\n            try:\n                event = event_queue.get(timeout=sec_till_end)\n                log.debug2(\"Yielding event %s\", event)\n                yield event\n            except Empty:\n                pass\n\n            if datetime.now() &gt; end_time:\n                return\n\n    ## Dry Run Methods #########################################################\n    def register_watch(  # pylint: disable=too-many-arguments\n        self,\n        api_version: str,\n        kind: str,\n        callback: Callable[[dict], None],\n        namespace=\"\",\n        name=\"\",\n    ):\n        \"\"\"Register a callback to watch for deploy events on a given\n        api_version/kind\n        \"\"\"\n        watch_key = self._watch_key(\n            api_version=api_version, kind=kind, namespace=namespace, name=name\n        )\n        log.debug(\"Registering watch for %s\", watch_key)\n        self._watches.setdefault(watch_key, []).append(callback)\n\n    def register_finalizer(  # pylint: disable=too-many-arguments\n        self,\n        api_version: str,\n        kind: str,\n        callback: Callable[[dict], None],\n        namespace=\"\",\n        name=\"\",\n    ):\n        \"\"\"Register a callback to call on deletion events on a given\n        api_version/kind\n        \"\"\"\n        watch_key = self._watch_key(\n            api_version=api_version, kind=kind, namespace=namespace, name=name\n        )\n        log.debug(\"Registering finalizer for %s\", watch_key)\n        self._finalizers.setdefault(watch_key, []).append(callback)\n\n    ## Implementation Details ##################################################\n\n    @staticmethod\n    def _watch_key(api_version=\"\", kind=\"\", namespace=\"\", name=\"\"):\n        return \":\".join([api_version or \"\", kind or \"\", namespace or \"\", name or \"\"])\n\n    def _get_registered_watches(  # pylint: disable=too-many-arguments\n        self,\n        api_version: str = \"\",\n        kind: str = \"\",\n        namespace: str = \"\",\n        name: str = \"\",\n        finalizer: bool = False,\n    ) -&gt; List[Tuple[str, Callable]]:\n        # Get the scoped watch key\n        resource_watch_key = self._watch_key(\n            api_version=api_version, kind=kind, namespace=namespace, name=name\n        )\n        namespaced_watch_key = self._watch_key(\n            api_version=api_version, kind=kind, namespace=namespace\n        )\n        global_watch_key = self._watch_key(api_version=api_version, kind=kind)\n\n        # Get which watch list we're pulling from\n        callback_map = self._watches\n        if finalizer:\n            callback_map = self._finalizers\n\n        output_list = []\n        log.debug3(\n            \"Looking for resourced key: %s namespace key %s global key %s\",\n            resource_watch_key,\n            namespaced_watch_key,\n            global_watch_key,\n        )\n        for key, callback_list in callback_map.items():\n            if key in [resource_watch_key, namespaced_watch_key, global_watch_key]:\n                log.debug3(\"%d Callbacks found for key %s\", len(callback_list), key)\n                for callback in callback_list:\n                    output_list.append((key, callback))\n\n        return output_list\n\n    def _delete_key(self, namespace, kind, api_version, name):\n        del self._cluster_content[namespace][kind][api_version][name]\n        if not self._cluster_content[namespace][kind][api_version]:\n            del self._cluster_content[namespace][kind][api_version]\n        if not self._cluster_content[namespace][kind]:\n            del self._cluster_content[namespace][kind]\n        if not self._cluster_content[namespace]:\n            del self._cluster_content[namespace]\n\n    def _deploy(\n        self,\n        resource_definitions,\n        call_watches=True,\n        manage_owner_references=True,\n        method: DeployMethod = DeployMethod.DEFAULT,\n    ):\n        log.info(\"DRY RUN deploy\")\n        changes = False\n        for resource in resource_definitions:\n            api_version = resource.get(\"apiVersion\")\n            kind = resource.get(\"kind\")\n            name = resource.get(\"metadata\", {}).get(\"name\")\n            namespace = resource.get(\"metadata\", {}).get(\"namespace\")\n            log.debug(\n                \"DRY RUN deploy [%s/%s/%s/%s]\", namespace, kind, api_version, name\n            )\n            log.debug4(resource)\n\n            # If owner CR configured, add ownerReferences\n            if self._owner_cr and manage_owner_references:\n                log.debug2(\"Adding dry-run owner references\")\n                update_owner_references(self, self._owner_cr, resource)\n                log.debug3(\n                    \"All owner references: %s\", resource[\"metadata\"][\"ownerReferences\"]\n                )\n\n            with DRY_RUN_CLUSTER_LOCK:\n                entries = (\n                    self._cluster_content.setdefault(namespace, {})\n                    .setdefault(kind, {})\n                    .setdefault(api_version, {})\n                )\n                current = copy.deepcopy(entries.get(name, {}))\n                old_resource_version = current.get(\"metadata\", {}).pop(\n                    \"resourceVersion\", None\n                )\n                changes = changes or (current != resource)\n\n                if \"metadata\" not in resource:\n                    resource[\"metadata\"] = {}\n\n                if (\n                    self.strict_resource_version\n                    and resource[\"metadata\"].get(\"resourceVersion\")\n                    and old_resource_version\n                    and resource[\"metadata\"].get(\"resourceVersion\")\n                    != old_resource_version\n                ):\n                    log.warning(\n                        \"Unable to deploy resource. resourceVersion is out of date\"\n                    )\n                    return False, False\n\n                resource[\"metadata\"][\"creationTimestamp\"] = entries.get(\n                    \"metadata\", {}\n                ).get(\"creationTimestamp\", datetime.now().isoformat())\n                resource[\"metadata\"][\"uid\"] = entries.get(\"metadata\", {}).get(\n                    \"uid\", str(uuid.uuid4())\n                )\n\n                if self.generate_resource_version:\n                    resource[\"metadata\"][\"resourceVersion\"] = str(\n                        random.randint(1, 1000)\n                    ).zfill(5)\n\n                # Depending on the deploy method either update or fully replace the object\n                if method == DeployMethod.DEFAULT or method == DeployMethod.REPLACE:\n                    entries[name] = resource\n                else:\n                    if name in entries:\n                        entries[name] = merge_configs(entries[name], resource)\n                    # If the object doesn't already exist then just add it\n                    else:\n                        entries[name] = resource\n\n            # Call any registered watches\n            if call_watches:\n                for key, callback in self._get_registered_watches(\n                    api_version, kind, namespace, name\n                ):\n                    log.debug2(\"Calling registered watch [%s] for [%s]\", callback, key)\n                    callback(resource)\n\n            # Delete Key if it has already been disabled and doesn't have finalizers\n            if self._cluster_content[namespace][kind][api_version][name].get(\n                \"metadata\", {}\n            ).get(\"deletionTimestamp\") and not self._cluster_content[namespace][kind][\n                api_version\n            ][\n                name\n            ].get(\n                \"metadata\", {}\n            ).get(\n                \"finalizers\"\n            ):\n                with DRY_RUN_CLUSTER_LOCK:\n                    self._delete_key(namespace, kind, api_version, name)\n\n        return True, changes\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.dry_run_deploy_manager.DryRunDeployManager.__init__","title":"<code>__init__(resources=None, owner_cr=None, strict_resource_version=False, generate_resource_version=True)</code>","text":"<p>Construct with a static value to use for whether or not the functions should report change.</p> Source code in <code>oper8/deploy_manager/dry_run_deploy_manager.py</code> <pre><code>def __init__(\n    self,\n    resources=None,\n    owner_cr=None,\n    strict_resource_version=False,\n    generate_resource_version=True,\n):\n    \"\"\"Construct with a static value to use for whether or not the functions\n    should report change.\n    \"\"\"\n    self._owner_cr = owner_cr\n    self._cluster_content = {}\n    self.strict_resource_version = strict_resource_version\n    self.generate_resource_version = generate_resource_version\n\n    # Dicts of registered watches and watchers\n    self._watches = {}\n    self._finalizers = {}\n\n    # Deploy provided resources\n    self._deploy(resources or [], call_watches=False, manage_owner_references=False)\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.dry_run_deploy_manager.DryRunDeployManager.register_finalizer","title":"<code>register_finalizer(api_version, kind, callback, namespace='', name='')</code>","text":"<p>Register a callback to call on deletion events on a given api_version/kind</p> Source code in <code>oper8/deploy_manager/dry_run_deploy_manager.py</code> <pre><code>def register_finalizer(  # pylint: disable=too-many-arguments\n    self,\n    api_version: str,\n    kind: str,\n    callback: Callable[[dict], None],\n    namespace=\"\",\n    name=\"\",\n):\n    \"\"\"Register a callback to call on deletion events on a given\n    api_version/kind\n    \"\"\"\n    watch_key = self._watch_key(\n        api_version=api_version, kind=kind, namespace=namespace, name=name\n    )\n    log.debug(\"Registering finalizer for %s\", watch_key)\n    self._finalizers.setdefault(watch_key, []).append(callback)\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.dry_run_deploy_manager.DryRunDeployManager.register_watch","title":"<code>register_watch(api_version, kind, callback, namespace='', name='')</code>","text":"<p>Register a callback to watch for deploy events on a given api_version/kind</p> Source code in <code>oper8/deploy_manager/dry_run_deploy_manager.py</code> <pre><code>def register_watch(  # pylint: disable=too-many-arguments\n    self,\n    api_version: str,\n    kind: str,\n    callback: Callable[[dict], None],\n    namespace=\"\",\n    name=\"\",\n):\n    \"\"\"Register a callback to watch for deploy events on a given\n    api_version/kind\n    \"\"\"\n    watch_key = self._watch_key(\n        api_version=api_version, kind=kind, namespace=namespace, name=name\n    )\n    log.debug(\"Registering watch for %s\", watch_key)\n    self._watches.setdefault(watch_key, []).append(callback)\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.dry_run_deploy_manager.DryRunDeployManager.watch_objects","title":"<code>watch_objects(kind, api_version=None, namespace=None, name=None, label_selector=None, field_selector=None, resource_version=None, timeout=15, **kwargs)</code>","text":"<p>Watch the DryRunDeployManager for resource changes by registering callbacks</p> Source code in <code>oper8/deploy_manager/dry_run_deploy_manager.py</code> <pre><code>def watch_objects(  # pylint: disable=too-many-arguments,too-many-locals,unused-argument\n    self,\n    kind: str,\n    api_version: Optional[str] = None,\n    namespace: Optional[str] = None,\n    name: Optional[str] = None,\n    label_selector: Optional[str] = None,\n    field_selector: Optional[str] = None,\n    resource_version: Optional[str] = None,\n    timeout: Optional[int] = 15,\n    **kwargs,\n) -&gt; Iterator[KubeWatchEvent]:\n    \"\"\"Watch the DryRunDeployManager for resource changes by registering\n    callbacks\"\"\"\n\n    event_queue = Queue()\n    resource_map = {}\n\n    def add_event(resource_map: dict, manifest: dict):\n        \"\"\"Callback triggered when resources are deployed\"\"\"\n        resource = ManagedObject(manifest)\n        event_type = KubeEventType.ADDED\n\n        watch_key = self._watch_key(\n            api_version=resource.api_version,\n            kind=resource.kind,\n            namespace=resource.namespace,\n            name=resource.name,\n        )\n        if watch_key in resource_map:\n            log.debug4(\"Watch key detected, setting Modified event type\")\n            event_type = KubeEventType.MODIFIED\n\n        resource_map[watch_key] = resource\n        event = KubeWatchEvent(\n            type=event_type,\n            resource=resource,\n        )\n        event_queue.put(event)\n\n    def delete_event(resource_map: dict, manifest: dict):\n        \"\"\"Callback triggered when resources are disabled\"\"\"\n        resource = ManagedObject(manifest)\n        watch_key = self._watch_key(\n            api_version=resource.api_version,\n            kind=resource.kind,\n            namespace=resource.namespace,\n            name=resource.name,\n        )\n        if watch_key in resource_map:\n            del resource_map[watch_key]\n\n        event = KubeWatchEvent(\n            type=KubeEventType.DELETED,\n            resource=resource,\n        )\n        event_queue.put(event)\n\n    # Get initial resources\n    _, manifests = self.filter_objects_current_state(\n        kind=kind,\n        api_version=api_version,\n        namespace=namespace,\n        label_selector=label_selector,\n        field_selector=field_selector,\n    )\n    for manifest in manifests:\n        resource = ManagedObject(manifest)\n        watch_key = self._watch_key(\n            kind=resource.kind,\n            api_version=resource.api_version,\n            name=resource.name,\n            namespace=resource.namespace,\n        )\n        resource_map[watch_key] = resource\n\n        event = KubeWatchEvent(type=KubeEventType.ADDED, resource=resource)\n        log.debug2(\"Yielding initial event %s\", event)\n        yield event\n\n    end_time = datetime.max\n    if timeout:\n        end_time = datetime.now() + timedelta(seconds=timeout)\n\n    # Register callbacks\n    self.register_watch(\n        api_version=api_version,\n        kind=kind,\n        namespace=namespace,\n        name=name,\n        callback=partial(add_event, resource_map),\n    )\n    self.register_finalizer(\n        api_version=api_version,\n        kind=kind,\n        namespace=namespace,\n        name=name,\n        callback=partial(delete_event, resource_map),\n    )\n\n    # Yield any events from the callback queue\n    log.debug2(\"Waiting till %s\", end_time)\n    while True:\n        sec_till_end = (end_time - datetime.now()).seconds or 1\n        try:\n            event = event_queue.get(timeout=sec_till_end)\n            log.debug2(\"Yielding event %s\", event)\n            yield event\n        except Empty:\n            pass\n\n        if datetime.now() &gt; end_time:\n            return\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.kube_event","title":"<code>kube_event</code>","text":"<p>Helper module to define shared types related to Kube Events</p>"},{"location":"API%20References/#oper8.deploy_manager.kube_event.KubeEventType","title":"<code>KubeEventType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for all possible kubernetes event types</p> Source code in <code>oper8/deploy_manager/kube_event.py</code> <pre><code>class KubeEventType(Enum):\n    \"\"\"Enum for all possible kubernetes event types\"\"\"\n\n    DELETED = \"DELETED\"\n    MODIFIED = \"MODIFIED\"\n    ADDED = \"ADDED\"\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.kube_event.KubeWatchEvent","title":"<code>KubeWatchEvent</code>  <code>dataclass</code>","text":"<p>DataClass containing the type, resource, and timestamp of a particular event</p> Source code in <code>oper8/deploy_manager/kube_event.py</code> <pre><code>@dataclass\nclass KubeWatchEvent:\n    \"\"\"DataClass containing the type, resource, and timestamp of a\n    particular event\"\"\"\n\n    type: KubeEventType\n    resource: ManagedObject\n    timestamp: datetime = field(default_factory=datetime.now)\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.openshift_deploy_manager","title":"<code>openshift_deploy_manager</code>","text":"<p>This DeployManager is responsible for delegating cluster operations to the openshift library. It is the one that will be used when the operator is running in the cluster or outside the cluster making live changes.</p>"},{"location":"API%20References/#oper8.deploy_manager.openshift_deploy_manager.OpenshiftDeployManager","title":"<code>OpenshiftDeployManager</code>","text":"<p>               Bases: <code>DeployManagerBase</code></p> <p>This DeployManager uses the openshift DynamicClient to interact with the cluster</p> Source code in <code>oper8/deploy_manager/openshift_deploy_manager.py</code> <pre><code>class OpenshiftDeployManager(DeployManagerBase):\n    \"\"\"This DeployManager uses the openshift DynamicClient to interact with the\n    cluster\n    \"\"\"\n\n    def __init__(\n        self,\n        manage_ansible_status: bool = False,\n        owner_cr: Optional[dict] = None,\n    ):\n        \"\"\"\n        Args:\n            manage_ansible_status:  bool\n                If true, oper8 will emulate the status management done natively\n                by ansible based on the readiness values of oper8's native status\n                management\n            owner_cr:  Optional[dict]\n                The dict content of the CR that triggered this reconciliation.\n                If given, deployed objects will have an ownerReference added to\n                assign ownership to this CR instance.\n        \"\"\"\n        self.manage_ansible_status = manage_ansible_status\n        self._owner_cr = owner_cr\n\n        # Set up the client\n        log.debug(\"Initializing openshift client\")\n        self._client = None\n\n        # Keep a threading lock for performing status updates. This is necessary\n        # to avoid running into 409 Conflict errors if concurrent threads are\n        # trying to perform status updates\n        self._status_lock = threading.Lock()\n\n    @property\n    def client(self):\n        \"\"\"Lazy property access to the client\"\"\"\n        if self._client is None:\n            self._client = self._setup_client()\n        return self._client\n\n    @alog.logged_function(log.debug)\n    def deploy(\n        self,\n        resource_definitions: List[dict],\n        manage_owner_references: bool = True,\n        retry_operation: bool = True,\n        method: DeployMethod = DeployMethod.DEFAULT,\n        **_,  # Accept any kwargs to compatibility\n    ) -&gt; Tuple[bool, bool]:\n        \"\"\"Deploy using the openshift client\n\n        Args:\n            resource_definitions:  list(dict)\n                List of resource object dicts to apply to the cluster\n            manage_owner_references:  bool\n                If true, ownerReferences for the parent CR will be applied to\n                the deployed object\n\n        Returns:\n            success:  bool\n                True if deploy succeeded, False otherwise\n            changed:  bool\n                Whether or not the deployment resulted in changes\n        \"\"\"\n        return self._retried_operation(\n            resource_definitions,\n            self._apply,\n            max_retries=config.deploy_retries if retry_operation else 0,\n            manage_owner_references=manage_owner_references,\n            method=method,\n        )\n\n    @alog.logged_function(log.debug)\n    def disable(self, resource_definitions: List[dict]) -&gt; Tuple[bool, bool]:\n        \"\"\"The disable process is the same as the deploy process, but the child\n        module params are set to 'state: absent'\n\n        Args:\n            resource_definitions:  list(dict)\n                List of resource object dicts to apply to the cluster\n\n        Returns:\n            success:  bool\n                True if deploy succeeded, False otherwise\n            changed:  bool\n                Whether or not the delete resulted in changes\n        \"\"\"\n        return self._retried_operation(\n            resource_definitions,\n            self._disable,\n            max_retries=config.deploy_retries,\n            manage_owner_references=False,\n        )\n\n    def get_object_current_state(\n        self,\n        kind: str,\n        name: str,\n        namespace: Optional[str] = None,\n        api_version: Optional[str] = None,\n    ) -&gt; Tuple[bool, dict]:\n        \"\"\"The get_current_objects function fetches the current state using\n        calls directly to the api client\n\n        Args:\n            kind:  str\n                The kind of the object ot fetch\n            name:  str\n                The full name of the object to fetch\n            namespace:  Optional[str]\n                The namespace to search for the object or None for no namespace\n            api_version:  Optional[str]\n                The api_version of the resource kind to fetch\n\n        Returns:\n            success:  bool\n                Whether or not the state fetch operation succeeded\n            current_state:  dict or None\n                The dict representation of the current object's configuration,\n                or None if not present\n        \"\"\"\n\n        # Use the lazy discovery tool to first get all objects of the given type\n        # in the given namespace, then look for the specific resource by name\n        resources = self._get_resource_handle(kind, api_version)\n        if not resources:\n            return True, None\n\n        if not namespace:\n            resources.namespaced = False\n\n        try:\n            resource = resources.get(name=name, namespace=namespace)\n        except ForbiddenError:\n            log.debug(\n                \"Fetching objects of kind [%s] forbidden in namespace [%s]\",\n                kind,\n                namespace,\n            )\n            return False, None\n        except NotFoundError:\n            log.debug(\n                \"No object named [%s/%s] found in namespace [%s]\", kind, name, namespace\n            )\n            return True, None\n\n        # If the resource was found, return it's dict representation\n        return True, resource.to_dict()\n\n    def watch_objects(  # pylint: disable=too-many-arguments\n        self,\n        kind: str,\n        api_version: Optional[str] = None,\n        namespace: Optional[str] = None,\n        name: Optional[str] = None,\n        label_selector: Optional[str] = None,\n        field_selector: Optional[str] = None,\n        resource_version: Optional[str] = None,\n        watch_manager: Optional[Watch] = None,\n    ) -&gt; Iterator[KubeWatchEvent]:\n        watch_manager = watch_manager if watch_manager else Watch()\n        resource_handle = self._get_resource_handle(kind, api_version)\n        assert_cluster(\n            resource_handle,\n            (\n                \"Failed to fetch resource handle for \"\n                + f\"{namespace}/{api_version}/{kind}\"\n            ),\n        )\n\n        resource_version = resource_version if resource_version else 0\n\n        while True:\n            try:\n                for event_obj in watch_manager.stream(\n                    resource_handle.get,\n                    resource_version=resource_version,\n                    namespace=namespace,\n                    name=name,\n                    label_selector=label_selector,\n                    field_selector=field_selector,\n                    serialize=False,\n                    timeout_seconds=SERVER_WATCH_TIMEOUT,\n                    _request_timeout=CLIENT_WATCH_TIMEOUT,\n                ):\n                    event_type = KubeEventType(event_obj[\"type\"])\n                    event_resource = ManagedObject(event_obj[\"object\"])\n                    yield KubeWatchEvent(event_type, event_resource)\n            except client.exceptions.ApiException as exception:\n                if exception.status == 410:\n                    log.debug2(\n                        f\"Resource age expired, restarting watch {kind}/{api_version}\"\n                    )\n                    resource_version = None\n                else:\n                    log.info(\"Unknown ApiException received, re-raising\")\n                    raise exception\n            except urllib3.exceptions.ReadTimeoutError:\n                log.debug4(\n                    f\"Watch Socket closed, restarting watch {kind}/{api_version}\"\n                )\n            except urllib3.exceptions.ProtocolError:\n                log.debug2(\n                    f\"Invalid Chunk from server, restarting watch {kind}/{api_version}\"\n                )\n\n            # This is hidden attribute so probably not best to check\n            if watch_manager._stop:  # pylint: disable=protected-access\n                log.debug(\n                    \"Internal watch stopped. Stopping deploy manager watch for %s/%s\",\n                    kind,\n                    api_version,\n                )\n                return\n\n    def filter_objects_current_state(  # pylint: disable=too-many-arguments\n        self,\n        kind: str,\n        namespace: Optional[str] = None,\n        api_version: Optional[str] = None,\n        label_selector: Optional[str] = None,\n        field_selector: Optional[str] = None,\n    ) -&gt; Tuple[bool, List[dict]]:\n        \"\"\"The filter_objects_current_state function fetches a list of objects\n        that match either/both the label or field selector\n        Args:\n            kind:  str\n                The kind of the object to fetch\n            namespace:  str\n                The namespace to search for the object\n            api_version:  str\n                The api_version of the resource kind to fetch\n            label_selector:  str\n                The label_selector to filter the resources\n            field_selector:  str\n                The field_selector to filter the resources\n\n        Returns:\n            success:  bool\n                Whether or not the state fetch operation succeeded\n            current_state:  List[dict]\n                A list of  dict representations for the objects configuration,\n                or an empty list if no objects match\n        \"\"\"\n        # Use the lazy discovery tool to first get all objects of the given type\n        # in the given namespace, then look for the specific resource by name\n        resources = self._get_resource_handle(kind, api_version)\n        if not resources:\n            return True, []\n\n        if not namespace:\n            resources.namespaced = False\n\n        try:\n            list_obj = resources.get(\n                label_selector=label_selector,\n                field_selector=field_selector,\n                namespace=namespace,\n            )\n        except ForbiddenError:\n            log.debug(\n                \"Fetching objects of kind [%s] forbidden in namespace [%s]\",\n                kind,\n                namespace,\n            )\n            return False, []\n        except NotFoundError:\n            log.debug(\n                \"No objects of kind [%s] found in namespace [%s]\", kind, namespace\n            )\n            return True, []\n\n        # If the resource was found, get it's dict representation\n        resource_list = list_obj.to_dict().get(\"items\", [])\n        return True, resource_list\n\n    def set_status(  # pylint: disable=too-many-arguments\n        self,\n        kind: str,\n        name: str,\n        namespace: Optional[str],\n        status: dict,\n        api_version: Optional[str] = None,\n    ) -&gt; Tuple[bool, bool]:\n        \"\"\"Set the status in the cluster manifest for an object managed by this\n        operator\n\n        Args:\n            kind:  str\n                The kind of the object ot fetch\n            name:  str\n                The full name of the object to fetch\n            namespace:  Optional[str]\n                The namespace to search for the object.\n            status:  dict\n                The status object to set onto the given object\n            api_version:  Optional[str]\n                The api_version of the resource to update\n\n        Returns:\n            success:  bool\n                Whether or not the status update operation succeeded\n            changed:  bool\n                Whether or not the status update resulted in a change\n        \"\"\"\n        # Create a dummy resource to use in the common retry function\n        resource_definitions = [\n            {\n                \"kind\": kind,\n                \"apiVersion\": api_version,\n                \"metadata\": {\n                    \"name\": name,\n                    \"namespace\": namespace,\n                },\n            }\n        ]\n\n        # Run it with retries\n        return self._retried_operation(\n            resource_definitions,\n            self._set_status,\n            max_retries=config.deploy_retries,\n            status=status,\n            manage_owner_references=False,\n        )\n\n    ## Implementation Helpers ##################################################\n\n    @staticmethod\n    def _setup_client():\n        \"\"\"Create a DynamicClient that will work based on where the operator is\n        running\n        \"\"\"\n        # Try in-cluster config\n        try:\n            log.debug2(\"Running with in-cluster config\")\n\n            # Create Empty Config and load in-cluster information\n            kube_config = kubernetes.client.Configuration()\n            kubernetes.config.load_incluster_config(client_configuration=kube_config)\n\n            # Generate ApiClient and return Openshift DynamicClient\n            api_client = kubernetes.client.ApiClient(kube_config)\n            return DynamicClient(api_client)\n\n        # Fall back to out-of-cluster config\n        except kubernetes.config.ConfigException:\n            log.debug2(\"Running with out-of-cluster config\")\n            return DynamicClient(kubernetes.config.new_client_from_config())\n\n    @staticmethod\n    def _strip_last_applied(resource_definitions):\n        \"\"\"Make sure that the last-applied annotation is not present in any of\n        the resources. This can lead to recursive nesting!\n        \"\"\"\n        for resource_definition in resource_definitions:\n            last_applied = (\n                resource_definition.get(\"metadata\", {})\n                .get(\"annotations\", {})\n                .get(LAST_APPLIED_CONFIG_ANNOTATION)\n            )\n            if last_applied:\n                log.debug3(\"Removing [%s]\", LAST_APPLIED_CONFIG_ANNOTATION)\n                del resource_definition[\"metadata\"][\"annotations\"][\n                    LAST_APPLIED_CONFIG_ANNOTATION\n                ]\n                if not resource_definition[\"metadata\"][\"annotations\"]:\n                    del resource_definition[\"metadata\"][\"annotations\"]\n\n    def _get_resource_handle(self, kind: str, api_version: str) -&gt; Optional[Resource]:\n        \"\"\"Get the openshift resource handle for a specified kind and api_version\"\"\"\n        resources = None\n        try:\n            resources = self.client.resources.get(kind=kind, api_version=api_version)\n        except (ResourceNotFoundError, ResourceNotUniqueError):\n            try:\n                resources = self.client.resources.get(\n                    short_names=[kind], api_version=api_version\n                )\n            except (ResourceNotFoundError, ResourceNotUniqueError):\n                log.debug(\n                    \"No objects of kind [%s] found or multiple objects matching request found\",\n                    kind,\n                )\n        return resources\n\n    def _update_owner_references(self, resource_definitions):\n        \"\"\"If configured to do so, add owner references to the given resources\"\"\"\n        if self._owner_cr:\n            for resource_definition in resource_definitions:\n                update_owner_references(self, self._owner_cr, resource_definition)\n\n    def _retried_operation(\n        self,\n        resource_definitions,\n        operation,\n        max_retries,\n        manage_owner_references,\n        **kwargs,\n    ):\n        \"\"\"Shared wrapper for executing a client operation with retries\"\"\"\n\n        # Make sure the resource_definitions is a list\n        assert isinstance(\n            resource_definitions, list\n        ), \"Programming Error: resource_definitions is not a list\"\n        log.debug3(\"Running module with %d retries\", max_retries)\n\n        # If there are no resource definitions given, consider it a success with\n        # no change\n        if not resource_definitions:\n            log.debug(\"Nothing to do for an empty list of resources\")\n            return True, False\n\n        # Strip out last-applied annotations from all resources to avoid nested\n        # annotations\n        self._strip_last_applied(resource_definitions)\n\n        # Add owner references if configured to do so\n        if manage_owner_references:\n            self._update_owner_references(resource_definitions)\n\n        # Run each resource individually so that we can track partial completion\n        success = True\n        changed = False\n        for resource_definition in resource_definitions:\n            # Perform the operation and update the aggregate changed status\n            try:\n                changed = (\n                    self._run_individual_operation_with_retries(\n                        operation,\n                        max_retries,\n                        resource_definition=resource_definition,\n                        **kwargs,\n                    )\n                    or changed\n                )\n\n            # On failure, mark it and stop processing the rest of the resources.\n            # This is done because the resources in the file are assumed to be\n            # in an intentional sequence and resources later in the file may\n            # depend on resources earlier in the file.\n            except Exception as err:  # pylint: disable=broad-except\n                log.warning(\n                    \"Operation [%s] failed to execute: %s\",\n                    operation,\n                    err,\n                    exc_info=True,\n                )\n                success = False\n                break\n\n        # Return the aggregate success and change values\n        return success, changed\n\n    def _run_individual_operation_with_retries(\n        self,\n        operation: Callable,\n        remaining_retries: int,\n        resource_definition: dict,\n        **kwargs,\n    ):\n        \"\"\"Helper to execute a single helper operation with retries\n\n        Args:\n            operation:  Callable\n                The operation function to run\n            remaining_retries:  int\n                The number of remaining retries\n            resource_definition:  dict\n                The dict representation of the resource being applied\n            **kwargs:  dict\n                Keyword args to pass to the operation beyond resource_definition\n\n        Returns:\n            changed:  bool\n                Whether or not the operation resulted in meaningful change\n        \"\"\"\n        try:\n            return operation(resource_definition=resource_definition, **kwargs)\n        except ConflictError as err:\n            log.debug2(\"Handling ConflictError: %s\", err)\n\n            # If we have retries left, try again\n            if remaining_retries:\n                # Sleep for the backoff duration\n                backoff_duration = config.retry_backoff_base_seconds * (\n                    config.deploy_retries - remaining_retries + 1\n                )\n                log.debug3(\"Retrying in %fs\", backoff_duration)\n                time.sleep(backoff_duration)\n\n                # Fetch the current resourceVersion and update in the\n                # resource definition\n                # NOTE: This can overwrite changes made external to the operator\n                #   but that's an acceptable case since resources managed by\n                #   oper8 should only be managed by oper8. In the rare case where\n                #   oper8 shares ownership of a resource, any conflicts should\n                #   be resoled cleanly on the next reconciliation.\n                res_id = self._get_resource_identifiers(resource_definition)\n                api_version = res_id.api_version\n                kind = res_id.kind\n                name = res_id.name\n                namespace = res_id.namespace\n                success, content = self.get_object_current_state(\n                    kind=kind,\n                    name=name,\n                    namespace=namespace,\n                    api_version=api_version,\n                )\n                assert_cluster(\n                    success and content is not None,\n                    (\n                        \"Failed to fetch updated resourceVersion for \"\n                        + f\"{namespace}/{api_version}/{kind}/{name}\"\n                    ),\n                )\n                updated_resource_version = content.get(\"metadata\", {}).get(\n                    \"resourceVersion\"\n                )\n                assert_cluster(\n                    updated_resource_version is not None,\n                    \"No updated resource version found!\",\n                )\n                log.debug3(\n                    \"Updating resourceVersion from %s -&gt; %s\",\n                    resource_definition.get(\"metadata\", {}).get(\"resourceVersion\"),\n                    updated_resource_version,\n                )\n                resource_definition.setdefault(\"metadata\", Config({}))[\n                    \"resourceVersion\"\n                ] = updated_resource_version\n\n                # Run the retry\n                log.debug3(\"Retrying\")\n                return self._run_individual_operation_with_retries(\n                    operation, remaining_retries - 1, resource_definition, **kwargs\n                )\n            raise\n\n    _ANSIBLE_COND_TYPE = \"Running\"\n    _ANSIBLE_COND_RES_READY = {\"ok\": 1, \"changed\": 0, \"skipped\": 0, \"failures\": 0}\n    _ANSIBLE_COND_RES_UNREADY = {\"ok\": 0, \"changed\": 0, \"skipped\": 0, \"failures\": 0}\n\n    def _inject_ansible_status(self, status, previous_status):\n        \"\"\"If manage_ansible_status is enabled, this will inject the right\n        ansible status values to emulate the format that ansible natively\n        supports\n        \"\"\"\n        previous_status = previous_status or {}\n\n        # Check if the oper8 status indicates readiness\n        is_ready = verify_subsystem(\n            {\"status\": status}, desired_version=oper8_status.get_version(status)\n        )\n        prev_is_ready = verify_subsystem(\n            {\"status\": previous_status},\n            desired_version=oper8_status.get_version(previous_status),\n        )\n        log.debug3(\n            \"Status shows ready? %s. Previous ready? %s\", is_ready, prev_is_ready\n        )\n\n        # Create the ansible status blob\n        ansible_result = (\n            self._ANSIBLE_COND_RES_READY if is_ready else self._ANSIBLE_COND_RES_UNREADY\n        )\n        log.debug3(\"Ansible Result: %s\", ansible_result)\n\n        # Determine if the condition has changed to know whether this is a\n        # transition time\n        current_ready_timestamp = oper8_status.get_condition(\n            oper8_status.READY_CONDITION, status\n        ).get(oper8_status.TIMESTAMP_KEY)\n        prev_ready_timestamp = oper8_status.get_condition(\n            oper8_status.READY_CONDITION, previous_status\n        ).get(oper8_status.TIMESTAMP_KEY)\n        if prev_ready_timestamp is not None and prev_is_ready == is_ready:\n            log.debug3(\"No readiness change. Not a transition.\")\n            transition_time = prev_ready_timestamp\n        else:\n            log.debug3(\n                \"Transitioning from Ready(%s) -&gt; Ready(%s)\", prev_is_ready, is_ready\n            )\n            transition_time = current_ready_timestamp\n\n        # Inject the final ansible condition\n        conditions = [\n            cond\n            for cond in status.get(\"conditions\", [])\n            if cond.get(\"type\") != self._ANSIBLE_COND_TYPE\n        ]\n        conditions.append(\n            {\n                \"type\": self._ANSIBLE_COND_TYPE,\n                \"ansibleResult\": ansible_result,\n                \"lastTransitionTime\": transition_time,\n            }\n        )\n        status[\"conditions\"] = conditions\n        log.debug4(\"Status With Ansible: %s\", status)\n\n    @classmethod\n    def _clean_manifest(cls, manifest_a: dict, manifest_b: dict) -&gt; Tuple[dict, dict]:\n        \"\"\"Clean two manifests before being compared. This removes fields that\n        change every reconcile\n\n        Returns:\n            Tuple[dict, dict]: The cleaned manifests\n        \"\"\"\n        manifest_a = copy.deepcopy(manifest_a)\n        manifest_b = copy.deepcopy(manifest_b)\n        for metadata_field in [\n            \"resourceVersion\",\n            \"generation\",\n            \"managedFields\",\n            \"uid\",\n            \"creationTimestamp\",\n        ]:\n            manifest_a.get(\"metadata\", {}).pop(metadata_field, None)\n            manifest_b.get(\"metadata\", {}).pop(metadata_field, None)\n        return (manifest_a, manifest_b)\n\n    @classmethod\n    def _manifest_diff(cls, manifest_a, manifest_b) -&gt; bool:\n        \"\"\"Helper to compare two manifests for meaningful diff while ignoring\n        fields that always change.\n\n        Returns:\n            [bool, bool]: The first bool identifies if the resource changed while the\n        \"\"\"\n\n        manifest_a, manifest_b = cls._clean_manifest(manifest_a, manifest_b)\n\n        cls._strip_last_applied([manifest_b, manifest_a])\n        diff = recursive_diff(\n            manifest_a,\n            manifest_b,\n        )\n        change = bool(diff)\n        log.debug2(\"Found change? %s\", change)\n        log.debug3(\"A: %s\", manifest_a)\n        log.debug3(\"B: %s\", manifest_b)\n        return change\n\n    @classmethod\n    def _retain_kubernetes_annotations(cls, current: dict, desired: dict) -&gt; dict:\n        \"\"\"Helper to update a desired manifest with certain annotations from the existing\n        resource. This stops other controllers from re-reconciling this resource\n\n        Returns:\n            dict: updated resource\n        \"\"\"\n\n        identifiers = cls._get_resource_identifiers(desired)\n\n        for annotation, annotation_value in (\n            current.get(\"metadata\", {}).get(\"annotations\", {}).items()\n        ):\n            for cluster_annotation in config.cluster_passthrough_annotations:\n                if cluster_annotation in annotation and annotation not in desired[\n                    \"metadata\"\n                ].get(\"annotations\", {}):\n                    log.debug4(\n                        \"Retaining annotation %s for [%s/%s/%s]\",\n                        annotation,\n                        identifiers.kind,\n                        identifiers.api_version,\n                        identifiers.name,\n                    )\n                    desired[\"metadata\"].setdefault(\"annotations\", {})[\n                        annotation\n                    ] = annotation_value\n        return desired\n\n    @classmethod\n    def _requires_replace(cls, manifest_a, manifest_b) -&gt; bool:\n        \"\"\"Helper to compare two manifests to see if they require\n        replace\n\n        Returns:\n            bool: If the resource requires a replace operation\n        \"\"\"\n\n        manifest_a, manifest_b = cls._clean_manifest(manifest_a, manifest_b)\n\n        change = bool(requires_replace(manifest_a, manifest_b))\n        log.debug2(\"Requires Replace? %s\", change)\n        return change\n\n    # Internal struct to hold the key resource identifier elements\n    _ResourceIdentifiers = namedtuple(\n        \"ResourceIdentifiers\", [\"api_version\", \"kind\", \"name\", \"namespace\"]\n    )\n\n    @classmethod\n    def _get_resource_identifiers(cls, resource_definition, require_api_version=True):\n        \"\"\"Helper for getting the required parts of a single resource definition\"\"\"\n        api_version = resource_definition.get(\"apiVersion\")\n        kind = resource_definition.get(\"kind\")\n        name = resource_definition.get(\"metadata\", {}).get(\"name\")\n        namespace = resource_definition.get(\"metadata\", {}).get(\"namespace\")\n        assert None not in [\n            kind,\n            name,\n        ], \"Cannot apply resource without kind or name\"\n        assert (\n            not require_api_version or api_version is not None\n        ), \"Cannot apply resource without apiVersion\"\n        return cls._ResourceIdentifiers(api_version, kind, name, namespace)\n\n    ################\n    ## Operations ##\n    ################\n\n    def _replace_resource(self, resource_definition: dict) -&gt; dict:\n        \"\"\"Helper function to forcibly replace a resource on the cluster\"\"\"\n        # Get the key elements of the resource\n        res_id = self._get_resource_identifiers(resource_definition)\n        api_version = res_id.api_version\n        kind = res_id.kind\n        name = res_id.name\n        namespace = res_id.namespace\n\n        # Strip out managedFields to let the sever set them\n        resource_definition[\"metadata\"][\"managedFields\"] = None\n\n        # Get the resource handle\n        log.debug2(\"Fetching resource handle [%s/%s]\", api_version, kind)\n        resource_handle = self._get_resource_handle(api_version=api_version, kind=kind)\n        assert_cluster(\n            resource_handle,\n            (\n                \"Failed to fetch resource handle for \"\n                + f\"{namespace}/{api_version}/{kind}\"\n            ),\n        )\n\n        log.debug2(\n            \"Attempting to put [%s/%s/%s] in %s\",\n            api_version,\n            kind,\n            name,\n            namespace,\n        )\n        return resource_handle.replace(\n            resource_definition,\n            name=name,\n            namespace=namespace,\n            field_manager=\"oper8\",\n        ).to_dict()\n\n    def _apply_resource(self, resource_definition: dict) -&gt; dict:\n        \"\"\"Helper function to apply a single resource to the cluster\"\"\"\n        # Get the key elements of the resource\n        res_id = self._get_resource_identifiers(resource_definition)\n        api_version = res_id.api_version\n        kind = res_id.kind\n        name = res_id.name\n        namespace = res_id.namespace\n\n        # Strip out managedFields to let the sever set them\n        resource_definition[\"metadata\"][\"managedFields\"] = None\n\n        # Get the resource handle\n        log.debug2(\"Fetching resource handle [%s/%s]\", api_version, kind)\n        resource_handle = self._get_resource_handle(api_version=api_version, kind=kind)\n        assert_cluster(\n            resource_handle,\n            (\n                \"Failed to fetch resource handle for \"\n                + f\"{namespace}/{api_version}/{kind}\"\n            ),\n        )\n\n        log.debug2(\n            \"Attempting to apply [%s/%s/%s] in %s\",\n            api_version,\n            kind,\n            name,\n            namespace,\n        )\n        try:\n            return resource_handle.server_side_apply(\n                resource_definition,\n                name=name,\n                namespace=namespace,\n                field_manager=\"oper8\",\n            ).to_dict()\n        except ConflictError:\n            log.debug(\n                \"Overriding field manager conflict for [%s/%s/%s] in %s \",\n                api_version,\n                kind,\n                name,\n                namespace,\n            )\n            return resource_handle.server_side_apply(\n                resource_definition,\n                name=name,\n                namespace=namespace,\n                field_manager=\"oper8\",\n                force_conflicts=True,\n            ).to_dict()\n\n    def _apply(self, resource_definition, method: DeployMethod):\n        \"\"\"Apply a single resource to the cluster\n\n        Args:\n            resource_definition:  dict\n                The resource manifest to apply\n\n        Returns:\n            changed:  bool\n                Whether or not the apply resulted in a meaningful change\n        \"\"\"\n        changed = False\n\n        res_id = self._get_resource_identifiers(resource_definition)\n        api_version = res_id.api_version\n        kind = res_id.kind\n        name = res_id.name\n        namespace = res_id.namespace\n\n        # Get the current resource state\n        success, current = self.get_object_current_state(\n            kind=kind,\n            name=name,\n            namespace=namespace,\n            api_version=api_version,\n        )\n        assert_cluster(\n            success,\n            (\n                \"Failed to fetch current state for \"\n                + f\"{namespace}/{api_version}/{kind}/{name}\"\n            ),\n        )\n        if not current:\n            current = {}\n\n        # Determine if there will be a meaningful change (ignoring fields that\n        # always change)\n        changed = self._manifest_diff(current, resource_definition)\n\n        # If there is meaningful change, apply this instance\n        if changed:\n\n            resource_definition = self._retain_kubernetes_annotations(\n                current, resource_definition\n            )\n\n            req_replace = False\n            if method is DeployMethod.DEFAULT:\n                req_replace = self._requires_replace(current, resource_definition)\n\n            log.debug2(\n                \"Attempting to deploy [%s/%s/%s] in %s with %s\",\n                api_version,\n                kind,\n                name,\n                namespace,\n                method,\n            )\n            # If the resource requires a replace operation then use put. Otherwise use\n            # server side apply\n            if (\n                (req_replace or method is DeployMethod.REPLACE)\n                and method != DeployMethod.UPDATE\n                and current != {}\n            ):\n                apply_res = self._replace_resource(\n                    resource_definition,\n                )\n            else:\n                try:\n                    apply_res = self._apply_resource(resource_definition)\n                except UnprocessibleEntityError as err:\n                    log.debug3(\"Caught 422 error: %s\", err, exc_info=True)\n                    if config.deploy_unprocessable_put_fallback:\n                        log.debug(\"Falling back to PUT on 422: %s\", err)\n                        apply_res = self._replace_resource(\n                            resource_definition,\n                        )\n                    else:\n                        raise\n\n            # Recompute the diff to determine if the apply actually caused a\n            # meaningful change. This may have a different result than the check\n            # above because the applied manifest does not always result in the\n            # resource looking identical (e.g. removing field from applied =\n            # manifest does not delete from the resource).\n            changed = self._manifest_diff(current, apply_res)\n\n        return changed\n\n    def _disable(self, resource_definition):\n        \"\"\"Disable a single resource to the cluster if it exists\n\n        Args:\n            resource_definition:  dict\n                The resource manifest to disable\n\n        Returns:\n            changed:  bool\n                Whether or not the disable resulted in a meaningful change\n        \"\"\"\n        changed = False\n\n        # Get the key elements of the resource\n        res_id = self._get_resource_identifiers(resource_definition)\n        api_version = res_id.api_version\n        kind = res_id.kind\n        name = res_id.name\n        namespace = res_id.namespace\n\n        # Get the resource handle, handling missing kinds as success without\n        # change\n        log.debug2(\"Fetching resource [%s/%s]\", api_version, kind)\n        try:\n            # Get a handle to the kind. This may fail with ResourceNotFoundError\n            resource_handle = self.client.resources.get(\n                api_version=api_version, kind=kind\n            )\n\n            # If resource is not namespaced set kubernetes api namespaced to false\n            if not namespace:\n                resource_handle.namespaced = False\n\n            # Attempt to delete this instance. This ay fail with NotFoundError\n            log.debug2(\n                \"Attempting to delete [%s/%s/%s] from %s\",\n                api_version,\n                kind,\n                name,\n                namespace,\n            )\n            resource_handle.delete(name=name, namespace=namespace)\n            changed = True\n\n        # If the kind or instance is not found, that's a success without change\n        except (ResourceNotFoundError, NotFoundError) as err:\n            log.debug2(\"Valid error caught when disabling [%s/%s]: %s\", kind, name, err)\n\n        return changed\n\n    def _set_status(self, resource_definition, status):\n        \"\"\"Disable a single resource to the cluster if it exists\n\n        Args:\n            resource_definition:  dict\n                A dummy manifest holding the resource identifiers\n            status:  dict\n                The status to apply\n\n        Returns:\n            changed:  bool\n                Whether or not the status update resulted in a meaningful change\n        \"\"\"\n        changed = False\n\n        # Get the key elements of the resource\n        res_id = self._get_resource_identifiers(\n            resource_definition, require_api_version=False\n        )\n        api_version = res_id.api_version\n        kind = res_id.kind\n        name = res_id.name\n        namespace = res_id.namespace\n        resource_handle = self.client.resources.get(api_version=api_version, kind=kind)\n\n        # If resource is not namespaced set kubernetes api namespaced to false\n        if not namespace:\n            resource_handle.namespaced = False\n\n        with self._status_lock:\n            # Get the resource itself\n            resource = resource_handle.get(name=name, namespace=namespace).to_dict()\n\n            # Get the previous status and compare with the proposed status\n            log.debug2(\n                \"Resource version: %s\",\n                resource.get(\"metadata\", {}).get(\"resourceVersion\"),\n            )\n            previous_status = resource.get(\"status\")\n            if previous_status == status:\n                log.debug(\"Status has not changed. No update\")\n\n            else:\n                # Inject the ansible status if enabled\n                if self.manage_ansible_status:\n                    log.debug2(\"Injecting ansible status\")\n                    self._inject_ansible_status(status, previous_status)\n\n                # Overwrite the status\n                resource[\"status\"] = status\n                resource_handle.status.replace(body=resource).to_dict()\n                log.debug2(\n                    \"Successfully set the status for [%s/%s] in %s\",\n                    kind,\n                    name,\n                    namespace,\n                )\n                changed = True\n\n            return changed\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.openshift_deploy_manager.OpenshiftDeployManager.client","title":"<code>client</code>  <code>property</code>","text":"<p>Lazy property access to the client</p>"},{"location":"API%20References/#oper8.deploy_manager.openshift_deploy_manager.OpenshiftDeployManager.__init__","title":"<code>__init__(manage_ansible_status=False, owner_cr=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>manage_ansible_status</code> <code>bool</code> <p>bool If true, oper8 will emulate the status management done natively by ansible based on the readiness values of oper8's native status management</p> <code>False</code> <code>owner_cr</code> <code>Optional[dict]</code> <p>Optional[dict] The dict content of the CR that triggered this reconciliation. If given, deployed objects will have an ownerReference added to assign ownership to this CR instance.</p> <code>None</code> Source code in <code>oper8/deploy_manager/openshift_deploy_manager.py</code> <pre><code>def __init__(\n    self,\n    manage_ansible_status: bool = False,\n    owner_cr: Optional[dict] = None,\n):\n    \"\"\"\n    Args:\n        manage_ansible_status:  bool\n            If true, oper8 will emulate the status management done natively\n            by ansible based on the readiness values of oper8's native status\n            management\n        owner_cr:  Optional[dict]\n            The dict content of the CR that triggered this reconciliation.\n            If given, deployed objects will have an ownerReference added to\n            assign ownership to this CR instance.\n    \"\"\"\n    self.manage_ansible_status = manage_ansible_status\n    self._owner_cr = owner_cr\n\n    # Set up the client\n    log.debug(\"Initializing openshift client\")\n    self._client = None\n\n    # Keep a threading lock for performing status updates. This is necessary\n    # to avoid running into 409 Conflict errors if concurrent threads are\n    # trying to perform status updates\n    self._status_lock = threading.Lock()\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.openshift_deploy_manager.OpenshiftDeployManager.deploy","title":"<code>deploy(resource_definitions, manage_owner_references=True, retry_operation=True, method=DeployMethod.DEFAULT, **_)</code>","text":"<p>Deploy using the openshift client</p> <p>Parameters:</p> Name Type Description Default <code>resource_definitions</code> <code>List[dict]</code> <p>list(dict) List of resource object dicts to apply to the cluster</p> required <code>manage_owner_references</code> <code>bool</code> <p>bool If true, ownerReferences for the parent CR will be applied to the deployed object</p> <code>True</code> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool True if deploy succeeded, False otherwise</p> <code>changed</code> <code>bool</code> <p>bool Whether or not the deployment resulted in changes</p> Source code in <code>oper8/deploy_manager/openshift_deploy_manager.py</code> <pre><code>@alog.logged_function(log.debug)\ndef deploy(\n    self,\n    resource_definitions: List[dict],\n    manage_owner_references: bool = True,\n    retry_operation: bool = True,\n    method: DeployMethod = DeployMethod.DEFAULT,\n    **_,  # Accept any kwargs to compatibility\n) -&gt; Tuple[bool, bool]:\n    \"\"\"Deploy using the openshift client\n\n    Args:\n        resource_definitions:  list(dict)\n            List of resource object dicts to apply to the cluster\n        manage_owner_references:  bool\n            If true, ownerReferences for the parent CR will be applied to\n            the deployed object\n\n    Returns:\n        success:  bool\n            True if deploy succeeded, False otherwise\n        changed:  bool\n            Whether or not the deployment resulted in changes\n    \"\"\"\n    return self._retried_operation(\n        resource_definitions,\n        self._apply,\n        max_retries=config.deploy_retries if retry_operation else 0,\n        manage_owner_references=manage_owner_references,\n        method=method,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.openshift_deploy_manager.OpenshiftDeployManager.disable","title":"<code>disable(resource_definitions)</code>","text":"<p>The disable process is the same as the deploy process, but the child module params are set to 'state: absent'</p> <p>Parameters:</p> Name Type Description Default <code>resource_definitions</code> <code>List[dict]</code> <p>list(dict) List of resource object dicts to apply to the cluster</p> required <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool True if deploy succeeded, False otherwise</p> <code>changed</code> <code>bool</code> <p>bool Whether or not the delete resulted in changes</p> Source code in <code>oper8/deploy_manager/openshift_deploy_manager.py</code> <pre><code>@alog.logged_function(log.debug)\ndef disable(self, resource_definitions: List[dict]) -&gt; Tuple[bool, bool]:\n    \"\"\"The disable process is the same as the deploy process, but the child\n    module params are set to 'state: absent'\n\n    Args:\n        resource_definitions:  list(dict)\n            List of resource object dicts to apply to the cluster\n\n    Returns:\n        success:  bool\n            True if deploy succeeded, False otherwise\n        changed:  bool\n            Whether or not the delete resulted in changes\n    \"\"\"\n    return self._retried_operation(\n        resource_definitions,\n        self._disable,\n        max_retries=config.deploy_retries,\n        manage_owner_references=False,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.openshift_deploy_manager.OpenshiftDeployManager.filter_objects_current_state","title":"<code>filter_objects_current_state(kind, namespace=None, api_version=None, label_selector=None, field_selector=None)</code>","text":"<p>The filter_objects_current_state function fetches a list of objects that match either/both the label or field selector Args:     kind:  str         The kind of the object to fetch     namespace:  str         The namespace to search for the object     api_version:  str         The api_version of the resource kind to fetch     label_selector:  str         The label_selector to filter the resources     field_selector:  str         The field_selector to filter the resources</p> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool Whether or not the state fetch operation succeeded</p> <code>current_state</code> <code>List[dict]</code> <p>List[dict] A list of  dict representations for the objects configuration, or an empty list if no objects match</p> Source code in <code>oper8/deploy_manager/openshift_deploy_manager.py</code> <pre><code>def filter_objects_current_state(  # pylint: disable=too-many-arguments\n    self,\n    kind: str,\n    namespace: Optional[str] = None,\n    api_version: Optional[str] = None,\n    label_selector: Optional[str] = None,\n    field_selector: Optional[str] = None,\n) -&gt; Tuple[bool, List[dict]]:\n    \"\"\"The filter_objects_current_state function fetches a list of objects\n    that match either/both the label or field selector\n    Args:\n        kind:  str\n            The kind of the object to fetch\n        namespace:  str\n            The namespace to search for the object\n        api_version:  str\n            The api_version of the resource kind to fetch\n        label_selector:  str\n            The label_selector to filter the resources\n        field_selector:  str\n            The field_selector to filter the resources\n\n    Returns:\n        success:  bool\n            Whether or not the state fetch operation succeeded\n        current_state:  List[dict]\n            A list of  dict representations for the objects configuration,\n            or an empty list if no objects match\n    \"\"\"\n    # Use the lazy discovery tool to first get all objects of the given type\n    # in the given namespace, then look for the specific resource by name\n    resources = self._get_resource_handle(kind, api_version)\n    if not resources:\n        return True, []\n\n    if not namespace:\n        resources.namespaced = False\n\n    try:\n        list_obj = resources.get(\n            label_selector=label_selector,\n            field_selector=field_selector,\n            namespace=namespace,\n        )\n    except ForbiddenError:\n        log.debug(\n            \"Fetching objects of kind [%s] forbidden in namespace [%s]\",\n            kind,\n            namespace,\n        )\n        return False, []\n    except NotFoundError:\n        log.debug(\n            \"No objects of kind [%s] found in namespace [%s]\", kind, namespace\n        )\n        return True, []\n\n    # If the resource was found, get it's dict representation\n    resource_list = list_obj.to_dict().get(\"items\", [])\n    return True, resource_list\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.openshift_deploy_manager.OpenshiftDeployManager.get_object_current_state","title":"<code>get_object_current_state(kind, name, namespace=None, api_version=None)</code>","text":"<p>The get_current_objects function fetches the current state using calls directly to the api client</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>str The kind of the object ot fetch</p> required <code>name</code> <code>str</code> <p>str The full name of the object to fetch</p> required <code>namespace</code> <code>Optional[str]</code> <p>Optional[str] The namespace to search for the object or None for no namespace</p> <code>None</code> <code>api_version</code> <code>Optional[str]</code> <p>Optional[str] The api_version of the resource kind to fetch</p> <code>None</code> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool Whether or not the state fetch operation succeeded</p> <code>current_state</code> <code>dict</code> <p>dict or None The dict representation of the current object's configuration, or None if not present</p> Source code in <code>oper8/deploy_manager/openshift_deploy_manager.py</code> <pre><code>def get_object_current_state(\n    self,\n    kind: str,\n    name: str,\n    namespace: Optional[str] = None,\n    api_version: Optional[str] = None,\n) -&gt; Tuple[bool, dict]:\n    \"\"\"The get_current_objects function fetches the current state using\n    calls directly to the api client\n\n    Args:\n        kind:  str\n            The kind of the object ot fetch\n        name:  str\n            The full name of the object to fetch\n        namespace:  Optional[str]\n            The namespace to search for the object or None for no namespace\n        api_version:  Optional[str]\n            The api_version of the resource kind to fetch\n\n    Returns:\n        success:  bool\n            Whether or not the state fetch operation succeeded\n        current_state:  dict or None\n            The dict representation of the current object's configuration,\n            or None if not present\n    \"\"\"\n\n    # Use the lazy discovery tool to first get all objects of the given type\n    # in the given namespace, then look for the specific resource by name\n    resources = self._get_resource_handle(kind, api_version)\n    if not resources:\n        return True, None\n\n    if not namespace:\n        resources.namespaced = False\n\n    try:\n        resource = resources.get(name=name, namespace=namespace)\n    except ForbiddenError:\n        log.debug(\n            \"Fetching objects of kind [%s] forbidden in namespace [%s]\",\n            kind,\n            namespace,\n        )\n        return False, None\n    except NotFoundError:\n        log.debug(\n            \"No object named [%s/%s] found in namespace [%s]\", kind, name, namespace\n        )\n        return True, None\n\n    # If the resource was found, return it's dict representation\n    return True, resource.to_dict()\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.openshift_deploy_manager.OpenshiftDeployManager.set_status","title":"<code>set_status(kind, name, namespace, status, api_version=None)</code>","text":"<p>Set the status in the cluster manifest for an object managed by this operator</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>str The kind of the object ot fetch</p> required <code>name</code> <code>str</code> <p>str The full name of the object to fetch</p> required <code>namespace</code> <code>Optional[str]</code> <p>Optional[str] The namespace to search for the object.</p> required <code>status</code> <code>dict</code> <p>dict The status object to set onto the given object</p> required <code>api_version</code> <code>Optional[str]</code> <p>Optional[str] The api_version of the resource to update</p> <code>None</code> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool Whether or not the status update operation succeeded</p> <code>changed</code> <code>bool</code> <p>bool Whether or not the status update resulted in a change</p> Source code in <code>oper8/deploy_manager/openshift_deploy_manager.py</code> <pre><code>def set_status(  # pylint: disable=too-many-arguments\n    self,\n    kind: str,\n    name: str,\n    namespace: Optional[str],\n    status: dict,\n    api_version: Optional[str] = None,\n) -&gt; Tuple[bool, bool]:\n    \"\"\"Set the status in the cluster manifest for an object managed by this\n    operator\n\n    Args:\n        kind:  str\n            The kind of the object ot fetch\n        name:  str\n            The full name of the object to fetch\n        namespace:  Optional[str]\n            The namespace to search for the object.\n        status:  dict\n            The status object to set onto the given object\n        api_version:  Optional[str]\n            The api_version of the resource to update\n\n    Returns:\n        success:  bool\n            Whether or not the status update operation succeeded\n        changed:  bool\n            Whether or not the status update resulted in a change\n    \"\"\"\n    # Create a dummy resource to use in the common retry function\n    resource_definitions = [\n        {\n            \"kind\": kind,\n            \"apiVersion\": api_version,\n            \"metadata\": {\n                \"name\": name,\n                \"namespace\": namespace,\n            },\n        }\n    ]\n\n    # Run it with retries\n    return self._retried_operation(\n        resource_definitions,\n        self._set_status,\n        max_retries=config.deploy_retries,\n        status=status,\n        manage_owner_references=False,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.owner_references","title":"<code>owner_references</code>","text":"<p>This module holds common functionality that the DeployManager implementations can use to manage ownerReferences on deployed resources</p>"},{"location":"API%20References/#oper8.deploy_manager.owner_references.update_owner_references","title":"<code>update_owner_references(deploy_manager, owner_cr, child_obj)</code>","text":"<p>Fetch current ownerReferences and merge a reference for this CR into the child object</p> Source code in <code>oper8/deploy_manager/owner_references.py</code> <pre><code>def update_owner_references(\n    deploy_manager: DeployManagerBase,\n    owner_cr: dict,\n    child_obj: dict,\n):\n    \"\"\"Fetch current ownerReferences and merge a reference for this CR into\n    the child object\n    \"\"\"\n\n    # Validate the shape of the owner CR and the chid object\n    _validate_object_struct(owner_cr)\n    _validate_object_struct(child_obj)\n\n    # Fetch the current state of this object\n    kind = child_obj[\"kind\"]\n    api_version = child_obj[\"apiVersion\"]\n    name = child_obj[\"metadata\"][\"name\"]\n    namespace = child_obj[\"metadata\"][\"namespace\"]\n    uid = child_obj[\"metadata\"].get(\"uid\")\n\n    success, content = deploy_manager.get_object_current_state(\n        kind=kind, name=name, api_version=api_version, namespace=namespace\n    )\n    assert_cluster(\n        success, f\"Failed to fetch current state of {api_version}.{kind}/{name}\"\n    )\n\n    # Get the current ownerReferences\n    owner_refs = []\n    if content is not None:\n        owner_refs = content.get(\"metadata\", {}).get(\"ownerReferences\", [])\n        log.debug3(\"Current owner refs: %s\", owner_refs)\n\n    # If the current CR is not represented and current CR is in the same\n    # namespace as the child object, add it\n    current_uid = owner_cr[\"metadata\"][\"uid\"]\n    log.debug3(\"Current CR UID: %s\", current_uid)\n    current_namespace = owner_cr[\"metadata\"][\"namespace\"]\n    log.debug3(\"Current CR namespace: %s\", current_namespace)\n\n    if current_uid == uid:\n        log.debug2(\"Owner is same as child; Not adding owner ref\")\n        return\n\n    if (namespace == current_namespace) and (\n        current_uid not in [ref[\"uid\"] for ref in owner_refs]\n    ):\n        log.debug2(\n            \"Adding current CR owner reference for %s.%s/%s\",\n            api_version,\n            kind,\n            name,\n        )\n        owner_refs.append(_make_owner_reference(owner_cr))\n\n    # Add the ownerReferences to the object that will be applied to the\n    # cluster\n    log.debug4(\"Final owner refs: %s\", owner_refs)\n    child_obj[\"metadata\"][\"ownerReferences\"] = owner_refs\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.replace_utils","title":"<code>replace_utils</code>","text":"<p>This file contains common utilities for detecting if a replace operation is required for a resource</p>"},{"location":"API%20References/#oper8.deploy_manager.replace_utils.modified_lists","title":"<code>modified_lists(current_manifest, desired_manifest, in_list=False)</code>","text":"<p>Helper function to check if there are any differences in the lists of the desired manifest. This is required because Kubernetes combines lists which is often not the desired use</p> Source code in <code>oper8/deploy_manager/replace_utils.py</code> <pre><code>def modified_lists(\n    current_manifest: dict, desired_manifest: dict, in_list: bool = False\n) -&gt; bool:\n    \"\"\"Helper function to check if there are any differences in the lists of the desired manifest.\n    This is required because Kubernetes combines lists which is often not the desired use\n    \"\"\"\n    # If type mismatch then assume replace\n    if (\n        desired_manifest\n        and current_manifest\n        and type(desired_manifest) is not type(current_manifest)\n    ):\n        log.debug4(\"Requires replace due to type mismatch\")\n        return True\n\n    if isinstance(current_manifest, list) and isinstance(desired_manifest, list):\n        # if the desired has less then the current then return True. Removing\n        # from a list requires a put\n        if len(current_manifest) &gt; len(desired_manifest):\n            log.debug4(\"Requires replace due to list deletion\")\n            return True\n        # Iterate over the desired manifest\n        for recurse_a, recurse_b in zip(current_manifest, desired_manifest):\n            if modified_lists(recurse_a, recurse_b, in_list=True):\n                return True\n    if isinstance(current_manifest, dict) and isinstance(desired_manifest, dict):\n        key_intersection = set(current_manifest.keys()).intersection(\n            set(desired_manifest.keys())\n        )\n        # If there are no common keys and we're in a list then return True\n        # this means we have a new object\n        if in_list and not key_intersection:\n            log.debug4(\"Requires replace due to no common key in list\")\n            return True\n\n        # Tack if one key has the same value. This is\n        # required for kubernetes merges\n        at_least_one_common = False\n        for k in key_intersection:\n            # Check if two objects are the same for their value operations\n            changed = False\n            if isinstance(current_manifest[k], list):\n                changed = bool(\n                    recursive_list_diff(current_manifest[k], desired_manifest[k])\n                )\n            elif isinstance(current_manifest[k], dict):\n                changed = bool(recursive_diff(current_manifest[k], desired_manifest[k]))\n            else:\n                changed = current_manifest[k] != desired_manifest[k]\n\n            if not changed:\n                at_least_one_common = True\n\n            # Recurse on modified lists\n            if modified_lists(current_manifest[k], desired_manifest[k]):\n                return True\n        if in_list and not at_least_one_common:\n            log.debug4(\"Requires replace due to no common key/value in list\")\n            return True\n    return False\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.replace_utils.modified_value_from","title":"<code>modified_value_from(current_manifest, desired_manifest)</code>","text":"<p>Helper function to check if a manifest switched from value to valueFrom. These are mutually exclusive thus they require a replace command.</p> Source code in <code>oper8/deploy_manager/replace_utils.py</code> <pre><code>def modified_value_from(current_manifest: Any, desired_manifest: Any) -&gt; bool:\n    \"\"\"Helper function to check if a manifest switched from value to valueFrom. These are mutually\n    exclusive thus they require a replace command.\n    \"\"\"\n    # If type mismatch then assume replace\n    if (\n        desired_manifest\n        and current_manifest\n        and type(desired_manifest) is not type(current_manifest)\n    ):\n        log.debug4(\"Requires replace due to type mismatch\")\n        return True\n\n    if isinstance(current_manifest, list) and isinstance(desired_manifest, list):\n        for recurse_a, recurse_b in zip(current_manifest, desired_manifest):\n            if modified_value_from(recurse_a, recurse_b):\n                return True\n    if isinstance(current_manifest, dict) and isinstance(desired_manifest, dict):\n        if (\"value\" in current_manifest and \"valueFrom\" in desired_manifest) or (\n            \"valueFrom\" in current_manifest and \"value\" in desired_manifest\n        ):\n            log.debug4(\"Requires replace due to value/valueFrom change\")\n            return True\n        else:\n            for k in set(current_manifest.keys()).intersection(\n                set(desired_manifest.keys())\n            ):\n                if modified_value_from(current_manifest[k], desired_manifest[k]):\n                    return True\n    return False\n</code></pre>"},{"location":"API%20References/#oper8.deploy_manager.replace_utils.requires_replace","title":"<code>requires_replace(current_manifest, desired_manifest)</code>","text":"<p>Function to determine if a resource requires a replace operation instead of apply. This can occur due to list merging, or updating envVars</p> <p>Parameters:</p> Name Type Description Default <code>current_manifest</code> <code>dict</code> <p>The current manifest in the cluster</p> required <code>desired_manifest</code> <code>dict</code> <p>The desired manifest that should be applied</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>If the current manifest requires a replace operation</p> Source code in <code>oper8/deploy_manager/replace_utils.py</code> <pre><code>def requires_replace(current_manifest: dict, desired_manifest: dict) -&gt; bool:\n    \"\"\"Function to determine if a resource requires a replace operation instead\n    of apply. This can occur due to list merging, or updating envVars\n\n    Args:\n        current_manifest (dict): The current manifest in the cluster\n        desired_manifest (dict): The desired manifest that should be applied\n\n    Returns:\n        bool: If the current manifest requires a replace operation\n    \"\"\"\n    for func in _REPLACE_FUNCS:\n        if func(current_manifest, desired_manifest):\n            log.debug4(\"Manifest requires replace\", desired_manifest)\n            return True\n    return False\n</code></pre>"},{"location":"API%20References/#oper8.exceptions","title":"<code>exceptions</code>","text":"<p>This module implements custom exceptions</p>"},{"location":"API%20References/#oper8.exceptions.ClusterError","title":"<code>ClusterError</code>","text":"<p>               Bases: <code>Oper8FatalError</code></p> <p>Exception caused during chart construction when a cluster operation fails in an unexpected way.</p> Source code in <code>oper8/exceptions.py</code> <pre><code>class ClusterError(Oper8FatalError):\n    \"\"\"Exception caused during chart construction when a cluster operation fails\n    in an unexpected way.\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.exceptions.ConfigError","title":"<code>ConfigError</code>","text":"<p>               Bases: <code>Oper8FatalError</code></p> <p>Exception caused during usage of user-provided configuration</p> Source code in <code>oper8/exceptions.py</code> <pre><code>class ConfigError(Oper8FatalError):\n    \"\"\"Exception caused during usage of user-provided configuration\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.exceptions.Oper8DeprecationWarning","title":"<code>Oper8DeprecationWarning</code>","text":"<p>               Bases: <code>DeprecationWarning</code></p> <p>This warning is issued for deprecated APIs</p> Source code in <code>oper8/exceptions.py</code> <pre><code>class Oper8DeprecationWarning(DeprecationWarning):\n    \"\"\"This warning is issued for deprecated APIs\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.exceptions.Oper8Error","title":"<code>Oper8Error</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for all oper8 exceptions</p> Source code in <code>oper8/exceptions.py</code> <pre><code>class Oper8Error(Exception):\n    \"\"\"Base class for all oper8 exceptions\"\"\"\n\n    def __init__(self, message: str, is_fatal_error: bool):\n        \"\"\"Construct with a flag indicating whether this is a fatal error. This\n        will be a static property of all children.\n        \"\"\"\n        super().__init__(message)\n        self._is_fatal_error = is_fatal_error\n\n    @property\n    def is_fatal_error(self):\n        \"\"\"Property indicating whether or not this error should signal a fatal\n        state in the rollout\n        \"\"\"\n        return self._is_fatal_error\n</code></pre>"},{"location":"API%20References/#oper8.exceptions.Oper8Error.is_fatal_error","title":"<code>is_fatal_error</code>  <code>property</code>","text":"<p>Property indicating whether or not this error should signal a fatal state in the rollout</p>"},{"location":"API%20References/#oper8.exceptions.Oper8Error.__init__","title":"<code>__init__(message, is_fatal_error)</code>","text":"<p>Construct with a flag indicating whether this is a fatal error. This will be a static property of all children.</p> Source code in <code>oper8/exceptions.py</code> <pre><code>def __init__(self, message: str, is_fatal_error: bool):\n    \"\"\"Construct with a flag indicating whether this is a fatal error. This\n    will be a static property of all children.\n    \"\"\"\n    super().__init__(message)\n    self._is_fatal_error = is_fatal_error\n</code></pre>"},{"location":"API%20References/#oper8.exceptions.Oper8ExpectedError","title":"<code>Oper8ExpectedError</code>","text":"<p>               Bases: <code>Oper8Error</code></p> <p>An Oper8ExpectedError is one that indicates an expected failure condition that should cause a reconciliation to terminate, but is expected to resolve in a subsequent reconciliation.</p> Source code in <code>oper8/exceptions.py</code> <pre><code>class Oper8ExpectedError(Oper8Error):\n    \"\"\"An Oper8ExpectedError is one that indicates an expected failure condition\n    that should cause a reconciliation to terminate, but is expected to resolve\n    in a subsequent reconciliation.\n    \"\"\"\n\n    def __init__(self, message: str = \"\"):\n        super().__init__(message=message, is_fatal_error=False)\n</code></pre>"},{"location":"API%20References/#oper8.exceptions.Oper8FatalError","title":"<code>Oper8FatalError</code>","text":"<p>               Bases: <code>Oper8Error</code></p> <p>An Oper8FatalError is one that indicates an unexpected, and likely unrecoverable, failure during a reconciliation.</p> Source code in <code>oper8/exceptions.py</code> <pre><code>class Oper8FatalError(Oper8Error):\n    \"\"\"An Oper8FatalError is one that indicates an unexpected, and likely\n    unrecoverable, failure during a reconciliation.\n    \"\"\"\n\n    def __init__(self, message: str = \"\"):\n        super().__init__(message=message, is_fatal_error=True)\n</code></pre>"},{"location":"API%20References/#oper8.exceptions.Oper8PendingDeprecationWarning","title":"<code>Oper8PendingDeprecationWarning</code>","text":"<p>               Bases: <code>PendingDeprecationWarning</code></p> <p>This warning is issued for APIs that are still supported but will be removed eventually</p> Source code in <code>oper8/exceptions.py</code> <pre><code>class Oper8PendingDeprecationWarning(PendingDeprecationWarning):\n    \"\"\"This warning is issued for APIs that are still supported but will be removed eventually\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.exceptions.PreconditionError","title":"<code>PreconditionError</code>","text":"<p>               Bases: <code>Oper8ExpectedError</code></p> <p>Exception caused during chart construction when an expected precondition is not met.</p> Source code in <code>oper8/exceptions.py</code> <pre><code>class PreconditionError(Oper8ExpectedError):\n    \"\"\"Exception caused during chart construction when an expected precondition\n    is not met.\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.exceptions.RolloutError","title":"<code>RolloutError</code>","text":"<p>               Bases: <code>Oper8FatalError</code></p> <p>Exception indicating a failure during application rollout</p> Source code in <code>oper8/exceptions.py</code> <pre><code>class RolloutError(Oper8FatalError):\n    \"\"\"Exception indicating a failure during application rollout\"\"\"\n\n    def __init__(self, message: str = \"\", completion_state=None):\n        self.completion_state = completion_state\n        super().__init__(message)\n</code></pre>"},{"location":"API%20References/#oper8.exceptions.VerificationError","title":"<code>VerificationError</code>","text":"<p>               Bases: <code>Oper8ExpectedError</code></p> <p>Exception caused during resource verification when a desired verification state is not reached.</p> Source code in <code>oper8/exceptions.py</code> <pre><code>class VerificationError(Oper8ExpectedError):\n    \"\"\"Exception caused during resource verification when a desired verification\n    state is not reached.\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.exceptions.assert_cluster","title":"<code>assert_cluster(condition, message='')</code>","text":"<p>Replacement for assert() which will throw a ClusterError. This should be used when building a chart which requires that an operation in the cluster (such as fetching an existing secret) succeeds.</p> Source code in <code>oper8/exceptions.py</code> <pre><code>def assert_cluster(condition: bool, message: str = \"\"):\n    \"\"\"Replacement for assert() which will throw a ClusterError. This should\n    be used when building a chart which requires that an operation in the\n    cluster (such as fetching an existing secret) succeeds.\n    \"\"\"\n    if not condition:\n        raise ClusterError(message)\n</code></pre>"},{"location":"API%20References/#oper8.exceptions.assert_config","title":"<code>assert_config(condition, message='')</code>","text":"<p>Replacement for assert() which will throw a ConfigError. This should be used when building a chart which requires that certain conditions be true in the deploy_config or app_config.</p> Source code in <code>oper8/exceptions.py</code> <pre><code>def assert_config(condition: bool, message: str = \"\"):\n    \"\"\"Replacement for assert() which will throw a ConfigError. This should be\n    used when building a chart which requires that certain conditions be true in\n    the deploy_config or app_config.\n    \"\"\"\n    if not condition:\n        raise ConfigError(message)\n</code></pre>"},{"location":"API%20References/#oper8.exceptions.assert_precondition","title":"<code>assert_precondition(condition, message='')</code>","text":"<p>Replacement for assert() which will throw a PreconditionError. This should be used when building a chart which requires that a precondition is met before continuing.</p> Source code in <code>oper8/exceptions.py</code> <pre><code>def assert_precondition(condition: bool, message: str = \"\"):\n    \"\"\"Replacement for assert() which will throw a PreconditionError. This\n    should be used when building a chart which requires that a precondition is\n    met before continuing.\n    \"\"\"\n    if not condition:\n        raise PreconditionError(message)\n</code></pre>"},{"location":"API%20References/#oper8.exceptions.assert_verified","title":"<code>assert_verified(condition, message='')</code>","text":"<p>Replacement for assert() which will throw a VerificationError. This should be used when verifying the state of a resource in the cluster.</p> Source code in <code>oper8/exceptions.py</code> <pre><code>def assert_verified(condition: bool, message: str = \"\"):\n    \"\"\"Replacement for assert() which will throw a VerificationError. This\n    should be used when verifying the state of a resource in the cluster.\n    \"\"\"\n    if not condition:\n        raise VerificationError(message)\n</code></pre>"},{"location":"API%20References/#oper8.log_format","title":"<code>log_format</code>","text":"<p>Custom logging formats that contain more detailed oper8 logs</p>"},{"location":"API%20References/#oper8.log_format.Oper8JsonFormatter","title":"<code>Oper8JsonFormatter</code>","text":"<p>               Bases: <code>AlogJsonFormatter</code></p> <p>Custom Log Format that extends AlogJsonFormatter to add multiple oper8 specific fields to the json. This includes things like identifiers of the resource being reconciled, reconciliationId, and thread information</p> Source code in <code>oper8/log_format.py</code> <pre><code>class Oper8JsonFormatter(AlogJsonFormatter):\n    \"\"\"Custom Log Format that extends AlogJsonFormatter to add multiple\n    oper8 specific fields to the json. This includes things like identifiers\n    of the resource being reconciled, reconciliationId, and thread information\n    \"\"\"\n\n    _FIELDS_TO_PRINT = AlogJsonFormatter._FIELDS_TO_PRINT + [\n        \"process\",\n        \"thread\",\n        \"threadName\",\n        \"kind\",\n        \"apiVersion\",\n        \"resourceVersion\",\n        \"resourceName\",\n        \"reconciliationId\",\n    ]\n\n    def __init__(self, manifest=None, reconciliation_id=None):\n        super().__init__()\n        self.manifest = manifest\n        self.reconciliation_id = reconciliation_id\n\n    def format(self, record):\n        if self.reconciliation_id:\n            record.reconciliationId = self.reconciliation_id\n\n        if resource := getattr(record, \"resource\", self.manifest):\n            record.kind = resource.get(\"kind\")\n            record.apiVersion = resource.get(\"apiVersion\")\n\n            metadata = resource.get(\"metadata\", {})\n            record.resourceVersion = metadata.get(\"resourceVersion\")\n            record.resourceName = metadata.get(\"name\")\n\n        return super().format(record)\n</code></pre>"},{"location":"API%20References/#oper8.managed_object","title":"<code>managed_object</code>","text":"<p>Helper object to represent a kubernetes object that is managed by the operator</p>"},{"location":"API%20References/#oper8.managed_object.ManagedObject","title":"<code>ManagedObject</code>","text":"<p>Basic struct to represent a managed kubernetes object</p> Source code in <code>oper8/managed_object.py</code> <pre><code>class ManagedObject:  # pylint: disable=too-many-instance-attributes\n    \"\"\"Basic struct to represent a managed kubernetes object\"\"\"\n\n    def __init__(\n        self,\n        definition: dict,\n        verify_function: Optional[Callable] = None,\n        deploy_method: Optional[\"DeployMethod\"] = None,  # noqa: F821\n    ):\n        self.kind = definition.get(\"kind\")\n        self.metadata = definition.get(\"metadata\", {})\n        self.name = self.metadata.get(\"name\")\n        self.namespace = self.metadata.get(\"namespace\")\n        self.uid = self.metadata.get(\"uid\", uuid.uuid4())\n        self.resource_version = self.metadata.get(\"resourceVersion\")\n        self.api_version = definition.get(\"apiVersion\")\n        self.definition = definition\n        self.verify_function = verify_function\n        self.deploy_method = deploy_method\n\n        # If resource is not list then check name\n        if KUBE_LIST_IDENTIFIER not in self.kind:\n            assert self.name is not None, \"No name found\"\n\n        assert self.kind is not None, \"No kind found\"\n        assert self.api_version is not None, \"No apiVersion found\"\n\n    def get(self, *args, **kwargs):\n        \"\"\"Pass get calls to the objects definition\"\"\"\n        return self.definition.get(*args, **kwargs)\n\n    def __str__(self):\n        return f\"{self.api_version}/{self.kind}/{self.name}\"\n\n    def __repr__(self):\n        return str(self)\n\n    def __hash__(self):\n        \"\"\"Hash explicitly excludes the definition so that the object's\n        identifier in a map can be based only on the unique identifier of the\n        resource in the cluster. If the original resource did not provide a unique\n        identifier then use the apiVersion, kind, and name\n        \"\"\"\n        return hash(self.metadata.get(\"uid\", str(self)))\n\n    def __eq__(self, other):\n        return hash(self) == hash(other)\n</code></pre>"},{"location":"API%20References/#oper8.managed_object.ManagedObject.__hash__","title":"<code>__hash__()</code>","text":"<p>Hash explicitly excludes the definition so that the object's identifier in a map can be based only on the unique identifier of the resource in the cluster. If the original resource did not provide a unique identifier then use the apiVersion, kind, and name</p> Source code in <code>oper8/managed_object.py</code> <pre><code>def __hash__(self):\n    \"\"\"Hash explicitly excludes the definition so that the object's\n    identifier in a map can be based only on the unique identifier of the\n    resource in the cluster. If the original resource did not provide a unique\n    identifier then use the apiVersion, kind, and name\n    \"\"\"\n    return hash(self.metadata.get(\"uid\", str(self)))\n</code></pre>"},{"location":"API%20References/#oper8.managed_object.ManagedObject.get","title":"<code>get(*args, **kwargs)</code>","text":"<p>Pass get calls to the objects definition</p> Source code in <code>oper8/managed_object.py</code> <pre><code>def get(self, *args, **kwargs):\n    \"\"\"Pass get calls to the objects definition\"\"\"\n    return self.definition.get(*args, **kwargs)\n</code></pre>"},{"location":"API%20References/#oper8.patch","title":"<code>patch</code>","text":"<p>This module holds shared semantics for patching resources using temporary_patch</p>"},{"location":"API%20References/#oper8.patch.apply_patches","title":"<code>apply_patches(internal_name, resource_definition, temporary_patches)</code>","text":"<p>Apply all temporary patches to the given resource from the given list. The patches are applied in-place.</p> <p>Parameters:</p> Name Type Description Default <code>internal_name</code> <code>str</code> <p>str The name given to the internal node of the object. This is used to identify which patches apply to this object.</p> required <code>resource_definition</code> <code>dict</code> <p>dict The dict representation of the object to patch</p> required <code>temporary_patches</code> <code>List[dict]</code> <p>List[dict] The list of temporary patches that apply to this rollout</p> required <p>Returns:</p> Name Type Description <code>patched_definition</code> <p>dict The dict representation of the object with patches applied</p> Source code in <code>oper8/patch.py</code> <pre><code>def apply_patches(\n    internal_name: str,\n    resource_definition: dict,\n    temporary_patches: List[dict],\n):\n    \"\"\"Apply all temporary patches to the given resource from the given list.\n    The patches are applied in-place.\n\n    Args:\n        internal_name:  str\n            The name given to the internal node of the object. This is used to\n            identify which patches apply to this object.\n        resource_definition:  dict\n            The dict representation of the object to patch\n        temporary_patches:  List[dict]\n            The list of temporary patches that apply to this rollout\n\n    Returns:\n        patched_definition:  dict\n            The dict representation of the object with patches applied\n    \"\"\"\n    log.debug2(\n        \"Looking for patches for %s/%s (%s)\",\n        resource_definition.get(\"kind\"),\n        resource_definition.get(\"metadata\", {}).get(\"name\"),\n        internal_name,\n    )\n    resource_definition = copy.deepcopy(resource_definition)\n    for patch_content in temporary_patches:\n        log.debug4(\"Checking patch: &lt;&lt; %s &gt;&gt;\", patch_content)\n\n        # Look to see if this patch contains a match for the internal name\n        internal_name_parts = internal_name.split(\".\")\n        internal_name_parts.reverse()\n        patch = patch_content.spec.patch\n        log.debug4(\"Full patch section: %s\", patch)\n        while internal_name_parts and isinstance(patch, dict):\n            patch_level = internal_name_parts.pop()\n            log.debug4(\"Getting patch level [%s]\", patch_level)\n            patch = patch.get(patch_level, {})\n            log.debug4(\"Patch level: %s\", patch)\n        log.debug4(\"Checking patch: %s\", patch)\n\n        # If the patch matches, apply the right merge\n        if patch and not internal_name_parts:\n            log.debug3(\"Found matching patch: %s\", patch_content.metadata.name)\n\n            # Dispatch the right patch type\n            if patch_content.spec.patchType == STRATEGIC_MERGE_PATCH:\n                resource_definition = _apply_patch_strategic_merge(\n                    resource_definition, patch\n                )\n            elif patch_content.spec.patchType == JSON_PATCH_6902:\n                resource_definition = _apply_json_patch(resource_definition, patch)\n            else:\n                raise ValueError(\n                    f\"Unsupported patch type [{patch_content.spec.patchType}]\"\n                )\n    return resource_definition\n</code></pre>"},{"location":"API%20References/#oper8.patch_strategic_merge","title":"<code>patch_strategic_merge</code>","text":"<p>This module implements Patch Strategic Merge following the semantics in:</p> <ul> <li>kustomize: https://kubectl.docs.kubernetes.io/references/kustomize/glossary/#patchstrategicmerge</li> <li>kubernetes: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/strategic-merge-patch.md</li> </ul>"},{"location":"API%20References/#oper8.patch_strategic_merge.patch_strategic_merge","title":"<code>patch_strategic_merge(resource_definition, patch, merge_patch_keys=None)</code>","text":"<p>Apply a Strategic Merge Patch based on JSON Merge Patch (rfc 7386)</p> <p>Parameters:</p> Name Type Description Default <code>resource_definition</code> <code>dict</code> <p>dict The dict representation of the kubernetes resource</p> required <code>patch</code> <code>dict</code> <p>dict The formatted patch to apply</p> required <code>merge_patch_keys</code> <code>Dict[str, str]</code> <p>Dict[str, str] The mapping from paths to merge keys used to perform merge semantics for list elements</p> <code>None</code> <p>Returns:</p> Name Type Description <code>patched_resource_definition</code> <code>dict</code> <p>dict The patched version of the resource_definition</p> Source code in <code>oper8/patch_strategic_merge.py</code> <pre><code>def patch_strategic_merge(\n    resource_definition: dict,\n    patch: dict,\n    merge_patch_keys: Dict[str, str] = None,\n) -&gt; dict:\n    \"\"\"Apply a Strategic Merge Patch based on JSON Merge Patch (rfc 7386)\n\n    Args:\n        resource_definition:  dict\n            The dict representation of the kubernetes resource\n        patch:  dict\n            The formatted patch to apply\n        merge_patch_keys:  Dict[str, str]\n            The mapping from paths to merge keys used to perform merge semantics\n            for list elements\n\n    Returns:\n        patched_resource_definition:  dict\n            The patched version of the resource_definition\n    \"\"\"\n    if merge_patch_keys is None:\n        merge_patch_keys = STRATEGIC_MERGE_PATCH_KEYS\n    return _strategic_merge(\n        current=copy.deepcopy(resource_definition),\n        desired=copy.deepcopy(patch),\n        position=resource_definition.get(\"kind\"),\n        merge_patch_keys=merge_patch_keys,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.reconcile","title":"<code>reconcile</code>","text":"<p>The ReconcileManager class manages an individual reconcile of a controller. This setups up the session, constructs the controller, and runs its reconcile</p>"},{"location":"API%20References/#oper8.reconcile.ReconcileManager","title":"<code>ReconcileManager</code>","text":"<p>This class manages reconciliations for an instance of Oper8. It's primary function is to run reconciles given a CR manifest, Controller, and the current cluster state via a DeployManager.</p> Source code in <code>oper8/reconcile.py</code> <pre><code>class ReconcileManager:  # pylint: disable=too-many-lines\n    \"\"\"This class manages reconciliations for an instance of Oper8. It's\n    primary function is to run reconciles given a CR manifest, Controller,\n    and the current cluster state via a DeployManager.\n    \"\"\"\n\n    ## Construction ############################################################\n\n    def __init__(\n        self,\n        home_dir: str = None,\n        deploy_manager: Optional[DeployManagerBase] = None,\n        enable_vcs: Optional[bool] = None,\n        reimport_controller: Optional[bool] = True,\n    ):\n        \"\"\"The constructor sets up the properties used across every\n        reconcile and checks that the current config is valid.\n\n        Args:\n            home_dir:  Optional[str]=None\n                The root directory for importing controllers or VCS checkout\n            deploy_manager:  Optional[DeployManager]=None\n                Deploy manager to use. If not given, a new DeployManager will\n                be created for each reconcile.\n            enable_vcs:  Optional[bool]=True\n                Parameter to manually control the state of VCS on a per instance\n                basis\n            reimport_controller:  Optional[bool]=None\n                Parameter to manually control if a controller needs to be reimported each\n                reconcile.\n        \"\"\"\n\n        if home_dir:\n            self.home_dir = home_dir\n        elif config.vcs.enabled:\n            self.home_dir = config.vcs.repo\n        else:\n            self.home_dir = os.getcwd()\n\n        self.vcs = None\n\n        # If enable_vcs is not provided than default to\n        # config\n        if enable_vcs is None:\n            enable_vcs = config.vcs.enabled\n\n        if enable_vcs:\n            assert_config(\n                config.vcs.repo,\n                \"Can not enable vcs without supply source repo at vcs.repo\",\n            )\n            assert_config(\n                config.vcs.dest,\n                \"Cannot require enable vcs without providing a destination\",\n            )\n            vcs_checkout_methods = [method.value for method in VCSCheckoutMethod]\n            assert_config(\n                config.vcs.checkout_method in vcs_checkout_methods,\n                f\"VCS checkout method must be one of the following {vcs_checkout_methods}\",\n            )\n\n            self.vcs = VCS(self.home_dir)\n\n        # Ensure config is setup correctly for strict_versioning\n        self.strict_version_regex = re.compile(config.strict_version_kind_regex or \".*\")\n        if config.strict_versioning:\n            assert_config(\n                config.supported_versions is not None,\n                \"Must provide supported_versions with strict_versioning=True\",\n            )\n            assert_config(\n                config.vcs.field is not None,\n                \"Must provide vcs.field with strict_versioning=True\",\n            )\n\n        self.vcs_kind_regex = re.compile(config.vcs.kind_regex or \".*\")\n\n        self.deploy_manager = deploy_manager\n        self.reimport_controller = reimport_controller\n\n    ## Reconciliation ############################################################\n\n    @alog.logged_function(log.info)\n    @alog.timed_function(log.info, \"Reconcile finished in: \")\n    def reconcile(\n        self,\n        controller_info: CONTROLLER_INFO,\n        resource: Union[dict, aconfig.Config],\n        is_finalizer: bool = False,\n    ) -&gt; ReconciliationResult:\n        \"\"\"This is the main entrypoint for reconciliations and contains the\n        core implementation. The general reconcile path is as follows:\n\n            1. Parse the raw CR manifest\n            2. Setup logging based on config with overrides from CR\n            3. Check if the CR is paused and for strict versioning\n            4. Setup directory if VCS is enabled\n            5. Import and construct the Controller\n            6. Setup the DeployManager and Session objects\n            7. Run the Controller reconcile\n\n        Args:\n            controller_info: CONTROLLER_INFO\n                The description of a controller. See CONTROLLER_INFO for\n                more information\n            resource: Union[dict, aconfig.Config]\n                A raw representation of the resource to be reconciled\n            is_finalizer: bool=False\n                Whether the resource is being deleted\n\n        Returns:\n            reconcile_result:  ReconciliationResult\n                The result of the reconcile\n        \"\"\"\n\n        # Parse the full CR content\n        cr_manifest = self.parse_manifest(resource)\n\n        # generate id unique to this session\n        reconcile_id = self.generate_id()\n\n        # Initialize logging prior to any other work\n        self.configure_logging(cr_manifest, reconcile_id)\n\n        # If paused, do nothing and don't requeue\n        if self._is_paused(cr_manifest):\n            log.info(\"CR is paused. Exiting reconciliation\")\n            result = ReconciliationResult(requeue=False, requeue_params=RequeueParams())\n            return result\n\n        # Check strict versioning before continuing\n        if self._should_check_strict_version(cr_manifest):\n            self._check_strict_versioning(cr_manifest)\n\n        # Check if VCS is enabled and then attempt to checkout\n        if config.vcs.enabled:\n            self.setup_vcs(cr_manifest)\n\n        # Import controller and setup the instance\n        controller = self.setup_controller(controller_info)\n\n        # Configure deploy manager on a per reconcile basis for\n        # owner references unless a manager is provided on initialization\n        deploy_manager = self.setup_deploy_manager(cr_manifest)\n\n        # Setup Session\n        session = self.setup_session(\n            controller, cr_manifest, deploy_manager, reconcile_id\n        )\n\n        # Run the controller reconcile\n        result = self.run_controller(controller, session, is_finalizer)\n\n        return result\n\n    def safe_reconcile(\n        self,\n        controller_info: CONTROLLER_INFO,\n        resource: dict,\n        is_finalizer: bool = False,\n    ) -&gt; ReconciliationResult:\n        \"\"\"\n        This function calls out to reconcile but catches any errors thrown. This\n        function guarantees a safe result which is needed by some Watch Managers\n\n        Args:\n            controller_info: CONTROLLER_INFO\n                The description of a controller. See CONTROLLER_INFO for\n                more information\n            resource: Union[dict, aconfig.Config]\n                A raw representation of the reconcile\n            is_finalize: bool=False\n                Whether the resource is being deleted\n\n        Returns:\n            reconcile_result:  ReconciliationResult\n                The result of the reconcile\n\n        \"\"\"\n\n        try:\n            return self.reconcile(controller_info, resource, is_finalizer)\n\n        # VCSMultiProcessError is an expected error caused by oper8 which should\n        # not be handled by the exception handling code\n        except VCSMultiProcessError as exc:\n            # Requeue after ~7.5 seconds. Add randomness to avoid\n            # repeated conflicts\n            requeue_time = 5 + random.uniform(0, 5)\n            params = RequeueParams(\n                requeue_after=datetime.timedelta(seconds=requeue_time)\n            )\n            log.debug(\"VCS Multiprocessing Error Detected: {%s}\", exc, exc_info=True)\n            log.warning(\n                \"VCS Setup failed due to other process. Requeueing in %ss\",\n                requeue_time,\n            )\n            return ReconciliationResult(\n                requeue=True, requeue_params=params, exception=exc\n            )\n\n        # Capture all generic exceptions\n        except Exception as exc:  # pylint: disable=broad-except\n            log.warning(\"Handling caught error in reconcile: %s\", exc, exc_info=True)\n            error = exc\n\n        if config.manage_status:\n            try:\n                self._update_error_status(resource, error)\n                log.debug(\"Update CR status with error message\")\n            except Exception as exc:  # pylint: disable=broad-except\n                log.error(\"Failed to update status: %s\", exc, exc_info=True)\n\n        # If we got to this return it means there was an\n        # exception during reconcile and we should requeue\n        # with the default backoff period\n        log.info(\"Requeuing CR due to error during reconcile\")\n        return ReconciliationResult(\n            requeue=True, requeue_params=RequeueParams(), exception=error\n        )\n\n    ## Reconciliation Stages ############################################################\n\n    @classmethod\n    def parse_manifest(cls, resource: Union[dict, aconfig.Config]) -&gt; aconfig.Config:\n        \"\"\"Parse a raw resource into an aconfig Config\n\n        Args:\n            resource: Union[dict, aconfig.Config])\n                The resource to be parsed into a manifest\n\n        Returns\n            cr_manifest: aconfig.Config\n                The parsed and validated config\n        \"\"\"\n        try:\n            cr_manifest = aconfig.Config(resource, override_env_vars=False)\n        except (ValueError, SyntaxError, AttributeError) as exc:\n            raise ValueError(\"Failed to parse full_cr\") from exc\n\n        return cr_manifest\n\n    @classmethod\n    def configure_logging(cls, cr_manifest: aconfig.Config, reconciliation_id: str):\n        \"\"\"Configure the logging for a given reconcile\n\n        Args:\n            cr_manifest: aconfig.Config\n                The resource to get annotation overrides from\n            reconciliation_id: str\n                The unique id for the reconciliation\n        \"\"\"\n\n        # Fetch the annotations for logging\n        # NOTE: We use safe fetching here because this happens before CR\n        #   verification in the Session constructor\n        annotations = cr_manifest.get(\"metadata\", {}).get(\"annotations\", {})\n        default_level = annotations.get(\n            constants.LOG_DEFAULT_LEVEL_NAME, config.log_level\n        )\n\n        filters = annotations.get(constants.LOG_FILTERS_NAME, config.log_filters)\n        log_json = annotations.get(constants.LOG_JSON_NAME, str(config.log_json))\n        log_thread_id = annotations.get(\n            constants.LOG_THREAD_ID_NAME, str(config.log_thread_id)\n        )\n\n        # Convert boolean args\n        log_json = (log_json or \"\").lower() == \"true\"\n        log_thread_id = (log_thread_id or \"\").lower() == \"true\"\n\n        # Keep the old handler. This is useful if running with ansible as\n        # it will preserve the handler generator set up to log to a file\n        # since ansible captures all logging output\n        handler_generator = None\n        if logging.root.handlers:\n            old_handler = logging.root.handlers[0]\n\n            def handler_generator():\n                return old_handler\n\n        alog.configure(\n            default_level=default_level,\n            filters=filters,\n            formatter=(\n                Oper8JsonFormatter(cr_manifest, reconciliation_id)\n                if log_json\n                else \"pretty\"\n            ),\n            thread_id=log_thread_id,\n            handler_generator=handler_generator,\n        )\n\n    @classmethod\n    def generate_id(cls) -&gt; str:\n        \"\"\"Generates a unique human readable id for this reconciliation\n\n        Returns:\n            id: str\n                A unique base32 encoded id\n        \"\"\"\n        uuid4 = uuid.uuid4()\n        base32_str = base64.b32encode(uuid4.bytes).decode(\"utf-8\")\n        reconcile_id = base32_str[:22]\n        log.debug(\"Generated reconcile id: %s\", reconcile_id)\n        return reconcile_id\n\n    def setup_vcs(self, cr_manifest: aconfig.Config):\n        \"\"\"Setups the VCS directory and sys.path for a reconcile.\n        This function also ensures that the version is valid if\n        config.strict_versioning is enabled.\n\n        Args:\n            cr_manifest: aconfig.Config\n                The cr manifest to pull the requested version from.\n        \"\"\"\n        if not self.vcs_kind_regex.match(cr_manifest.get(\"kind\", \"\")):\n            log.debug(\"Skipping vcs checkout due to kind regex\")\n            return\n\n        version = get_manifest_version(cr_manifest)\n        if not version:\n            raise ValueError(\"CR Manifest has no version\")\n\n        log.debug(\n            \"Setting up working directory with src: %s and version: %s\",\n            self.home_dir,\n            version,\n        )\n        working_dir = self._setup_directory(cr_manifest, version)\n\n        # Construct working dir path from vcs and git directory\n        if config.vcs.module_dir:\n            module_path = pathlib.Path(config.vcs.module_dir)\n            working_dir = working_dir / module_path\n\n        if not working_dir.is_dir():\n            log.error(\n                \"Working directory %s could not be found. Invalid module path\",\n                working_dir,\n            )\n            raise ConfigError(\n                f\"Module path: '{module_path}' could not be found in repository\"\n            )\n\n        log.debug4(\"Changing working directory to %s\", working_dir)\n        os.chdir(working_dir)\n        sys.path.insert(0, str(working_dir))\n\n    def setup_controller(\n        self, controller_info: CONTROLLER_INFO\n    ) -&gt; CONTROLLER_CLASS_TYPE:\n        \"\"\"\n        Import the requested Controller class and enable any compatibility layers\n\n        Args:\n            controller_info:CONTROLLER_INFO\n                The description of a controller. See CONTROLLER_INFO for\n                more information\n        Returns:\n            controller:\n                The required Controller Class\n        \"\"\"\n\n        # Local\n        from .controller import (  # pylint: disable=import-outside-toplevel, cyclic-import\n            Controller,\n        )\n\n        # If controller info is already a constructed controller then\n        # skip importing\n        if isinstance(controller_info, Controller):\n            return controller_info\n\n        controller_class = self._import_controller(controller_info)\n        return self._configure_controller(controller_class)\n\n    def setup_deploy_manager(self, cr_manifest: aconfig.Config) -&gt; DeployManagerBase:\n        \"\"\"\n        Configure a deploy_manager for a reconcile given a manifest\n\n        Args:\n            cr_manifest: aconfig.Config\n                The resource to be used as an owner_ref\n\n        Returns:\n            deploy_manager: DeployManagerBase\n                The deploy_manager to be used during reconcile\n        \"\"\"\n        if self.deploy_manager:\n            return self.deploy_manager\n\n        if config.dry_run:\n            log.debug(\"Using DryRunDeployManager\")\n            return DryRunDeployManager()\n\n        log.debug(\"Using OpenshiftDeployManager\")\n        return OpenshiftDeployManager(owner_cr=cr_manifest)\n\n    def setup_session(\n        self,\n        controller: CONTROLLER_TYPE,\n        cr_manifest: aconfig.Config,\n        deploy_manager: DeployManagerBase,\n        reconciliation_id: str,\n    ) -&gt; Session:\n        \"\"\"Construct the session, including gathering the backend config and any temp patches\n\n        Args:\n            controller: Controller\n                The controller class being reconciled\n            cr_manifest: aconfig.Config\n                The resource being reconciled\n            deploy_manager: DeployManagerBase\n                The deploy manager used in the cluster\n            reconciliation_id: str\n                The id for the reconcile\n\n        Return:\n            session: Session\n                The session for reconcile\n        \"\"\"\n        # Get backend config for reconciliation\n        controller_defaults = controller.get_config_defaults()\n        reconciliation_config = self._get_reconcile_config(\n            cr_manifest=cr_manifest,\n            deploy_manager=deploy_manager,\n            controller_defaults=controller_defaults,\n        )\n        log.debug4(\"Gathered Config: %s\", reconciliation_config)\n\n        # Get Temporary patches\n        patches = self._get_temp_patches(deploy_manager, cr_manifest)\n        log.debug3(\"Found %d patches\", len(patches))\n\n        # Get the complete CR Manifest including defaults\n        cr_manifest_defaults = controller.get_cr_manifest_defaults()\n        full_cr_manifest = merge_configs(\n            aconfig.Config(cr_manifest_defaults),\n            cr_manifest,\n        )\n\n        return Session(\n            reconciliation_id=reconciliation_id,\n            cr_manifest=full_cr_manifest,\n            config=reconciliation_config,\n            deploy_manager=deploy_manager,\n            temporary_patches=patches,\n        )\n\n    def run_controller(\n        self, controller: CONTROLLER_TYPE, session: Session, is_finalizer: bool\n    ) -&gt; ReconciliationResult:\n        \"\"\"Run the Controller's reconciliation or finalizer with the constructed Session.\n        This function also updates the CR status and handles requeue logic.\n\n        Args:\n            controller: Controller\n                The Controller being reconciled\n            session: Session\n                The current Session state\n            is_finalizer:\n                Whether the resource is being deleted\n\n        Returns:\n            reconciliation_result: ReconciliationResult\n                The result of the reconcile\n        \"\"\"\n        log.info(\n            \"%s resource %s/%s/%s\",\n            \"Finalizing\" if is_finalizer else \"Reconciling\",\n            session.kind,\n            session.namespace,\n            session.name,\n        )\n\n        # Ensure the resource has the proper finalizers\n        if controller.has_finalizer:\n            add_finalizer(session, controller.finalizer)\n\n        # Update the Resource status\n        if config.manage_status:\n            self._update_reconcile_start_status(session)\n\n        # Reconcile the controller\n        completion_state = controller.run_reconcile(\n            session,\n            is_finalizer=is_finalizer,\n        )\n\n        if config.manage_status:\n            self._update_reconcile_completion_status(session, completion_state)\n\n        # Check if the controller session should requeue\n        requeue, requeue_params = controller.should_requeue(session)\n        if not requeue_params:\n            requeue_params = RequeueParams()\n\n        # Remove managed finalizers if not requeuing\n        if not requeue and is_finalizer and controller.has_finalizer:\n            remove_finalizer(session, controller.finalizer)\n\n        return ReconciliationResult(requeue=requeue, requeue_params=requeue_params)\n\n    ## Implementation Details ############################################################\n\n    @classmethod\n    def _is_paused(cls, cr_manifest: aconfig.Config) -&gt; bool:\n        \"\"\"Check if a manifest has a paused annotation\n\n        Args:\n            cr_manifest: aconfig.Config\n                The manifest becking checked\n\n        Returns:\n            is_paused: bool\n                If the manifest contains the paused annotation\n        \"\"\"\n        annotations = cr_manifest.metadata.get(\"annotations\", {})\n        paused = annotations.get(constants.PAUSE_ANNOTATION_NAME)\n        return paused and paused.lower() == \"true\"\n\n    def _check_strict_versioning(self, cr_manifest: aconfig.Config):\n        \"\"\"Check the version against config and vcs directory\n\n        Args:\n            version_directory: str\n                The repo directory to check\n            version: str\n                The version from the manifest\n        \"\"\"\n        version = get_manifest_version(cr_manifest)\n        if not version:\n            raise ValueError(\"CR Manifest has no version\")\n\n        # Ensure version is in list of supported versions\n        assert_config(\n            version in config.supported_versions,\n            f\"Unsupported version: {version}\",\n        )\n\n        # If VCS is enabled ensure the branch or tag exists\n        if self.vcs:\n            repo_versions = self.vcs.list_refs()\n            assert_config(\n                version in repo_versions,\n                f\"Version not found in repo: {version}\",\n            )\n            log.debug3(\"Supported VCS Versions: %s\", repo_versions)\n\n    def _should_check_strict_version(self, cr_manifest: aconfig.Config) -&gt; bool:\n        \"\"\"Helper function to check if we should enforce strict versioning. Checks the\n        general config and then matches the kind regex\n\n        Args:\n            cr_manifest: aconfig.Config\n                The CR for this reconciliation\n\n        Returns:\n            bool: True if we should check this kind\n        \"\"\"\n        if not config.strict_versioning:\n            return False\n\n        return bool(self.strict_version_regex.match(cr_manifest.get(\"kind\")))\n\n    def _setup_directory(\n        self, cr_manifest: aconfig.Config, version: str\n    ) -&gt; pathlib.Path:\n        \"\"\"Construct the VCS directory from the cr_manifest and version. Then\n        checkout the directory\n\n        Args:\n            cr_manifest: aconfig.Config\n                The manifest to be used for the checkout path\n            version: str\n                The version to checkout\n\n        Returns:\n            destination_directory: pathlib.Path\n                The destination directory for the checkout\n        \"\"\"\n\n        # Generate checkout directory and ensure path exists\n        def sanitize_for_path(path):\n            keepcharacters = (\" \", \".\", \"_\")\n            return \"\".join(\n                c for c in path if c.isalnum() or c in keepcharacters\n            ).rstrip()\n\n        # Setup destination template to allow for CR specific checkout paths\n        # The entirety of the cr_manifest is included as a dict as well as some\n        # custom keys\n        template_mappings = {\n            # Include the entire dict first so that the sanitized default values\n            # take precedence\n            **cr_manifest,\n            \"version\": version,\n            \"kind\": sanitize_for_path(cr_manifest.kind),\n            \"apiVersion\": sanitize_for_path(cr_manifest.apiVersion),\n            \"namespace\": sanitize_for_path(cr_manifest.metadata.namespace),\n            \"name\": sanitize_for_path(cr_manifest.metadata.name),\n        }\n\n        # Get the checkout directory and method\n        try:\n            formatted_path = config.vcs.dest.format(**template_mappings)\n        except KeyError as exc:\n            log.warning(\n                \"Invalid key: %s found in vcs destination template\", exc, exc_info=True\n            )\n            raise ConfigError(\"Invalid Key found in vcs destination template\") from exc\n\n        checkout_dir = pathlib.Path(formatted_path)\n        checkout_method = VCSCheckoutMethod(config.vcs.checkout_method)\n\n        log.debug2(\n            \"Checking out into directory %s with method %s\",\n            checkout_dir,\n            checkout_method.value,\n        )\n        self.vcs.checkout_ref(version, checkout_dir, checkout_method)\n        return checkout_dir\n\n    @staticmethod\n    def _unimport_controller_module(module_name: str) -&gt; Set[str]:\n        \"\"\"Helper to un-import the given module and its parents/siblings/\n        children\n\n        Args:\n            module_name: str\n                The name of the module that holds the Controller\n\n        Returns:\n            reimport_modules: Set[str]\n                All modules that were unimported and will need to be reimported\n        \"\"\"\n        reimport_modules = set()\n        if module_name in sys.modules:\n            log.debug2(\"UnImporting controller module: %s\", module_name)\n            sys.modules.pop(module_name)\n            reimport_modules.add(module_name)\n\n        # UnImport the controller and any parent/sibling/child modules so\n        # controller can be reimported from the most recent sys path\n        module_parts = module_name.split(\".\")\n        for i in range(1, len(module_parts)):\n            parent_module = \".\".join(module_parts[:-i])\n            if parent_module in sys.modules:\n                log.debug3(\"UnImporting module: %s\", parent_module)\n                if sys.modules.pop(parent_module, None):\n                    reimport_modules.add(parent_module)\n        for child_module in [\n            mod_name\n            for mod_name in sys.modules\n            if mod_name.startswith(f\"{module_parts[0]}.\")\n        ]:\n            log.debug3(\"UnImporting child module: %s\", child_module)\n            if sys.modules.pop(child_module, None):\n                reimport_modules.add(child_module)\n        return reimport_modules\n\n    def _import_controller(\n        self, controller_info: CONTROLLER_INFO\n    ) -&gt; CONTROLLER_CLASS_TYPE:\n        \"\"\"Parse the controller info and reimport the controller\n\n        Args:\n            controller_info:CONTROLLER_INFO\n                The description of a controller. See CONTROLLER_INFO for\n                more information\n        Returns:\n            controller_class: Type[Controller]\n                The reimported Controller\n\n        \"\"\"\n        log.debug2(\"Parsing controller_info\")\n        if isinstance(controller_info, str):\n            class_module_parts = controller_info.rsplit(\".\", maxsplit=1)\n            assert_config(\n                len(class_module_parts) == 2,\n                f\"Invalid controller_class [{controller_info}]. Format is &lt;module&gt;.&lt;class&gt;\",\n            )\n            module_name, class_name = class_module_parts\n        else:\n            class_name = controller_info.__name__\n            module_name = controller_info.__module__\n\n        # Reimport module if reimporting is enabled and if it already exists\n        log.debug3(\n            \"Running controller %s from module %s [reimport? %s, in sys.modules? %s]\",\n            class_name,\n            module_name,\n            self.reimport_controller,\n            module_name in sys.modules,\n        )\n        reimport_modules = {module_name}\n        if self.reimport_controller:\n            reimport_modules = reimport_modules.union(\n                self._unimport_controller_module(module_name)\n            )\n\n        # Attempt to import the modules\n        log.debug2(\"Attempting to import [%s.%s]\", module_name, class_name)\n        for reimport_name in reimport_modules:\n            try:\n                app_module = importlib.import_module(reimport_name)\n                if reimport_name == module_name:\n                    if not hasattr(app_module, class_name):\n                        raise ConfigError(\n                            f\"Invalid controller_class [{class_name}].\"\n                            f\" Class not found in module [{reimport_name}]\"\n                        )\n                    controller_class = getattr(app_module, class_name)\n\n                    # Import controller in function to avoid circular imports\n                    # Local\n                    from .controller import (  # pylint: disable=import-outside-toplevel\n                        Controller,\n                    )\n\n                    if not issubclass(controller_class, Controller):\n                        raise ConfigError(\n                            f\"Invalid controller_class [{module_name}.{class_name}].\"\n                            f\" [{class_name}] is not a Controller\"\n                        )\n\n            except ImportError as exc:\n                # If this is the module that holds the controller, it _needs_ to\n                # be reimported\n                if reimport_name == module_name:\n                    log.error(\n                        \"Failed to import [%s.%s]. Failed to import [%s]\",\n                        reimport_name,\n                        class_name,\n                        reimport_name,\n                        exc_info=True,\n                    )\n                    raise ConfigError(\"Invalid Controller Class Specified\") from exc\n                # Otherwise, it's ok for import to fail\n                else:\n                    log.debug(\"Not able to reimport %s\", reimport_name)\n\n        log.debug(\n            \"Imported Controller %s from file %s\",\n            controller_class,\n            sys.modules[controller_class.__module__].__file__,\n        )\n\n        return controller_class\n\n    def _configure_controller(\n        self, controller_class: CONTROLLER_CLASS_TYPE\n    ) -&gt; CONTROLLER_TYPE:\n        \"\"\"Construct the Controller Class\n\n        Args:\n            controller_class: CONTROLLER_CLASS_TYPE\n                The Controller class to be reconciled\n\n        Returns:\n            controller: Controller\n                The constructed Controller\n\n        \"\"\"\n        log.debug3(\"Constructing controller\")\n        controller = controller_class()\n        return controller\n\n    def _get_reconcile_config(\n        self,\n        cr_manifest: aconfig.Config,\n        deploy_manager: DeployManagerBase,\n        controller_defaults: aconfig.Config,\n    ) -&gt; aconfig.Config:\n        \"\"\"Construct the flattened backend config for this reconciliation\n        starting with a deepcopy of the base and merge in overrides from the CR\n\n        Args:\n            cr_manifest: aconfig.Config:\n                The manifest to get overrides from\n            deploy_manager: DeployManagerBase:\n                The deploy manager to get the default configmap config\n            controller_defaults: aconfig.Config:\n                The config defaults provided by the controller class\n\n        Returns:\n            reconcile_config: aconfig.Config\n                The reconciliation config\n        \"\"\"\n        metadata = cr_manifest.get(\"metadata\", {})\n        annotations = metadata.get(\"annotations\", {})\n        namespace = metadata.get(\"namespace\")\n        cr_config_defaults = cr_manifest.get(constants.CONFIG_OVERRIDES, {})\n        annotation_config_defaults = {}\n        if constants.CONFIG_DEFAULTS_ANNOTATION_NAME in annotations:\n            log.debug(\"Pulling config_defaults based on annotation\")\n            config_defaults_name = annotations[\n                constants.CONFIG_DEFAULTS_ANNOTATION_NAME\n            ]\n\n            # Allow sub-keys to be delineated by \"/\"\n            parts = config_defaults_name.split(\"/\")\n            config_defaults_cm_name = parts[0]\n\n            log.debug2(\n                \"Reading config_defaults from ConfigMap [%s]\", config_defaults_cm_name\n            )\n            success, content = deploy_manager.get_object_current_state(\n                kind=\"ConfigMap\",\n                name=config_defaults_cm_name,\n                namespace=namespace,\n                api_version=\"v1\",\n            )\n            assert_cluster(success, \"Failed to look up config defaults form ConfigMap\")\n            assert_config(\n                content is not None,\n                f\"Did not find configured config defaults ConfigMap: {config_defaults_cm_name}\",\n            )\n            assert_config(\"data\" in content, \"Got ConfigMap content with out 'data'\")\n            config_defaults_content = content[\"data\"]\n            assert_config(\n                isinstance(config_defaults_content, dict),\n                f\"Incorrectly formatted config_defaults ConfigMap: {config_defaults_cm_name}\",\n            )\n\n            # Parse as a Config\n            log.debug2(\"Parsing app config dict\")\n            annotation_config_defaults = aconfig.Config(\n                config_defaults_content, override_env_vars=False\n            )\n\n        return merge_configs(\n            copy.deepcopy(controller_defaults),\n            merge_configs(annotation_config_defaults, cr_config_defaults),\n        )\n\n    def _get_temp_patches(  # pylint: disable=too-many-locals\n        self, deploy_manager: DeployManagerBase, cr_manifest: aconfig.Config\n    ) -&gt; List[aconfig.Config]:\n        \"\"\"Fetch the ordered list of temporary patches that should apply to this\n        rollout.\n\n        Args:\n            deploy_manager: DeployManagerBase\n                The DeployManager used to get the current temporary patches\n            cr_manifest: aconfig.Config\n                The manifest of this reconciliation\n        \"\"\"\n\n        # Look for patch annotations on the CR\n        patch_annotation = (\n            cr_manifest.get(\"metadata\", {})\n            .get(\"annotations\", {})\n            .get(constants.TEMPORARY_PATCHES_ANNOTATION_NAME, \"{}\")\n        )\n        log.debug3(\"Raw patch annotation: %s\", patch_annotation)\n        try:\n            raw_patches = json.loads(patch_annotation)\n            if not isinstance(raw_patches, dict):\n                msg = f\"Patches annotation not a dict: {raw_patches}\"\n                log.error(msg)\n                raise RolloutError(msg)\n            patches = {}\n            for patch_name, patch_meta in raw_patches.items():\n                patch_meta[\"timestamp\"] = dateutil.parser.parse(patch_meta[\"timestamp\"])\n                patches[patch_name] = patch_meta\n                if \"api_version\" not in patch_meta:\n                    raise KeyError(\"api_version\")\n        except json.decoder.JSONDecodeError as err:\n            msg = f\"Could not parse patches from annotation [{patch_annotation}]\"\n            log.error(msg)\n            raise RolloutError(msg) from err\n        except dateutil.parser.ParserError as err:\n            msg = f\"Failed to parse patch timestamp [{patch_annotation}]\"\n            log.error(msg)\n            raise RolloutError(msg) from err\n        except KeyError as err:\n            msg = f\"Patch meta incorrectly formatted [{patch_annotation}]\"\n            log.error(msg)\n            raise RolloutError(msg) from err\n\n        # Fetch the state of each patch and add it to the output, sorted by\n        # timestamp with the earliest first\n        temporary_patches = []\n        for patch_name, patch_meta in sorted(\n            list(patches.items()), key=lambda x: x[1][\"timestamp\"]\n        ):\n            # Do the fetch\n            log.debug2(\"Fetching patch [%s/%s]\", patch_name, patch_meta[\"timestamp\"])\n            namespace = cr_manifest.get(\"metadata\", {}).get(\"namespace\")\n            patch_api_version = patch_meta[\"api_version\"]\n            patch_kind = patch_meta.get(\"kind\", \"TemporaryPatch\")\n            success, content = deploy_manager.get_object_current_state(\n                kind=patch_kind,\n                name=patch_name,\n                api_version=patch_api_version,\n                namespace=namespace,\n            )\n            assert_cluster(success, f\"Failed to fetch patch content for [{patch_name}]\")\n            assert_config(content is not None, f\"Patch not found [{patch_name}]\")\n\n            # Pull the patch spec and add it to the list\n            assert_config(\n                content.get(\"spec\") is not None,\n                f\"No spec found in patch [{patch_name}]\",\n            )\n            temporary_patches.append(aconfig.Config(content, override_env_vars=False))\n\n        return temporary_patches\n\n    ## Status Details ############################################################\n\n    def _update_resource_status(\n        self, deploy_manager: DeployManagerBase, manifest: aconfig.Config, **kwargs\n    ) -&gt; dict:\n        \"\"\"Helper function to update the status of a resource given a deploy_manager, manifest\n        and status kwargs\n\n        Args:\n            deploy_manager: DeployManagerBase\n                The DeployManager used to update the resource\n            manifest: aconfig.Config\n                The manifest of the resource being updated\n            **kwargs:\n                The key word arguments passed to update_resource_status\n\n        Returns:\n            updated_status: dict\n                The updated status applied to the resource\n        \"\"\"\n        return status.update_resource_status(\n            deploy_manager,\n            manifest.kind,\n            manifest.apiVersion,\n            manifest.metadata.name,\n            manifest.metadata.namespace,\n            **kwargs,\n        )\n\n    def _update_reconcile_start_status(self, session: Session):\n        \"\"\"Update the status for a resource at the start of a reconciliation\n\n        Args:\n            session: Session\n                The session of the reconcile which includes the DeployManager and resource\n\n        \"\"\"\n        ready_condition = status.get_condition(status.READY_CONDITION, session.status)\n        ready_reason = ready_condition.get(\"reason\")\n        if ready_reason is None or session.current_version is None:\n            ready_reason = status.ReadyReason.INITIALIZING\n\n        optional_kwargs = {}\n        if session.current_version and session.version != session.current_version:\n            log.debug(\n                \"Version change detected: %s -&gt; %s\",\n                session.current_version,\n                session.version,\n            )\n            optional_kwargs = {\n                \"updating_reason\": status.UpdatingReason.VERSION_CHANGE,\n                \"updating_message\": \"Version Change Started: \"\n                f\"[{session.current_version}] -&gt; [{session.version}]\",\n            }\n            ready_reason = status.ReadyReason.IN_PROGRESS\n\n        self._update_resource_status(\n            session.deploy_manager,\n            session.cr_manifest,\n            ready_reason=ready_reason,\n            ready_message=ready_condition.get(\"message\", \"Initial Rollout Started\"),\n            supported_versions=config.supported_versions,\n            **optional_kwargs,\n        )\n\n    def _update_reconcile_completion_status(\n        self, session: Session, completion_state: CompletionState\n    ):\n        \"\"\"Perform CR status updates based on the results of the rollout steps. The status logic is\n        as follows:\n          1. Initial Rollout: Ready-INITIALIZING, Updating-VERIFY_WAIT\n          2. Everything complete: Ready-STABLE, Updating-STABLE\n          3. Everything except after_verify: Ready-IN_PROGRESS, Updating-STABLE\n          4. other: Updating-VERIFY_WAIT\n\n          Args:\n            session: Session\n                The session of the reconcile which includes the DeployManager and resource\n            completion_state: CompletionState\n                The result of the rollout\n        \"\"\"\n        status_update = {\"component_state\": completion_state}\n\n        # If everything completed and verified, set ready and updating to STABLE\n        # and set the status's reconciled version to the desired version\n        if completion_state.verify_completed():\n            status_update[\"ready_reason\"] = status.ReadyReason.STABLE\n            status_update[\"ready_message\"] = \"Verify Complete\"\n            status_update[\"updating_reason\"] = status.UpdatingReason.STABLE\n            status_update[\"updating_message\"] = \"Rollout Complete\"\n            status_update[\"version\"] = session.version\n\n        # If the completion_state didn't fail then update the ready condition with\n        # in_progress and the updating condition with verification incomplete\n        else:\n            current_status = session.get_status()\n\n            # If not initializing then update the ready condition with in_progress\n            current_ready_cond = status.get_condition(\n                status.READY_CONDITION, current_status\n            )\n            if (\n                current_ready_cond.get(\"reason\")\n                != status.ReadyReason.INITIALIZING.value\n            ):\n                status_update[\"ready_reason\"] = status.ReadyReason.IN_PROGRESS\n                status_update[\"ready_message\"] = \"Verify InProgress\"\n\n            status_update[\"updating_reason\"] = status.UpdatingReason.VERIFY_WAIT\n            status_update[\"updating_message\"] = \"Component verification incomplete\"\n\n        log.debug3(\"Updating status after reconcile: %s\", status_update)\n        self._update_resource_status(\n            session.deploy_manager, session.cr_manifest, **status_update\n        )\n\n    def _update_error_status(\n        self, resource: Union[dict, aconfig.Config], error: Exception\n    ) -&gt; dict:\n        \"\"\"Update the status of a resource after an error occurred. This function\n        setups up it's own deploy manager and parses the resource. This way errors at any\n        phase of reconciliation can still get updated\n\n        Args:\n            resource: Union[dict, aconfig.Config]\n                The resource that's status is being updated\n            error: Exception\n                The exception that stopped the reconciliation\n\n        Returns:\n            status: dict\n                The updated status after the error message\n        \"\"\"\n        cr_manifest = self.parse_manifest(resource)\n        deploy_manager = self.setup_deploy_manager(resource)\n\n        # Get the completion state if possible\n        component_state = getattr(error, \"completion_state\", None)\n\n        # Expected Oper8 Errors\n        if isinstance(error, PreconditionError):\n            status_update = {\n                \"updating_reason\": status.UpdatingReason.PRECONDITION_WAIT,\n                \"updating_message\": str(error),\n                \"component_state\": component_state,\n            }\n        elif isinstance(error, (VerificationError, Oper8ExpectedError)):\n            status_update = {\n                \"updating_reason\": status.UpdatingReason.VERIFY_WAIT,\n                \"updating_message\": str(error),\n                \"component_state\": component_state,\n            }\n        elif isinstance(error, ConfigError):\n            status_update = {\n                \"ready_reason\": status.ReadyReason.CONFIG_ERROR,\n                \"ready_message\": str(error),\n                \"updating_reason\": status.UpdatingReason.ERRORED,\n                \"updating_message\": str(error),\n                \"component_state\": component_state,\n            }\n        elif isinstance(error, ClusterError):\n            status_update = {\n                \"updating_reason\": status.UpdatingReason.CLUSTER_ERROR,\n                \"updating_message\": str(error),\n                \"component_state\": component_state,\n            }\n\n        elif isinstance(error, (RolloutError, Oper8FatalError)):\n            status_update = {\n                \"ready_reason\": status.ReadyReason.ERRORED,\n                \"ready_message\": str(error),\n                \"updating_reason\": status.UpdatingReason.ERRORED,\n                \"updating_message\": str(error),\n                \"component_state\": component_state,\n            }\n\n        # Catchall for non oper8 errors\n        else:\n            status_update = {\n                \"ready_reason\": status.ReadyReason.ERRORED,\n                \"ready_message\": str(error),\n                \"updating_reason\": status.UpdatingReason.ERRORED,\n                \"updating_message\": str(error),\n            }\n\n        return self._update_resource_status(\n            deploy_manager, cr_manifest, **status_update\n        )\n</code></pre>"},{"location":"API%20References/#oper8.reconcile.ReconcileManager.__init__","title":"<code>__init__(home_dir=None, deploy_manager=None, enable_vcs=None, reimport_controller=True)</code>","text":"<p>The constructor sets up the properties used across every reconcile and checks that the current config is valid.</p> <p>Parameters:</p> Name Type Description Default <code>home_dir</code> <code>str</code> <p>Optional[str]=None The root directory for importing controllers or VCS checkout</p> <code>None</code> <code>deploy_manager</code> <code>Optional[DeployManagerBase]</code> <p>Optional[DeployManager]=None Deploy manager to use. If not given, a new DeployManager will be created for each reconcile.</p> <code>None</code> <code>enable_vcs</code> <code>Optional[bool]</code> <p>Optional[bool]=True Parameter to manually control the state of VCS on a per instance basis</p> <code>None</code> <code>reimport_controller</code> <code>Optional[bool]</code> <p>Optional[bool]=None Parameter to manually control if a controller needs to be reimported each reconcile.</p> <code>True</code> Source code in <code>oper8/reconcile.py</code> <pre><code>def __init__(\n    self,\n    home_dir: str = None,\n    deploy_manager: Optional[DeployManagerBase] = None,\n    enable_vcs: Optional[bool] = None,\n    reimport_controller: Optional[bool] = True,\n):\n    \"\"\"The constructor sets up the properties used across every\n    reconcile and checks that the current config is valid.\n\n    Args:\n        home_dir:  Optional[str]=None\n            The root directory for importing controllers or VCS checkout\n        deploy_manager:  Optional[DeployManager]=None\n            Deploy manager to use. If not given, a new DeployManager will\n            be created for each reconcile.\n        enable_vcs:  Optional[bool]=True\n            Parameter to manually control the state of VCS on a per instance\n            basis\n        reimport_controller:  Optional[bool]=None\n            Parameter to manually control if a controller needs to be reimported each\n            reconcile.\n    \"\"\"\n\n    if home_dir:\n        self.home_dir = home_dir\n    elif config.vcs.enabled:\n        self.home_dir = config.vcs.repo\n    else:\n        self.home_dir = os.getcwd()\n\n    self.vcs = None\n\n    # If enable_vcs is not provided than default to\n    # config\n    if enable_vcs is None:\n        enable_vcs = config.vcs.enabled\n\n    if enable_vcs:\n        assert_config(\n            config.vcs.repo,\n            \"Can not enable vcs without supply source repo at vcs.repo\",\n        )\n        assert_config(\n            config.vcs.dest,\n            \"Cannot require enable vcs without providing a destination\",\n        )\n        vcs_checkout_methods = [method.value for method in VCSCheckoutMethod]\n        assert_config(\n            config.vcs.checkout_method in vcs_checkout_methods,\n            f\"VCS checkout method must be one of the following {vcs_checkout_methods}\",\n        )\n\n        self.vcs = VCS(self.home_dir)\n\n    # Ensure config is setup correctly for strict_versioning\n    self.strict_version_regex = re.compile(config.strict_version_kind_regex or \".*\")\n    if config.strict_versioning:\n        assert_config(\n            config.supported_versions is not None,\n            \"Must provide supported_versions with strict_versioning=True\",\n        )\n        assert_config(\n            config.vcs.field is not None,\n            \"Must provide vcs.field with strict_versioning=True\",\n        )\n\n    self.vcs_kind_regex = re.compile(config.vcs.kind_regex or \".*\")\n\n    self.deploy_manager = deploy_manager\n    self.reimport_controller = reimport_controller\n</code></pre>"},{"location":"API%20References/#oper8.reconcile.ReconcileManager.configure_logging","title":"<code>configure_logging(cr_manifest, reconciliation_id)</code>  <code>classmethod</code>","text":"<p>Configure the logging for a given reconcile</p> <p>Parameters:</p> Name Type Description Default <code>cr_manifest</code> <code>Config</code> <p>aconfig.Config The resource to get annotation overrides from</p> required <code>reconciliation_id</code> <code>str</code> <p>str The unique id for the reconciliation</p> required Source code in <code>oper8/reconcile.py</code> <pre><code>@classmethod\ndef configure_logging(cls, cr_manifest: aconfig.Config, reconciliation_id: str):\n    \"\"\"Configure the logging for a given reconcile\n\n    Args:\n        cr_manifest: aconfig.Config\n            The resource to get annotation overrides from\n        reconciliation_id: str\n            The unique id for the reconciliation\n    \"\"\"\n\n    # Fetch the annotations for logging\n    # NOTE: We use safe fetching here because this happens before CR\n    #   verification in the Session constructor\n    annotations = cr_manifest.get(\"metadata\", {}).get(\"annotations\", {})\n    default_level = annotations.get(\n        constants.LOG_DEFAULT_LEVEL_NAME, config.log_level\n    )\n\n    filters = annotations.get(constants.LOG_FILTERS_NAME, config.log_filters)\n    log_json = annotations.get(constants.LOG_JSON_NAME, str(config.log_json))\n    log_thread_id = annotations.get(\n        constants.LOG_THREAD_ID_NAME, str(config.log_thread_id)\n    )\n\n    # Convert boolean args\n    log_json = (log_json or \"\").lower() == \"true\"\n    log_thread_id = (log_thread_id or \"\").lower() == \"true\"\n\n    # Keep the old handler. This is useful if running with ansible as\n    # it will preserve the handler generator set up to log to a file\n    # since ansible captures all logging output\n    handler_generator = None\n    if logging.root.handlers:\n        old_handler = logging.root.handlers[0]\n\n        def handler_generator():\n            return old_handler\n\n    alog.configure(\n        default_level=default_level,\n        filters=filters,\n        formatter=(\n            Oper8JsonFormatter(cr_manifest, reconciliation_id)\n            if log_json\n            else \"pretty\"\n        ),\n        thread_id=log_thread_id,\n        handler_generator=handler_generator,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.reconcile.ReconcileManager.generate_id","title":"<code>generate_id()</code>  <code>classmethod</code>","text":"<p>Generates a unique human readable id for this reconciliation</p> <p>Returns:</p> Name Type Description <code>id</code> <code>str</code> <p>str A unique base32 encoded id</p> Source code in <code>oper8/reconcile.py</code> <pre><code>@classmethod\ndef generate_id(cls) -&gt; str:\n    \"\"\"Generates a unique human readable id for this reconciliation\n\n    Returns:\n        id: str\n            A unique base32 encoded id\n    \"\"\"\n    uuid4 = uuid.uuid4()\n    base32_str = base64.b32encode(uuid4.bytes).decode(\"utf-8\")\n    reconcile_id = base32_str[:22]\n    log.debug(\"Generated reconcile id: %s\", reconcile_id)\n    return reconcile_id\n</code></pre>"},{"location":"API%20References/#oper8.reconcile.ReconcileManager.parse_manifest","title":"<code>parse_manifest(resource)</code>  <code>classmethod</code>","text":"<p>Parse a raw resource into an aconfig Config</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Union[dict, Config]</code> <p>Union[dict, aconfig.Config]) The resource to be parsed into a manifest</p> required <p>Returns     cr_manifest: aconfig.Config         The parsed and validated config</p> Source code in <code>oper8/reconcile.py</code> <pre><code>@classmethod\ndef parse_manifest(cls, resource: Union[dict, aconfig.Config]) -&gt; aconfig.Config:\n    \"\"\"Parse a raw resource into an aconfig Config\n\n    Args:\n        resource: Union[dict, aconfig.Config])\n            The resource to be parsed into a manifest\n\n    Returns\n        cr_manifest: aconfig.Config\n            The parsed and validated config\n    \"\"\"\n    try:\n        cr_manifest = aconfig.Config(resource, override_env_vars=False)\n    except (ValueError, SyntaxError, AttributeError) as exc:\n        raise ValueError(\"Failed to parse full_cr\") from exc\n\n    return cr_manifest\n</code></pre>"},{"location":"API%20References/#oper8.reconcile.ReconcileManager.reconcile","title":"<code>reconcile(controller_info, resource, is_finalizer=False)</code>","text":"<p>This is the main entrypoint for reconciliations and contains the core implementation. The general reconcile path is as follows:</p> <pre><code>1. Parse the raw CR manifest\n2. Setup logging based on config with overrides from CR\n3. Check if the CR is paused and for strict versioning\n4. Setup directory if VCS is enabled\n5. Import and construct the Controller\n6. Setup the DeployManager and Session objects\n7. Run the Controller reconcile\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>controller_info</code> <code>CONTROLLER_INFO</code> <p>CONTROLLER_INFO The description of a controller. See CONTROLLER_INFO for more information</p> required <code>resource</code> <code>Union[dict, Config]</code> <p>Union[dict, aconfig.Config] A raw representation of the resource to be reconciled</p> required <code>is_finalizer</code> <code>bool</code> <p>bool=False Whether the resource is being deleted</p> <code>False</code> <p>Returns:</p> Name Type Description <code>reconcile_result</code> <code>ReconciliationResult</code> <p>ReconciliationResult The result of the reconcile</p> Source code in <code>oper8/reconcile.py</code> <pre><code>@alog.logged_function(log.info)\n@alog.timed_function(log.info, \"Reconcile finished in: \")\ndef reconcile(\n    self,\n    controller_info: CONTROLLER_INFO,\n    resource: Union[dict, aconfig.Config],\n    is_finalizer: bool = False,\n) -&gt; ReconciliationResult:\n    \"\"\"This is the main entrypoint for reconciliations and contains the\n    core implementation. The general reconcile path is as follows:\n\n        1. Parse the raw CR manifest\n        2. Setup logging based on config with overrides from CR\n        3. Check if the CR is paused and for strict versioning\n        4. Setup directory if VCS is enabled\n        5. Import and construct the Controller\n        6. Setup the DeployManager and Session objects\n        7. Run the Controller reconcile\n\n    Args:\n        controller_info: CONTROLLER_INFO\n            The description of a controller. See CONTROLLER_INFO for\n            more information\n        resource: Union[dict, aconfig.Config]\n            A raw representation of the resource to be reconciled\n        is_finalizer: bool=False\n            Whether the resource is being deleted\n\n    Returns:\n        reconcile_result:  ReconciliationResult\n            The result of the reconcile\n    \"\"\"\n\n    # Parse the full CR content\n    cr_manifest = self.parse_manifest(resource)\n\n    # generate id unique to this session\n    reconcile_id = self.generate_id()\n\n    # Initialize logging prior to any other work\n    self.configure_logging(cr_manifest, reconcile_id)\n\n    # If paused, do nothing and don't requeue\n    if self._is_paused(cr_manifest):\n        log.info(\"CR is paused. Exiting reconciliation\")\n        result = ReconciliationResult(requeue=False, requeue_params=RequeueParams())\n        return result\n\n    # Check strict versioning before continuing\n    if self._should_check_strict_version(cr_manifest):\n        self._check_strict_versioning(cr_manifest)\n\n    # Check if VCS is enabled and then attempt to checkout\n    if config.vcs.enabled:\n        self.setup_vcs(cr_manifest)\n\n    # Import controller and setup the instance\n    controller = self.setup_controller(controller_info)\n\n    # Configure deploy manager on a per reconcile basis for\n    # owner references unless a manager is provided on initialization\n    deploy_manager = self.setup_deploy_manager(cr_manifest)\n\n    # Setup Session\n    session = self.setup_session(\n        controller, cr_manifest, deploy_manager, reconcile_id\n    )\n\n    # Run the controller reconcile\n    result = self.run_controller(controller, session, is_finalizer)\n\n    return result\n</code></pre>"},{"location":"API%20References/#oper8.reconcile.ReconcileManager.run_controller","title":"<code>run_controller(controller, session, is_finalizer)</code>","text":"<p>Run the Controller's reconciliation or finalizer with the constructed Session. This function also updates the CR status and handles requeue logic.</p> <p>Parameters:</p> Name Type Description Default <code>controller</code> <code>CONTROLLER_TYPE</code> <p>Controller The Controller being reconciled</p> required <code>session</code> <code>Session</code> <p>Session The current Session state</p> required <code>is_finalizer</code> <code>bool</code> <p>Whether the resource is being deleted</p> required <p>Returns:</p> Name Type Description <code>reconciliation_result</code> <code>ReconciliationResult</code> <p>ReconciliationResult The result of the reconcile</p> Source code in <code>oper8/reconcile.py</code> <pre><code>def run_controller(\n    self, controller: CONTROLLER_TYPE, session: Session, is_finalizer: bool\n) -&gt; ReconciliationResult:\n    \"\"\"Run the Controller's reconciliation or finalizer with the constructed Session.\n    This function also updates the CR status and handles requeue logic.\n\n    Args:\n        controller: Controller\n            The Controller being reconciled\n        session: Session\n            The current Session state\n        is_finalizer:\n            Whether the resource is being deleted\n\n    Returns:\n        reconciliation_result: ReconciliationResult\n            The result of the reconcile\n    \"\"\"\n    log.info(\n        \"%s resource %s/%s/%s\",\n        \"Finalizing\" if is_finalizer else \"Reconciling\",\n        session.kind,\n        session.namespace,\n        session.name,\n    )\n\n    # Ensure the resource has the proper finalizers\n    if controller.has_finalizer:\n        add_finalizer(session, controller.finalizer)\n\n    # Update the Resource status\n    if config.manage_status:\n        self._update_reconcile_start_status(session)\n\n    # Reconcile the controller\n    completion_state = controller.run_reconcile(\n        session,\n        is_finalizer=is_finalizer,\n    )\n\n    if config.manage_status:\n        self._update_reconcile_completion_status(session, completion_state)\n\n    # Check if the controller session should requeue\n    requeue, requeue_params = controller.should_requeue(session)\n    if not requeue_params:\n        requeue_params = RequeueParams()\n\n    # Remove managed finalizers if not requeuing\n    if not requeue and is_finalizer and controller.has_finalizer:\n        remove_finalizer(session, controller.finalizer)\n\n    return ReconciliationResult(requeue=requeue, requeue_params=requeue_params)\n</code></pre>"},{"location":"API%20References/#oper8.reconcile.ReconcileManager.safe_reconcile","title":"<code>safe_reconcile(controller_info, resource, is_finalizer=False)</code>","text":"<p>This function calls out to reconcile but catches any errors thrown. This function guarantees a safe result which is needed by some Watch Managers</p> <p>Parameters:</p> Name Type Description Default <code>controller_info</code> <code>CONTROLLER_INFO</code> <p>CONTROLLER_INFO The description of a controller. See CONTROLLER_INFO for more information</p> required <code>resource</code> <code>dict</code> <p>Union[dict, aconfig.Config] A raw representation of the reconcile</p> required <code>is_finalize</code> <p>bool=False Whether the resource is being deleted</p> required <p>Returns:</p> Name Type Description <code>reconcile_result</code> <code>ReconciliationResult</code> <p>ReconciliationResult The result of the reconcile</p> Source code in <code>oper8/reconcile.py</code> <pre><code>def safe_reconcile(\n    self,\n    controller_info: CONTROLLER_INFO,\n    resource: dict,\n    is_finalizer: bool = False,\n) -&gt; ReconciliationResult:\n    \"\"\"\n    This function calls out to reconcile but catches any errors thrown. This\n    function guarantees a safe result which is needed by some Watch Managers\n\n    Args:\n        controller_info: CONTROLLER_INFO\n            The description of a controller. See CONTROLLER_INFO for\n            more information\n        resource: Union[dict, aconfig.Config]\n            A raw representation of the reconcile\n        is_finalize: bool=False\n            Whether the resource is being deleted\n\n    Returns:\n        reconcile_result:  ReconciliationResult\n            The result of the reconcile\n\n    \"\"\"\n\n    try:\n        return self.reconcile(controller_info, resource, is_finalizer)\n\n    # VCSMultiProcessError is an expected error caused by oper8 which should\n    # not be handled by the exception handling code\n    except VCSMultiProcessError as exc:\n        # Requeue after ~7.5 seconds. Add randomness to avoid\n        # repeated conflicts\n        requeue_time = 5 + random.uniform(0, 5)\n        params = RequeueParams(\n            requeue_after=datetime.timedelta(seconds=requeue_time)\n        )\n        log.debug(\"VCS Multiprocessing Error Detected: {%s}\", exc, exc_info=True)\n        log.warning(\n            \"VCS Setup failed due to other process. Requeueing in %ss\",\n            requeue_time,\n        )\n        return ReconciliationResult(\n            requeue=True, requeue_params=params, exception=exc\n        )\n\n    # Capture all generic exceptions\n    except Exception as exc:  # pylint: disable=broad-except\n        log.warning(\"Handling caught error in reconcile: %s\", exc, exc_info=True)\n        error = exc\n\n    if config.manage_status:\n        try:\n            self._update_error_status(resource, error)\n            log.debug(\"Update CR status with error message\")\n        except Exception as exc:  # pylint: disable=broad-except\n            log.error(\"Failed to update status: %s\", exc, exc_info=True)\n\n    # If we got to this return it means there was an\n    # exception during reconcile and we should requeue\n    # with the default backoff period\n    log.info(\"Requeuing CR due to error during reconcile\")\n    return ReconciliationResult(\n        requeue=True, requeue_params=RequeueParams(), exception=error\n    )\n</code></pre>"},{"location":"API%20References/#oper8.reconcile.ReconcileManager.setup_controller","title":"<code>setup_controller(controller_info)</code>","text":"<p>Import the requested Controller class and enable any compatibility layers</p> <p>Parameters:</p> Name Type Description Default <code>controller_info</code> <code>CONTROLLER_INFO</code> <p>CONTROLLER_INFO The description of a controller. See CONTROLLER_INFO for more information</p> required <p>Returns:     controller:         The required Controller Class</p> Source code in <code>oper8/reconcile.py</code> <pre><code>def setup_controller(\n    self, controller_info: CONTROLLER_INFO\n) -&gt; CONTROLLER_CLASS_TYPE:\n    \"\"\"\n    Import the requested Controller class and enable any compatibility layers\n\n    Args:\n        controller_info:CONTROLLER_INFO\n            The description of a controller. See CONTROLLER_INFO for\n            more information\n    Returns:\n        controller:\n            The required Controller Class\n    \"\"\"\n\n    # Local\n    from .controller import (  # pylint: disable=import-outside-toplevel, cyclic-import\n        Controller,\n    )\n\n    # If controller info is already a constructed controller then\n    # skip importing\n    if isinstance(controller_info, Controller):\n        return controller_info\n\n    controller_class = self._import_controller(controller_info)\n    return self._configure_controller(controller_class)\n</code></pre>"},{"location":"API%20References/#oper8.reconcile.ReconcileManager.setup_deploy_manager","title":"<code>setup_deploy_manager(cr_manifest)</code>","text":"<p>Configure a deploy_manager for a reconcile given a manifest</p> <p>Parameters:</p> Name Type Description Default <code>cr_manifest</code> <code>Config</code> <p>aconfig.Config The resource to be used as an owner_ref</p> required <p>Returns:</p> Name Type Description <code>deploy_manager</code> <code>DeployManagerBase</code> <p>DeployManagerBase The deploy_manager to be used during reconcile</p> Source code in <code>oper8/reconcile.py</code> <pre><code>def setup_deploy_manager(self, cr_manifest: aconfig.Config) -&gt; DeployManagerBase:\n    \"\"\"\n    Configure a deploy_manager for a reconcile given a manifest\n\n    Args:\n        cr_manifest: aconfig.Config\n            The resource to be used as an owner_ref\n\n    Returns:\n        deploy_manager: DeployManagerBase\n            The deploy_manager to be used during reconcile\n    \"\"\"\n    if self.deploy_manager:\n        return self.deploy_manager\n\n    if config.dry_run:\n        log.debug(\"Using DryRunDeployManager\")\n        return DryRunDeployManager()\n\n    log.debug(\"Using OpenshiftDeployManager\")\n    return OpenshiftDeployManager(owner_cr=cr_manifest)\n</code></pre>"},{"location":"API%20References/#oper8.reconcile.ReconcileManager.setup_session","title":"<code>setup_session(controller, cr_manifest, deploy_manager, reconciliation_id)</code>","text":"<p>Construct the session, including gathering the backend config and any temp patches</p> <p>Parameters:</p> Name Type Description Default <code>controller</code> <code>CONTROLLER_TYPE</code> <p>Controller The controller class being reconciled</p> required <code>cr_manifest</code> <code>Config</code> <p>aconfig.Config The resource being reconciled</p> required <code>deploy_manager</code> <code>DeployManagerBase</code> <p>DeployManagerBase The deploy manager used in the cluster</p> required <code>reconciliation_id</code> <code>str</code> <p>str The id for the reconcile</p> required Return <p>session: Session     The session for reconcile</p> Source code in <code>oper8/reconcile.py</code> <pre><code>def setup_session(\n    self,\n    controller: CONTROLLER_TYPE,\n    cr_manifest: aconfig.Config,\n    deploy_manager: DeployManagerBase,\n    reconciliation_id: str,\n) -&gt; Session:\n    \"\"\"Construct the session, including gathering the backend config and any temp patches\n\n    Args:\n        controller: Controller\n            The controller class being reconciled\n        cr_manifest: aconfig.Config\n            The resource being reconciled\n        deploy_manager: DeployManagerBase\n            The deploy manager used in the cluster\n        reconciliation_id: str\n            The id for the reconcile\n\n    Return:\n        session: Session\n            The session for reconcile\n    \"\"\"\n    # Get backend config for reconciliation\n    controller_defaults = controller.get_config_defaults()\n    reconciliation_config = self._get_reconcile_config(\n        cr_manifest=cr_manifest,\n        deploy_manager=deploy_manager,\n        controller_defaults=controller_defaults,\n    )\n    log.debug4(\"Gathered Config: %s\", reconciliation_config)\n\n    # Get Temporary patches\n    patches = self._get_temp_patches(deploy_manager, cr_manifest)\n    log.debug3(\"Found %d patches\", len(patches))\n\n    # Get the complete CR Manifest including defaults\n    cr_manifest_defaults = controller.get_cr_manifest_defaults()\n    full_cr_manifest = merge_configs(\n        aconfig.Config(cr_manifest_defaults),\n        cr_manifest,\n    )\n\n    return Session(\n        reconciliation_id=reconciliation_id,\n        cr_manifest=full_cr_manifest,\n        config=reconciliation_config,\n        deploy_manager=deploy_manager,\n        temporary_patches=patches,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.reconcile.ReconcileManager.setup_vcs","title":"<code>setup_vcs(cr_manifest)</code>","text":"<p>Setups the VCS directory and sys.path for a reconcile. This function also ensures that the version is valid if config.strict_versioning is enabled.</p> <p>Parameters:</p> Name Type Description Default <code>cr_manifest</code> <code>Config</code> <p>aconfig.Config The cr manifest to pull the requested version from.</p> required Source code in <code>oper8/reconcile.py</code> <pre><code>def setup_vcs(self, cr_manifest: aconfig.Config):\n    \"\"\"Setups the VCS directory and sys.path for a reconcile.\n    This function also ensures that the version is valid if\n    config.strict_versioning is enabled.\n\n    Args:\n        cr_manifest: aconfig.Config\n            The cr manifest to pull the requested version from.\n    \"\"\"\n    if not self.vcs_kind_regex.match(cr_manifest.get(\"kind\", \"\")):\n        log.debug(\"Skipping vcs checkout due to kind regex\")\n        return\n\n    version = get_manifest_version(cr_manifest)\n    if not version:\n        raise ValueError(\"CR Manifest has no version\")\n\n    log.debug(\n        \"Setting up working directory with src: %s and version: %s\",\n        self.home_dir,\n        version,\n    )\n    working_dir = self._setup_directory(cr_manifest, version)\n\n    # Construct working dir path from vcs and git directory\n    if config.vcs.module_dir:\n        module_path = pathlib.Path(config.vcs.module_dir)\n        working_dir = working_dir / module_path\n\n    if not working_dir.is_dir():\n        log.error(\n            \"Working directory %s could not be found. Invalid module path\",\n            working_dir,\n        )\n        raise ConfigError(\n            f\"Module path: '{module_path}' could not be found in repository\"\n        )\n\n    log.debug4(\"Changing working directory to %s\", working_dir)\n    os.chdir(working_dir)\n    sys.path.insert(0, str(working_dir))\n</code></pre>"},{"location":"API%20References/#oper8.reconcile.ReconciliationResult","title":"<code>ReconciliationResult</code>  <code>dataclass</code>","text":"<p>ReconciliationResult is the result of a reconciliation session</p> Source code in <code>oper8/reconcile.py</code> <pre><code>@dataclass\nclass ReconciliationResult:\n    \"\"\"ReconciliationResult is the result of a reconciliation session\"\"\"\n\n    # Flag to control requeue of current reconcile request\n    requeue: bool\n    # Parameters for requeue request\n    requeue_params: RequeueParams = field(default_factory=RequeueParams)\n    # Flag to identify if the reconciliation raised an exception\n    exception: Exception = None\n</code></pre>"},{"location":"API%20References/#oper8.reconcile.RequeueParams","title":"<code>RequeueParams</code>  <code>dataclass</code>","text":"<p>RequeueParams holds parameters for requeue request</p> Source code in <code>oper8/reconcile.py</code> <pre><code>@dataclass\nclass RequeueParams:\n    \"\"\"RequeueParams holds parameters for requeue request\"\"\"\n\n    requeue_after: datetime.timedelta = field(\n        default_factory=lambda: datetime.timedelta(\n            seconds=float(config.requeue_after_seconds)\n        )\n    )\n</code></pre>"},{"location":"API%20References/#oper8.rollout_manager","title":"<code>rollout_manager</code>","text":"<p>This module holds the implementation of the DAG constructs used to perform the dependency management for rollout</p>"},{"location":"API%20References/#oper8.rollout_manager.RolloutManager","title":"<code>RolloutManager</code>","text":"<p>This class manages the dependencies needed to roll out a set of nodes</p> Source code in <code>oper8/rollout_manager.py</code> <pre><code>class RolloutManager:\n    \"\"\"This class manages the dependencies needed to roll out a set of nodes\"\"\"\n\n    @classmethod\n    def run_node(\n        cls,\n        func: Callable[[Component, Session], bool],\n        session: Session,\n        component: Component,\n        fail_halt_runner=True,\n    ):\n        \"\"\"\n        Generic function to execute a node during Rollout\n\n        Args:\n            func: Callable[[Component,Session], bool]\n                The function to be called\n            session: Session\n                The session that's currently being rolled out\n            component: Component\n                The component being rolled out\n        \"\"\"\n        success = False\n        exception = None\n        rollout_failed = False\n\n        try:\n            success = func(session, component)\n            if fail_halt_runner and not success:\n                rollout_failed = True\n\n        # If a failure occurred by throwing, treat that the same as an\n        # explicit failure.\n        except Oper8Error as err:\n            log.debug(\"Caught Oper8Error during rollout of [%s]\", component)\n            success = False\n            rollout_failed = err.is_fatal_error\n            exception = err\n        except Exception as err:  # pylint: disable=broad-except\n            log.warning(\n                \"Caught exception during rollout of [%s]\",\n                component,\n                exc_info=True,\n            )\n            success = False\n            rollout_failed = True\n            exception = err\n\n        # If the rollout failed for any reason, raise an exception. This\n        # will halt the graph execution.\n        if not success:\n            log.debug(\"[deploy] Halting rollout\")\n            raise DagHaltError(rollout_failed, exception=exception)\n\n        log.debug3(\"Done with executing node: %s\", component)\n\n    def __init__(\n        self,\n        session: Session,\n        after_deploy: Optional[Callable[[Session], bool]] = None,\n        after_deploy_unsuccessful: Optional[Callable[[Session], bool]] = None,\n        after_verify: Optional[Callable[[Session], bool]] = None,\n        after_verify_unsuccessful: Optional[Callable[[Session], bool]] = None,\n    ):\n        \"\"\"Construct with the fully-populated session for the rollout\n\n        Args:\n            session:  Session\n                The current session for the reconciliation\n            after_deploy:  Optional[Callable[[Session] bool]]\n                An optional callback hook that will be invoked after the deploy\n                phase completes. The return indicates whether the validation has\n                passed.\n            after_deploy_unsuccessful:  Optional[Callable[[Session] bool]]\n                An optional callback hook that will be invoked after the deploy\n                phase ends with incomplete or failed status. The return indicates\n                whether the validation has passed.\n            after_verify:  Optional[Callable[[Session] None]]\n                An optional callback hook that will be invoked after the verify\n                phase completes. The return indicates whether the validation has\n                passed.\n            after_verify_unsuccessful:  Optional[Callable[[Session] None]]\n                An optional callback hook that will be invoked after the verify\n                phase ends with failure. The return indicates whether the validation\n                has passed.\n        \"\"\"\n        self._session = session\n        self._after_deploy = after_deploy\n        self._after_deploy_unsuccessful = after_deploy_unsuccessful\n        self._after_verify = after_verify\n        self._after_verify_unsuccessful = after_verify_unsuccessful\n\n        # Read pool size from config\n        deploy_threads = config.rollout_manager.deploy_threads\n        verify_threads = config.rollout_manager.verify_threads\n\n        # If session is in standalone mode, the recursive deploy -&gt; render in\n        # subsystems can cause jsii to fail in some spectacular ways. As such,\n        # we force single-threaded execution in standalone mode.\n        if config.standalone:\n            log.info(\"Running rollout without threads in standalone mode\")\n            deploy_threads = 0\n            verify_threads = 0\n\n        deploy_node_fn = partial(\n            RolloutManager.run_node,\n            deploy_component,\n            self._session,\n        )\n\n        verify_node_fn = partial(\n            RolloutManager.run_node,\n            verify_component,\n            self._session,\n            fail_halt_runner=False,\n        )\n\n        # Override disabled components with the disable function\n        self.disabled_nodes = set()\n        for component in session.graph:\n            if component.disabled:\n                component.set_data(\n                    partial(\n                        RolloutManager.run_node,\n                        disable_component,\n                        self._session,\n                        component,\n                    )\n                )\n\n                self.disabled_nodes.add(component)\n\n        self._deploy_graph = Runner(\n            \"deploy\",\n            threads=deploy_threads,\n            graph=session.graph,\n            default_function=deploy_node_fn,\n            verify_upstream=not config.dry_run,\n        )\n        self._verify_graph = Runner(\n            \"verify\",\n            threads=verify_threads,\n            graph=session.graph,\n            default_function=verify_node_fn,\n            verify_upstream=not config.dry_run,\n        )\n\n    def rollout(  # pylint: disable=too-many-locals,too-many-statements\n        self,\n    ) -&gt; CompletionState:\n        \"\"\"Perform the rollout\n\n        The logic has four phases:\n            1. Deploy Graph: This phase executes the Runner which runs the deploy()\n                function for each Component in dependency order. For graph edges\n                with configured verification functions, they are also executed\n                during this phase.\n            2. After Deploy: If configured with an after_deploy hook and (1)\n                completed all nodes successfully, this function is called. Any\n                raised exceptions indicate that the rollout should not proceed.\n            3. Verify Graph: This phase executes the Runner which runs the verify()\n                function for each Component in dependency order.\n            4. After Verify: If configured with an after_verify hook and (3)\n                completed all nodes successfully, this function is called. Any\n                raised exceptions indicate that the rollout is not fully\n                verified.\n\n        The rollout can terminate in one of three states:\n            1. incomplete AND failed: Something unexpected happened and the\n                rollout terminated in a fatal state.\n            2. incomplete AND NOT failed: One or more nodes did not pass their\n                verify steps, but all deploy steps that were attempted\n                succeeded.\n            3. complete AND NOT failed: The rollout completed all nodes,\n                including all verify steps. The managed components are ready to\n                take traffic.\n\n        Returns:\n            completion_state:  CompletionState\n                The final completion state of all nodes in the rollout Runner. This\n                is a logical composition of the outputs of the above phases\n                based on the termination logic above.\n        \"\"\"\n\n        # NOTE: The Runner is guaranteed to not throw (unless there's a bug!)\n        #   so we don't need to wrap run() in a try/except since the except\n        #   clause will never catch \"expected\" errors\n\n        ###########################\n        ## Phase 1: Deploy Graph ##\n        ###########################\n        with alog.ContextTimer(log.trace, \"Deploy Graph [%s]: \", self._session.id):\n            self._deploy_graph.run()\n        deploy_completion_state = self._deploy_graph.completion_state()\n\n        # Log phase 1 results:\n        #   * SUCCESS =&gt; All Components ran `render()` and `deploy()` without\n        #       raising. This is fetched from the `verify_completed()` state\n        #       since Components may raise precondition errors resulting in\n        #       `deploy_completed()` returning True, indicating that all nodes\n        #       ran and none reached a failed state, but not all nodes running\n        #       to final completion\n        #   * FAILED =&gt; Some nodes raised fatal errors\n        #   * INCOMPLETE =&gt; No errors were raised, but some nodes did not fully\n        #       complete without raising\n        log.debug3(\"Deploy completion: %s\", deploy_completion_state)\n        phase1_complete = deploy_completion_state.verify_completed()\n        phase1_failed = deploy_completion_state.failed()\n        log.debug(\n            \"[Phase 1] Deploy result: %s\",\n            (\n                \"SUCCESS\"\n                if phase1_complete\n                else (\"FAILED\" if phase1_failed else \"INCOMPLETE\")\n            ),\n        )\n\n        ###########################\n        ## Phase 2: After Deploy ##\n        ###########################\n\n        phase2_complete = phase1_complete\n        phase2_exception = None\n\n        # After unsuccessful deploy.\n        if (not phase1_complete or phase1_failed) and self._after_deploy_unsuccessful:\n            log.debug2(\"Running after-deploy-unsuccessful\")\n            try:\n                # Check deploy_completion_state argument for backward compatibility.\n                try:\n                    is_after_deploy_unsuccessful_completed = (\n                        self._after_deploy_unsuccessful(\n                            self._session, phase1_failed, deploy_completion_state\n                        )\n                    )\n                except TypeError:\n                    log.warning(\n                        \"Detected old after_deploy function. Please migrate oper8 to ^0.1.33.\"\n                    )\n                    is_after_deploy_unsuccessful_completed = (\n                        self._after_deploy_unsuccessful(self._session, phase1_failed)\n                    )\n\n                if not is_after_deploy_unsuccessful_completed:\n                    phase2_exception = VerificationError(\n                        \"After-deploy-unsuccessful verification failed\"\n                    )\n            except Exception as err:  # pylint: disable=broad-except\n                log.debug2(\n                    \"Error caught during after-deploy-unsuccessful: %s\",\n                    err,\n                    exc_info=True,\n                )\n                phase2_exception = err\n\n        # After successful deploy.\n        if phase1_complete and self._after_deploy:\n            log.debug2(\"Running after-deploy\")\n            try:\n                try:\n                    phase2_complete = self._after_deploy(\n                        self._session, deploy_completion_state\n                    )\n                except TypeError:\n                    log.warning(\n                        \"Detected old after_deploy function. Please migrate oper8 to ^0.1.33.\"\n                    )\n                    phase2_complete = self._after_deploy(self._session)\n\n                if not phase2_complete:\n                    phase2_exception = VerificationError(\n                        \"After-deploy verification failed\"\n                    )\n            except Exception as err:  # pylint: disable=broad-except\n                log.debug2(\"Error caught during after-deploy: %s\", err, exc_info=True)\n                phase2_complete = False\n                phase2_exception = err\n\n        # Log phase 2 results\n        log.debug(\n            \"[Phase 2] After deploy result: %s\",\n            (\n                \"SUCCESS\"\n                if phase2_complete\n                else (\"FAILED\" if phase2_exception else \"NOT RUN\")\n            ),\n        )\n\n        ###########################\n        ## Phase 3: Verify Graph ##\n        ###########################\n\n        # If phase 1 ran without erroring, then run the verify Runner\n        phase3_complete = False\n        phase3_failed = False\n        if not phase1_failed:\n            # Configured the verify Runner based off of deployed nodes\n            # This way only components that have started will be verified\n            deployed_nodes = (\n                deploy_completion_state.verified_nodes.union(\n                    deploy_completion_state.unverified_nodes\n                )\n            ) - deploy_completion_state.failed_nodes\n            log.debug3(\"Verify phase running with Nodes: %s\", deployed_nodes)\n\n            # Enable/Disable all nodes in verify_graph based on whether they\n            # were deployed or not\n            for comp in set(self._session.get_components()):\n                if comp in deployed_nodes:\n                    self._verify_graph.enable_node(comp)\n                else:\n                    self._verify_graph.disable_node(comp)\n\n                # Disabled components should immediately verify\n                if comp in self.disabled_nodes:\n                    comp.set_data(lambda *_: True)\n\n            # Run the verify Runner\n            with alog.ContextTimer(log.trace, \"Verify Graph [%s]: \", self._session.id):\n                self._verify_graph.run()\n            verify_completion_state = self._verify_graph.completion_state()\n            log.debug3(\"Verify completion: %s\", verify_completion_state)\n            # Only consider phase3 completed if phase1 and phase2 fully completed\n            phase3_complete = (\n                verify_completion_state.verify_completed()\n                and phase1_complete\n                and phase2_complete\n            )\n            phase3_failed = verify_completion_state.failed()\n        else:\n            verify_completion_state = CompletionState()\n\n        # Log phase 3 results\n        log.debug(\n            \"[Phase 3] Verify result: %s\",\n            (\n                \"SUCCESS\"\n                if phase3_complete\n                else (\n                    \"FAILED\"\n                    if phase3_failed\n                    else (\"INCOMPLETE\" if phase2_complete else \"NOT RUN\")\n                )\n            ),\n        )\n\n        ###########################\n        ## Phase 4: After Verify ##\n        ###########################\n\n        phase4_complete = phase3_complete\n        phase4_exception = None\n\n        # If deployment is completed, but verification is not, run _after_verify_unsuccessful.\n        if (\n            phase1_complete and phase2_complete and not phase3_complete\n        ) and self._after_verify_unsuccessful:\n            log.debug(\"Running after-verify-unsuccessful\")\n            try:\n                try:\n                    is_after_verify_unsuccessful_completed = (\n                        self._after_verify_unsuccessful(\n                            self._session,\n                            phase3_failed,\n                            verify_completion_state,\n                            deploy_completion_state,\n                        )\n                    )\n                except TypeError:\n                    log.warning(\n                        \"Detected old after_verify function. Please migrate oper8 to ^0.1.33.\"\n                    )\n                    is_after_verify_unsuccessful_completed = (\n                        self._after_verify_unsuccessful(self._session, phase3_failed)\n                    )\n\n                if not is_after_verify_unsuccessful_completed:\n                    phase4_exception = VerificationError(\n                        \"After-verify-unsuccessful failed\"\n                    )\n            except Exception as err:  # pylint: disable=broad-except\n                log.debug2(\"Error caught during after-verify: %s\", err, exc_info=True)\n                phase4_exception = err\n\n        # If both deployment and verification is completed, run _after_verify.\n        if phase3_complete and self._after_verify:\n            log.debug(\"Running after-verify\")\n            try:\n                try:\n                    phase4_complete = self._after_verify(\n                        self._session, verify_completion_state, deploy_completion_state\n                    )\n                except TypeError:\n                    log.warning(\n                        \"Detected old after_verify function. Please migrate oper8 to ^0.1.33.\"\n                    )\n                    phase4_complete = self._after_verify(self._session)\n\n                if not phase4_complete:\n                    phase4_exception = VerificationError(\"After-verify failed\")\n            except Exception as err:  # pylint: disable=broad-except\n                log.debug2(\"Error caught during after-verify: %s\", err, exc_info=True)\n                phase4_complete = False\n                phase4_exception = err\n\n        # Log phase 4 results\n        log.debug(\n            \"[Phase 4] After deploy result: %s\",\n            (\n                \"SUCCESS\"\n                if phase4_complete\n                else (\"FAILED\" if phase4_exception else \"NOT RUN\")\n            ),\n        )\n\n        # Create a final completion state with the \"deployed nodes\" pulled\n        # from the deploy results and the \"verified nodes\" pulled from the\n        # verify results.\n        #\n        # Verified Nodes: Nodes that made it all the way through the verify\n        #   graph\n        # Unverified Nodes: Nodes that are \"verified\" in the deploy graph, but\n        #   are unverified in the verify graph or were not run in the verify\n        #   graph and did not fail in the verify graph\n        # Failed Nodes: Nodes that failed in either graph\n        # Unstarted Nodes: Nodes that were unstarted in the deploy graph\n        # Exception: Any exception from any of the phases above\n        verified_nodes = verify_completion_state.verified_nodes\n        failed_nodes = verify_completion_state.failed_nodes.union(\n            deploy_completion_state.failed_nodes\n        )\n        unverified_nodes = (\n            (\n                deploy_completion_state.verified_nodes.union(\n                    deploy_completion_state.unverified_nodes\n                ).union(verify_completion_state.unverified_nodes)\n            )\n            - verified_nodes\n            - failed_nodes\n        )\n        unstarted_nodes = deploy_completion_state.unstarted_nodes\n        exception = (\n            deploy_completion_state.exception\n            or phase2_exception\n            or verify_completion_state.exception\n            or phase4_exception\n        )\n        completion_state = CompletionState(\n            verified_nodes=verified_nodes,\n            unverified_nodes=unverified_nodes,\n            failed_nodes=failed_nodes,\n            unstarted_nodes=unstarted_nodes,\n            exception=exception,\n        )\n        log.debug2(\"Final rollout state: %s\", completion_state)\n        return completion_state\n</code></pre>"},{"location":"API%20References/#oper8.rollout_manager.RolloutManager.__init__","title":"<code>__init__(session, after_deploy=None, after_deploy_unsuccessful=None, after_verify=None, after_verify_unsuccessful=None)</code>","text":"<p>Construct with the fully-populated session for the rollout</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current session for the reconciliation</p> required <code>after_deploy</code> <code>Optional[Callable[[Session], bool]]</code> <p>Optional[Callable[[Session] bool]] An optional callback hook that will be invoked after the deploy phase completes. The return indicates whether the validation has passed.</p> <code>None</code> <code>after_deploy_unsuccessful</code> <code>Optional[Callable[[Session], bool]]</code> <p>Optional[Callable[[Session] bool]] An optional callback hook that will be invoked after the deploy phase ends with incomplete or failed status. The return indicates whether the validation has passed.</p> <code>None</code> <code>after_verify</code> <code>Optional[Callable[[Session], bool]]</code> <p>Optional[Callable[[Session] None]] An optional callback hook that will be invoked after the verify phase completes. The return indicates whether the validation has passed.</p> <code>None</code> <code>after_verify_unsuccessful</code> <code>Optional[Callable[[Session], bool]]</code> <p>Optional[Callable[[Session] None]] An optional callback hook that will be invoked after the verify phase ends with failure. The return indicates whether the validation has passed.</p> <code>None</code> Source code in <code>oper8/rollout_manager.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    after_deploy: Optional[Callable[[Session], bool]] = None,\n    after_deploy_unsuccessful: Optional[Callable[[Session], bool]] = None,\n    after_verify: Optional[Callable[[Session], bool]] = None,\n    after_verify_unsuccessful: Optional[Callable[[Session], bool]] = None,\n):\n    \"\"\"Construct with the fully-populated session for the rollout\n\n    Args:\n        session:  Session\n            The current session for the reconciliation\n        after_deploy:  Optional[Callable[[Session] bool]]\n            An optional callback hook that will be invoked after the deploy\n            phase completes. The return indicates whether the validation has\n            passed.\n        after_deploy_unsuccessful:  Optional[Callable[[Session] bool]]\n            An optional callback hook that will be invoked after the deploy\n            phase ends with incomplete or failed status. The return indicates\n            whether the validation has passed.\n        after_verify:  Optional[Callable[[Session] None]]\n            An optional callback hook that will be invoked after the verify\n            phase completes. The return indicates whether the validation has\n            passed.\n        after_verify_unsuccessful:  Optional[Callable[[Session] None]]\n            An optional callback hook that will be invoked after the verify\n            phase ends with failure. The return indicates whether the validation\n            has passed.\n    \"\"\"\n    self._session = session\n    self._after_deploy = after_deploy\n    self._after_deploy_unsuccessful = after_deploy_unsuccessful\n    self._after_verify = after_verify\n    self._after_verify_unsuccessful = after_verify_unsuccessful\n\n    # Read pool size from config\n    deploy_threads = config.rollout_manager.deploy_threads\n    verify_threads = config.rollout_manager.verify_threads\n\n    # If session is in standalone mode, the recursive deploy -&gt; render in\n    # subsystems can cause jsii to fail in some spectacular ways. As such,\n    # we force single-threaded execution in standalone mode.\n    if config.standalone:\n        log.info(\"Running rollout without threads in standalone mode\")\n        deploy_threads = 0\n        verify_threads = 0\n\n    deploy_node_fn = partial(\n        RolloutManager.run_node,\n        deploy_component,\n        self._session,\n    )\n\n    verify_node_fn = partial(\n        RolloutManager.run_node,\n        verify_component,\n        self._session,\n        fail_halt_runner=False,\n    )\n\n    # Override disabled components with the disable function\n    self.disabled_nodes = set()\n    for component in session.graph:\n        if component.disabled:\n            component.set_data(\n                partial(\n                    RolloutManager.run_node,\n                    disable_component,\n                    self._session,\n                    component,\n                )\n            )\n\n            self.disabled_nodes.add(component)\n\n    self._deploy_graph = Runner(\n        \"deploy\",\n        threads=deploy_threads,\n        graph=session.graph,\n        default_function=deploy_node_fn,\n        verify_upstream=not config.dry_run,\n    )\n    self._verify_graph = Runner(\n        \"verify\",\n        threads=verify_threads,\n        graph=session.graph,\n        default_function=verify_node_fn,\n        verify_upstream=not config.dry_run,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.rollout_manager.RolloutManager.rollout","title":"<code>rollout()</code>","text":"<p>Perform the rollout</p> The logic has four phases <ol> <li>Deploy Graph: This phase executes the Runner which runs the deploy()     function for each Component in dependency order. For graph edges     with configured verification functions, they are also executed     during this phase.</li> <li>After Deploy: If configured with an after_deploy hook and (1)     completed all nodes successfully, this function is called. Any     raised exceptions indicate that the rollout should not proceed.</li> <li>Verify Graph: This phase executes the Runner which runs the verify()     function for each Component in dependency order.</li> <li>After Verify: If configured with an after_verify hook and (3)     completed all nodes successfully, this function is called. Any     raised exceptions indicate that the rollout is not fully     verified.</li> </ol> The rollout can terminate in one of three states <ol> <li>incomplete AND failed: Something unexpected happened and the     rollout terminated in a fatal state.</li> <li>incomplete AND NOT failed: One or more nodes did not pass their     verify steps, but all deploy steps that were attempted     succeeded.</li> <li>complete AND NOT failed: The rollout completed all nodes,     including all verify steps. The managed components are ready to     take traffic.</li> </ol> <p>Returns:</p> Name Type Description <code>completion_state</code> <code>CompletionState</code> <p>CompletionState The final completion state of all nodes in the rollout Runner. This is a logical composition of the outputs of the above phases based on the termination logic above.</p> Source code in <code>oper8/rollout_manager.py</code> <pre><code>def rollout(  # pylint: disable=too-many-locals,too-many-statements\n    self,\n) -&gt; CompletionState:\n    \"\"\"Perform the rollout\n\n    The logic has four phases:\n        1. Deploy Graph: This phase executes the Runner which runs the deploy()\n            function for each Component in dependency order. For graph edges\n            with configured verification functions, they are also executed\n            during this phase.\n        2. After Deploy: If configured with an after_deploy hook and (1)\n            completed all nodes successfully, this function is called. Any\n            raised exceptions indicate that the rollout should not proceed.\n        3. Verify Graph: This phase executes the Runner which runs the verify()\n            function for each Component in dependency order.\n        4. After Verify: If configured with an after_verify hook and (3)\n            completed all nodes successfully, this function is called. Any\n            raised exceptions indicate that the rollout is not fully\n            verified.\n\n    The rollout can terminate in one of three states:\n        1. incomplete AND failed: Something unexpected happened and the\n            rollout terminated in a fatal state.\n        2. incomplete AND NOT failed: One or more nodes did not pass their\n            verify steps, but all deploy steps that were attempted\n            succeeded.\n        3. complete AND NOT failed: The rollout completed all nodes,\n            including all verify steps. The managed components are ready to\n            take traffic.\n\n    Returns:\n        completion_state:  CompletionState\n            The final completion state of all nodes in the rollout Runner. This\n            is a logical composition of the outputs of the above phases\n            based on the termination logic above.\n    \"\"\"\n\n    # NOTE: The Runner is guaranteed to not throw (unless there's a bug!)\n    #   so we don't need to wrap run() in a try/except since the except\n    #   clause will never catch \"expected\" errors\n\n    ###########################\n    ## Phase 1: Deploy Graph ##\n    ###########################\n    with alog.ContextTimer(log.trace, \"Deploy Graph [%s]: \", self._session.id):\n        self._deploy_graph.run()\n    deploy_completion_state = self._deploy_graph.completion_state()\n\n    # Log phase 1 results:\n    #   * SUCCESS =&gt; All Components ran `render()` and `deploy()` without\n    #       raising. This is fetched from the `verify_completed()` state\n    #       since Components may raise precondition errors resulting in\n    #       `deploy_completed()` returning True, indicating that all nodes\n    #       ran and none reached a failed state, but not all nodes running\n    #       to final completion\n    #   * FAILED =&gt; Some nodes raised fatal errors\n    #   * INCOMPLETE =&gt; No errors were raised, but some nodes did not fully\n    #       complete without raising\n    log.debug3(\"Deploy completion: %s\", deploy_completion_state)\n    phase1_complete = deploy_completion_state.verify_completed()\n    phase1_failed = deploy_completion_state.failed()\n    log.debug(\n        \"[Phase 1] Deploy result: %s\",\n        (\n            \"SUCCESS\"\n            if phase1_complete\n            else (\"FAILED\" if phase1_failed else \"INCOMPLETE\")\n        ),\n    )\n\n    ###########################\n    ## Phase 2: After Deploy ##\n    ###########################\n\n    phase2_complete = phase1_complete\n    phase2_exception = None\n\n    # After unsuccessful deploy.\n    if (not phase1_complete or phase1_failed) and self._after_deploy_unsuccessful:\n        log.debug2(\"Running after-deploy-unsuccessful\")\n        try:\n            # Check deploy_completion_state argument for backward compatibility.\n            try:\n                is_after_deploy_unsuccessful_completed = (\n                    self._after_deploy_unsuccessful(\n                        self._session, phase1_failed, deploy_completion_state\n                    )\n                )\n            except TypeError:\n                log.warning(\n                    \"Detected old after_deploy function. Please migrate oper8 to ^0.1.33.\"\n                )\n                is_after_deploy_unsuccessful_completed = (\n                    self._after_deploy_unsuccessful(self._session, phase1_failed)\n                )\n\n            if not is_after_deploy_unsuccessful_completed:\n                phase2_exception = VerificationError(\n                    \"After-deploy-unsuccessful verification failed\"\n                )\n        except Exception as err:  # pylint: disable=broad-except\n            log.debug2(\n                \"Error caught during after-deploy-unsuccessful: %s\",\n                err,\n                exc_info=True,\n            )\n            phase2_exception = err\n\n    # After successful deploy.\n    if phase1_complete and self._after_deploy:\n        log.debug2(\"Running after-deploy\")\n        try:\n            try:\n                phase2_complete = self._after_deploy(\n                    self._session, deploy_completion_state\n                )\n            except TypeError:\n                log.warning(\n                    \"Detected old after_deploy function. Please migrate oper8 to ^0.1.33.\"\n                )\n                phase2_complete = self._after_deploy(self._session)\n\n            if not phase2_complete:\n                phase2_exception = VerificationError(\n                    \"After-deploy verification failed\"\n                )\n        except Exception as err:  # pylint: disable=broad-except\n            log.debug2(\"Error caught during after-deploy: %s\", err, exc_info=True)\n            phase2_complete = False\n            phase2_exception = err\n\n    # Log phase 2 results\n    log.debug(\n        \"[Phase 2] After deploy result: %s\",\n        (\n            \"SUCCESS\"\n            if phase2_complete\n            else (\"FAILED\" if phase2_exception else \"NOT RUN\")\n        ),\n    )\n\n    ###########################\n    ## Phase 3: Verify Graph ##\n    ###########################\n\n    # If phase 1 ran without erroring, then run the verify Runner\n    phase3_complete = False\n    phase3_failed = False\n    if not phase1_failed:\n        # Configured the verify Runner based off of deployed nodes\n        # This way only components that have started will be verified\n        deployed_nodes = (\n            deploy_completion_state.verified_nodes.union(\n                deploy_completion_state.unverified_nodes\n            )\n        ) - deploy_completion_state.failed_nodes\n        log.debug3(\"Verify phase running with Nodes: %s\", deployed_nodes)\n\n        # Enable/Disable all nodes in verify_graph based on whether they\n        # were deployed or not\n        for comp in set(self._session.get_components()):\n            if comp in deployed_nodes:\n                self._verify_graph.enable_node(comp)\n            else:\n                self._verify_graph.disable_node(comp)\n\n            # Disabled components should immediately verify\n            if comp in self.disabled_nodes:\n                comp.set_data(lambda *_: True)\n\n        # Run the verify Runner\n        with alog.ContextTimer(log.trace, \"Verify Graph [%s]: \", self._session.id):\n            self._verify_graph.run()\n        verify_completion_state = self._verify_graph.completion_state()\n        log.debug3(\"Verify completion: %s\", verify_completion_state)\n        # Only consider phase3 completed if phase1 and phase2 fully completed\n        phase3_complete = (\n            verify_completion_state.verify_completed()\n            and phase1_complete\n            and phase2_complete\n        )\n        phase3_failed = verify_completion_state.failed()\n    else:\n        verify_completion_state = CompletionState()\n\n    # Log phase 3 results\n    log.debug(\n        \"[Phase 3] Verify result: %s\",\n        (\n            \"SUCCESS\"\n            if phase3_complete\n            else (\n                \"FAILED\"\n                if phase3_failed\n                else (\"INCOMPLETE\" if phase2_complete else \"NOT RUN\")\n            )\n        ),\n    )\n\n    ###########################\n    ## Phase 4: After Verify ##\n    ###########################\n\n    phase4_complete = phase3_complete\n    phase4_exception = None\n\n    # If deployment is completed, but verification is not, run _after_verify_unsuccessful.\n    if (\n        phase1_complete and phase2_complete and not phase3_complete\n    ) and self._after_verify_unsuccessful:\n        log.debug(\"Running after-verify-unsuccessful\")\n        try:\n            try:\n                is_after_verify_unsuccessful_completed = (\n                    self._after_verify_unsuccessful(\n                        self._session,\n                        phase3_failed,\n                        verify_completion_state,\n                        deploy_completion_state,\n                    )\n                )\n            except TypeError:\n                log.warning(\n                    \"Detected old after_verify function. Please migrate oper8 to ^0.1.33.\"\n                )\n                is_after_verify_unsuccessful_completed = (\n                    self._after_verify_unsuccessful(self._session, phase3_failed)\n                )\n\n            if not is_after_verify_unsuccessful_completed:\n                phase4_exception = VerificationError(\n                    \"After-verify-unsuccessful failed\"\n                )\n        except Exception as err:  # pylint: disable=broad-except\n            log.debug2(\"Error caught during after-verify: %s\", err, exc_info=True)\n            phase4_exception = err\n\n    # If both deployment and verification is completed, run _after_verify.\n    if phase3_complete and self._after_verify:\n        log.debug(\"Running after-verify\")\n        try:\n            try:\n                phase4_complete = self._after_verify(\n                    self._session, verify_completion_state, deploy_completion_state\n                )\n            except TypeError:\n                log.warning(\n                    \"Detected old after_verify function. Please migrate oper8 to ^0.1.33.\"\n                )\n                phase4_complete = self._after_verify(self._session)\n\n            if not phase4_complete:\n                phase4_exception = VerificationError(\"After-verify failed\")\n        except Exception as err:  # pylint: disable=broad-except\n            log.debug2(\"Error caught during after-verify: %s\", err, exc_info=True)\n            phase4_complete = False\n            phase4_exception = err\n\n    # Log phase 4 results\n    log.debug(\n        \"[Phase 4] After deploy result: %s\",\n        (\n            \"SUCCESS\"\n            if phase4_complete\n            else (\"FAILED\" if phase4_exception else \"NOT RUN\")\n        ),\n    )\n\n    # Create a final completion state with the \"deployed nodes\" pulled\n    # from the deploy results and the \"verified nodes\" pulled from the\n    # verify results.\n    #\n    # Verified Nodes: Nodes that made it all the way through the verify\n    #   graph\n    # Unverified Nodes: Nodes that are \"verified\" in the deploy graph, but\n    #   are unverified in the verify graph or were not run in the verify\n    #   graph and did not fail in the verify graph\n    # Failed Nodes: Nodes that failed in either graph\n    # Unstarted Nodes: Nodes that were unstarted in the deploy graph\n    # Exception: Any exception from any of the phases above\n    verified_nodes = verify_completion_state.verified_nodes\n    failed_nodes = verify_completion_state.failed_nodes.union(\n        deploy_completion_state.failed_nodes\n    )\n    unverified_nodes = (\n        (\n            deploy_completion_state.verified_nodes.union(\n                deploy_completion_state.unverified_nodes\n            ).union(verify_completion_state.unverified_nodes)\n        )\n        - verified_nodes\n        - failed_nodes\n    )\n    unstarted_nodes = deploy_completion_state.unstarted_nodes\n    exception = (\n        deploy_completion_state.exception\n        or phase2_exception\n        or verify_completion_state.exception\n        or phase4_exception\n    )\n    completion_state = CompletionState(\n        verified_nodes=verified_nodes,\n        unverified_nodes=unverified_nodes,\n        failed_nodes=failed_nodes,\n        unstarted_nodes=unstarted_nodes,\n        exception=exception,\n    )\n    log.debug2(\"Final rollout state: %s\", completion_state)\n    return completion_state\n</code></pre>"},{"location":"API%20References/#oper8.rollout_manager.RolloutManager.run_node","title":"<code>run_node(func, session, component, fail_halt_runner=True)</code>  <code>classmethod</code>","text":"<p>Generic function to execute a node during Rollout</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[[Component, Session], bool]</code> <p>Callable[[Component,Session], bool] The function to be called</p> required <code>session</code> <code>Session</code> <p>Session The session that's currently being rolled out</p> required <code>component</code> <code>Component</code> <p>Component The component being rolled out</p> required Source code in <code>oper8/rollout_manager.py</code> <pre><code>@classmethod\ndef run_node(\n    cls,\n    func: Callable[[Component, Session], bool],\n    session: Session,\n    component: Component,\n    fail_halt_runner=True,\n):\n    \"\"\"\n    Generic function to execute a node during Rollout\n\n    Args:\n        func: Callable[[Component,Session], bool]\n            The function to be called\n        session: Session\n            The session that's currently being rolled out\n        component: Component\n            The component being rolled out\n    \"\"\"\n    success = False\n    exception = None\n    rollout_failed = False\n\n    try:\n        success = func(session, component)\n        if fail_halt_runner and not success:\n            rollout_failed = True\n\n    # If a failure occurred by throwing, treat that the same as an\n    # explicit failure.\n    except Oper8Error as err:\n        log.debug(\"Caught Oper8Error during rollout of [%s]\", component)\n        success = False\n        rollout_failed = err.is_fatal_error\n        exception = err\n    except Exception as err:  # pylint: disable=broad-except\n        log.warning(\n            \"Caught exception during rollout of [%s]\",\n            component,\n            exc_info=True,\n        )\n        success = False\n        rollout_failed = True\n        exception = err\n\n    # If the rollout failed for any reason, raise an exception. This\n    # will halt the graph execution.\n    if not success:\n        log.debug(\"[deploy] Halting rollout\")\n        raise DagHaltError(rollout_failed, exception=exception)\n\n    log.debug3(\"Done with executing node: %s\", component)\n</code></pre>"},{"location":"API%20References/#oper8.rollout_manager.deploy_component","title":"<code>deploy_component(session, component)</code>","text":"<p>Deploy a component given a particular session</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current rollout session</p> required <code>component</code> <code>Component</code> <p>Component The component to deploy</p> required <p>Returns:</p> Name Type Description <code>result</code> <code>bool</code> <p>bool The result of the deploy</p> Source code in <code>oper8/rollout_manager.py</code> <pre><code>def deploy_component(session: Session, component: Component) -&gt; bool:\n    \"\"\"Deploy a component given a particular session\n\n    Args:\n        session: Session\n            The current rollout session\n        component: Component\n            The component to deploy\n\n    Returns:\n        result: bool\n            The result of the deploy\n    \"\"\"\n    # Do the render\n    with alog.ContextTimer(log.debug2, \"Render duration for %s\", component):\n        component.render_chart(session)\n        log.debug3(\n            \"Rendered objects for [%s]: %s\",\n            component,\n            [str(obj) for obj in component.managed_objects],\n        )\n\n    # Do the deploy\n    with alog.ContextTimer(log.debug2, \"Deploy duration for %s: \", component):\n        return component.deploy(session)\n</code></pre>"},{"location":"API%20References/#oper8.rollout_manager.disable_component","title":"<code>disable_component(session, component)</code>","text":"<p>Disable a component given a particular session</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current rollout session</p> required <code>component</code> <code>Component</code> <p>Component The component to disable</p> required <p>Returns:</p> Name Type Description <code>result</code> <code>bool</code> <p>bool The result of the disable</p> Source code in <code>oper8/rollout_manager.py</code> <pre><code>def disable_component(session: Session, component: Component) -&gt; bool:\n    \"\"\"Disable a component given a particular session\n\n    Args:\n        session: Session\n            The current rollout session\n        component: Component\n            The component to disable\n\n    Returns:\n        result: bool\n            The result of the disable\n    \"\"\"\n    # Do the render\n    with alog.ContextTimer(log.debug2, \"Render duration for %s\", component):\n        component.render_chart(session)\n        log.debug3(\n            \"Rendered objects for [%s]: %s\",\n            component,\n            [str(obj) for obj in component.managed_objects],\n        )\n\n    # Do the deploy\n    with alog.ContextTimer(log.debug2, \"Disable duration for %s: \", component):\n        return component.disable(session)\n</code></pre>"},{"location":"API%20References/#oper8.rollout_manager.verify_component","title":"<code>verify_component(session, component)</code>","text":"<p>Verify a component given a particular session</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current rollout session</p> required <code>component</code> <code>Component</code> <p>Component The component to verify</p> required <p>Returns:</p> Name Type Description <code>result</code> <code>bool</code> <p>bool The result of the verify</p> Source code in <code>oper8/rollout_manager.py</code> <pre><code>def verify_component(session: Session, component: Component) -&gt; bool:\n    \"\"\"Verify a component given a particular session\n\n    Args:\n        session: Session\n            The current rollout session\n        component: Component\n            The component to verify\n\n    Returns:\n        result: bool\n            The result of the verify\n    \"\"\"\n    # Do the verify\n    with alog.ContextTimer(log.debug2, \"Verify duration for %s: \", component):\n        return component.verify(session)\n</code></pre>"},{"location":"API%20References/#oper8.session","title":"<code>session</code>","text":"<p>This module holds the core session state for an individual reconciliation</p>"},{"location":"API%20References/#oper8.session.Session","title":"<code>Session</code>","text":"<p>A session is the core context manager for the state of an in-progress reconciliation</p> Source code in <code>oper8/session.py</code> <pre><code>class Session:  # pylint: disable=too-many-instance-attributes,too-many-public-methods\n    \"\"\"A session is the core context manager for the state of an in-progress\n    reconciliation\n    \"\"\"\n\n    # We strictly define the set of attributes that a Session can have to\n    # disallow arbitrary assignment\n    __slots__ = [\n        \"__components\",\n        \"__component_dependencies\",\n        \"__enabled_components\",\n        \"__disabled_components\",\n        \"__id\",\n        \"__cr_manifest\",\n        \"__config\",\n        \"__temporary_patches\",\n        \"__deploy_manager\",\n        \"__status\",\n        \"__current_version\",\n        \"__graph\",\n        # _app is retained for backwards compatibility\n        \"_app\",\n    ]\n\n    def __init__(  # pylint: disable=too-many-arguments\n        self,\n        reconciliation_id: str,\n        cr_manifest: aconfig.Config,\n        config: aconfig.Config,\n        deploy_manager: DeployManagerBase,\n        temporary_patches: Optional[List[dict]] = None,\n    ):\n        \"\"\"Construct a session object to hold the state for a reconciliation\n\n        Args:\n            reconciliation_id:  str\n                The unique ID for this reconciliation\n            cr_manifest:  aconfig.Config\n                The full value of the CR manifest that triggered this\n                reconciliation\n            config:  aconfig.Config\n                The compiled backend config for this reconciliation\n            deploy_manager:  DeployManagerBase\n                The preconfigured DeployManager in charge of running the actual\n                deploy operations for this deployment\n            temporary_patches:  list(dict)\n                List of temporary patch object to apply to resources managed by\n                this rollout\n        \"\"\"\n\n        ##################################################################\n        # Private Members: These members will be hidden from client code #\n        ##################################################################\n\n        # Mapping from component name to Component instance\n        self.__graph = Graph()\n\n        ###################################################\n        # Properties: These properties will be exposed as #\n        # @property members to be used by client code     #\n        ###################################################\n\n        self.__id = reconciliation_id\n        if not isinstance(cr_manifest, aconfig.Config):\n            cr_manifest = aconfig.Config(cr_manifest, override_env_vars=False)\n        self._validate_cr(cr_manifest)\n        self.__cr_manifest = cr_manifest\n        if not isinstance(config, aconfig.Config):\n            config = aconfig.Config(config, override_env_vars=False)\n        self.__config = config\n        self.__temporary_patches = temporary_patches or []\n\n        # The deploy manager that will be used to manage interactions with the\n        # cluster\n        self.__deploy_manager = deploy_manager\n\n        # Get the current status and version so that it can be referenced by the\n        # Application and Components that use it\n        self.__status = self.get_status()\n        self.__current_version = get_version(self.status)\n\n    ## Properties ##############################################################\n\n    @property\n    def id(self) -&gt; str:  # pylint: disable=invalid-name\n        \"\"\"The unique reconciliation ID\"\"\"\n        return self.__id\n\n    @property\n    def cr_manifest(self) -&gt; aconfig.Config:\n        \"\"\"The full CR manifest that triggered this reconciliation\"\"\"\n        return self.__cr_manifest\n\n    @property\n    def spec(self) -&gt; aconfig.Config:\n        \"\"\"The spec section of the CR manifest\"\"\"\n        return self.cr_manifest.get(\"spec\", aconfig.Config({}))\n\n    @property\n    def version(self) -&gt; str:\n        \"\"\"The spec.version for this CR\"\"\"\n        return get_manifest_version(self.cr_manifest)\n\n    @property\n    def metadata(self) -&gt; aconfig.Config:\n        \"\"\"The metadata for this CR\"\"\"\n        return self.cr_manifest.metadata\n\n    @property\n    def kind(self) -&gt; str:\n        \"\"\"The kind of the operand for this CR\"\"\"\n        return self.cr_manifest.kind\n\n    @property\n    def api_version(self) -&gt; str:\n        \"\"\"The api version of the operand for this CR\"\"\"\n        return self.cr_manifest.apiVersion\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"The metadata.name for this CR\"\"\"\n        return self.metadata.name\n\n    @property\n    def namespace(self) -&gt; str:\n        \"\"\"The metadata.namespace for this CR\"\"\"\n        return self.metadata.namespace\n\n    @property\n    def finalizers(self) -&gt; str:\n        \"\"\"The metadata.namespace for this CR\"\"\"\n\n        # Manually create finalizer list if it doesn't exist so its\n        # editable\n        if \"finalizers\" not in self.metadata:\n            self.metadata[\"finalizers\"] = []\n\n        return self.metadata.get(\"finalizers\")\n\n    @property\n    def config(self) -&gt; aconfig.Config:\n        \"\"\"The backend config for this reconciliation\"\"\"\n        return self.__config\n\n    @property\n    def temporary_patches(self) -&gt; List[aconfig.Config]:\n        \"\"\"Ordered list of temporary patches that apply to the operand being\n        reconciled\n        \"\"\"\n        return self.__temporary_patches\n\n    @property\n    def status(self) -&gt; aconfig.Config:\n        \"\"\"The operand status\"\"\"\n        return self.__status\n\n    @property\n    def current_version(self) -&gt; aconfig.Config:\n        \"\"\"The most recently reconciled version of the operand\"\"\"\n        return self.__current_version\n\n    @property\n    def deploy_manager(self) -&gt; DeployManagerBase:\n        \"\"\"Allow read access to the deploy manager\"\"\"\n        return self.__deploy_manager\n\n    @property\n    def graph(self) -&gt; str:  # pylint: disable=invalid-name\n        \"\"\"The component graph\"\"\"\n        return self.__graph\n\n    ## State Management ########################################################\n    #\n    # These functions are used by derived controllers in their setup_components\n    # implementations\n    ##\n\n    @alog.logged_function(log.debug2)\n    def add_component(self, component: COMPONENT_INSTANCE_TYPE):\n        \"\"\"Add a component to this deploy associated with a specific application\n\n        Args:\n            component:  Component\n                The component to add to this deploy\n            disabled:  bool\n                Whether or not the component is disabled in this deploy\n        \"\"\"\n        self.graph.add_node(component)\n\n    def add_component_dependency(\n        self,\n        component: Union[str, COMPONENT_INSTANCE_TYPE],\n        upstream_component: Union[str, COMPONENT_INSTANCE_TYPE],\n        verify_function: Optional[COMPONENT_VERIFY_FUNCTION] = None,\n    ):\n        \"\"\"Add a dependency indicating that one component requires an upstream\n        component to be deployed before it can be deployed.\n\n        Args:\n            component:  str or Component\n                The component or name of component in the deploy that must wait for the upstream\n            upstream_component:  str or Component\n                The upstream component or name of upstream that must be deployed before component\n            verify_function:  callable\n                A callable function of the form `def verify(session) -&gt; bool:`\n                to use to verify that the dependency has been satisfied. This\n                will be used to block deployment of the component beyond\n                requiring that the upstream has been deployed successfully.\n        \"\"\"\n        # Get component obj if name was provided\n        component_node = component\n        if isinstance(component, str):\n            component_node = self.get_component(component)\n\n        upstream_component_node = upstream_component\n        if isinstance(upstream_component, str):\n            upstream_component_node = self.get_component(upstream_component)\n\n        if not component_node or not upstream_component_node:\n            raise ValueError(\n                f\"Cannot add dependency [{component} -&gt; {upstream_component}]\",\n                \" for unknown component(s)\",\n            )\n\n        if component_node.disabled or upstream_component_node.disabled:\n            raise ValueError(\n                f\"Cannot add dependency [{component} -&gt; {upstream_component}]\",\n                \" for with disabled component(s)\",\n            )\n\n        # Add session parameter to verify function if one was provided\n        if verify_function:\n            verify_function = partial(verify_function, self)\n        self.graph.add_node_dependency(\n            component_node, upstream_component_node, verify_function\n        )\n\n    ## Utilities ###############################################################\n    #\n    # These utilities may be used anywhere in client code to perform common\n    # operations based on the state of the session.\n    ##\n    def get_component(\n        self, name: str, disabled: Optional[bool] = None\n    ) -&gt; Optional[COMPONENT_INSTANCE_TYPE]:\n        \"\"\"Get an individual component by name\n\n        Args:\n            name: str\n                Name of component to return\n            disabled: Optional[bool]\n                Option on wether to return disabled components. If this option is not supplied then\n                the referenced component will be returned irregardless whether its disabled\n                or enabled\n\n        Returns:\n            component: Optional[Component]\n                The component with the given name or None if component does not exit or does\n                not match disabled arg\n        \"\"\"\n        comp = self.graph.get_node(name)\n\n        # Only filter disabled/enabled components if the option was passed in.\n        if isinstance(disabled, bool):\n            if disabled:\n                return comp if comp.disabled else None\n            return comp if not comp.disabled else None\n\n        return comp\n\n    def get_components(self, disabled: bool = False) -&gt; List[COMPONENT_INSTANCE_TYPE]:\n        \"\"\"Get all components associated with an application\n\n        Args:\n            disabled:  bool\n                Whether to return disabled or enabled components\n\n        Returns:\n            components:  list(Component)\n                The list of Component objects associated with the given\n                application\n        \"\"\"\n        assert isinstance(\n            disabled, bool\n        ), \"Disabled flag must be a bool. You may be using the old function signature!\"\n\n        # Get list of all components.\n        comp_list = self.graph.get_all_nodes()\n\n        # Filter out disabled/enabled components using get_component\n        filtered_list = [\n            comp for comp in comp_list if self.get_component(comp.get_name(), disabled)\n        ]\n\n        return filtered_list\n\n    def get_component_dependencies(\n        self,\n        component: Union[str, COMPONENT_INSTANCE_TYPE],\n    ) -&gt; List[Tuple[COMPONENT_INSTANCE_TYPE, Optional[COMPONENT_VERIFY_FUNCTION]]]:\n        \"\"\"Get the list of (upstream_name, verify_function) tuples for a given\n        component.\n\n        NOTE: This is primarily for use inside of the RolloutManager. Do not use\n            this method in user code unless you know what you're doing!\n\n        Args:\n            component_name:  str\n                The name of the component to lookup dependencies for\n\n        Returns:\n            upstreams:  List[Tuple[str, Optional[VERIFY_FUNCTION]]]\n                The list of upstream (name, verify_fn) pairs\n        \"\"\"\n        component_node = component\n        if isinstance(component, str):\n            component_node = self.get_component(component)\n\n        return component_node.get_children()\n\n    def get_scoped_name(self, name: str) -&gt; str:\n        \"\"\"Get a name that is scoped to the application instance\n\n        Args:\n            name:  str\n                The name of a resource that will be managed by this operator\n                which should have instance name scoping applied\n\n        Returns:\n            scoped_name:  str\n                The scoped and truncated version of the input name\n        \"\"\"\n        scoped_name = self.get_truncated_name(f\"{self.name}-{name}\")\n        log.debug3(\"Scoped name [%s] -&gt; [%s]\", name, scoped_name)\n        return scoped_name\n\n    @staticmethod\n    def get_truncated_name(name: str) -&gt; str:\n        \"\"\"Perform truncation on a cluster name to make it conform to kubernetes\n        limits while remaining unique.\n\n        Args:\n            name:  str\n                The name of the resource that should be truncated and made\n                unique\n\n        Returns:\n            truncated_name:  str\n                A version of name that has been truncated and made unique\n        \"\"\"\n        if len(name) &gt; MAX_NAME_LEN:\n            sha = hashlib.sha256()\n            sha.update(name.encode(\"utf-8\"))\n            trunc_name = name[: MAX_NAME_LEN - 4] + sha.hexdigest()[:4]\n            log.debug2(\"Truncated name [%s] -&gt; [%s]\", name, trunc_name)\n            name = trunc_name\n        return name\n\n    def get_object_current_state(\n        self,\n        kind: str,\n        name: str,\n        api_version: Optional[str] = None,\n        namespace: Optional[str] = _SESSION_NAMESPACE,\n    ) -&gt; Tuple[bool, Optional[dict]]:\n        \"\"\"Get the current state of the given object in the namespace of this\n        session\n\n        Args:\n            kind:  str\n                The kind of the object to fetch\n            name:  str\n                The full name of the object to fetch\n            api_version:  str\n                The api_version of the resource kind to fetch\n\n        Returns:\n            success:  bool\n                Whether or not the state fetch operation succeeded\n            current_state:  dict or None\n                The dict representation of the current object's configuration,\n                or None if not present\n        \"\"\"\n        namespace = namespace if namespace != _SESSION_NAMESPACE else self.namespace\n        return self.deploy_manager.get_object_current_state(\n            kind=kind,\n            name=name,\n            namespace=namespace,\n            api_version=api_version,\n        )\n\n    def filter_objects_current_state(  # pylint: disable=too-many-arguments\n        self,\n        kind: str,\n        api_version: Optional[str] = None,\n        label_selector: Optional[str] = None,\n        field_selector: Optional[str] = None,\n        namespace: Optional[str] = _SESSION_NAMESPACE,\n    ) -&gt; Tuple[bool, List[dict]]:\n        \"\"\"Get the current state of the given object in the namespace of this\n        session\n\n        Args:\n            kind:  str\n                The kind of the object to fetch\n            label_selector:  str\n                The label selector to filter the results by\n            field_selector:  str\n                The field selector to filter the results by\n            api_version:  str\n                The api_version of the resource kind to fetch\n\n        Returns:\n            success:  bool\n                Whether or not the state fetch operation succeeded\n            current_state:  List[Dict]\n                The list of resources in dict representation,\n                or [] if none match\n        \"\"\"\n        namespace = namespace if namespace != _SESSION_NAMESPACE else self.namespace\n        return self.deploy_manager.filter_objects_current_state(\n            kind=kind,\n            namespace=namespace,\n            api_version=api_version,\n            label_selector=label_selector,\n            field_selector=field_selector,\n        )\n\n    @alog.logged_function(log.debug2)\n    @alog.timed_function(log.debug2)\n    def get_status(self) -&gt; dict:\n        \"\"\"Get the status of the resource being managed by this session or an\n        empty dict if not available\n\n        Returns:\n            current_status:  dict\n                The dict representation of the status subresource for the CR\n                being managed by this session\n        \"\"\"\n\n        # Pull the kind, name, and namespace\n        kind = self.cr_manifest.get(\"kind\")\n        name = self.name\n        api_version = self.api_version\n        log.debug3(\"Getting status for %s.%s/%s\", api_version, kind, name)\n\n        # Fetch the current status\n        success, content = self.get_object_current_state(\n            kind=kind,\n            name=name,\n            api_version=api_version,\n        )\n        assert_cluster(\n            success, f\"Failed to fetch status for [{api_version}/{kind}/{name}]\"\n        )\n        if content:\n            return content.get(\"status\", {})\n        return {}\n\n    ## Implementation Details ##################################################\n\n    @staticmethod\n    def _validate_cr(cr_manifest: aconfig.Config):\n        \"\"\"Ensure that all expected elements of the CR are present. Expected\n        elements are those that are guaranteed to be present by the kube API.\n        \"\"\"\n        assert \"kind\" in cr_manifest, \"CR missing required section ['kind']\"\n        assert \"apiVersion\" in cr_manifest, \"CR missing required section ['apiVersion']\"\n        assert \"metadata\" in cr_manifest, \"CR missing required section ['metadata']\"\n        assert (\n            \"name\" in cr_manifest.metadata\n        ), \"CR missing required section ['metadata.name']\"\n        assert (\n            \"namespace\" in cr_manifest.metadata\n        ), \"CR missing required section ['metadata.namespace']\"\n</code></pre>"},{"location":"API%20References/#oper8.session.Session.api_version","title":"<code>api_version</code>  <code>property</code>","text":"<p>The api version of the operand for this CR</p>"},{"location":"API%20References/#oper8.session.Session.config","title":"<code>config</code>  <code>property</code>","text":"<p>The backend config for this reconciliation</p>"},{"location":"API%20References/#oper8.session.Session.cr_manifest","title":"<code>cr_manifest</code>  <code>property</code>","text":"<p>The full CR manifest that triggered this reconciliation</p>"},{"location":"API%20References/#oper8.session.Session.current_version","title":"<code>current_version</code>  <code>property</code>","text":"<p>The most recently reconciled version of the operand</p>"},{"location":"API%20References/#oper8.session.Session.deploy_manager","title":"<code>deploy_manager</code>  <code>property</code>","text":"<p>Allow read access to the deploy manager</p>"},{"location":"API%20References/#oper8.session.Session.finalizers","title":"<code>finalizers</code>  <code>property</code>","text":"<p>The metadata.namespace for this CR</p>"},{"location":"API%20References/#oper8.session.Session.graph","title":"<code>graph</code>  <code>property</code>","text":"<p>The component graph</p>"},{"location":"API%20References/#oper8.session.Session.id","title":"<code>id</code>  <code>property</code>","text":"<p>The unique reconciliation ID</p>"},{"location":"API%20References/#oper8.session.Session.kind","title":"<code>kind</code>  <code>property</code>","text":"<p>The kind of the operand for this CR</p>"},{"location":"API%20References/#oper8.session.Session.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>The metadata for this CR</p>"},{"location":"API%20References/#oper8.session.Session.name","title":"<code>name</code>  <code>property</code>","text":"<p>The metadata.name for this CR</p>"},{"location":"API%20References/#oper8.session.Session.namespace","title":"<code>namespace</code>  <code>property</code>","text":"<p>The metadata.namespace for this CR</p>"},{"location":"API%20References/#oper8.session.Session.spec","title":"<code>spec</code>  <code>property</code>","text":"<p>The spec section of the CR manifest</p>"},{"location":"API%20References/#oper8.session.Session.status","title":"<code>status</code>  <code>property</code>","text":"<p>The operand status</p>"},{"location":"API%20References/#oper8.session.Session.temporary_patches","title":"<code>temporary_patches</code>  <code>property</code>","text":"<p>Ordered list of temporary patches that apply to the operand being reconciled</p>"},{"location":"API%20References/#oper8.session.Session.version","title":"<code>version</code>  <code>property</code>","text":"<p>The spec.version for this CR</p>"},{"location":"API%20References/#oper8.session.Session.__init__","title":"<code>__init__(reconciliation_id, cr_manifest, config, deploy_manager, temporary_patches=None)</code>","text":"<p>Construct a session object to hold the state for a reconciliation</p> <p>Parameters:</p> Name Type Description Default <code>reconciliation_id</code> <code>str</code> <p>str The unique ID for this reconciliation</p> required <code>cr_manifest</code> <code>Config</code> <p>aconfig.Config The full value of the CR manifest that triggered this reconciliation</p> required <code>config</code> <code>Config</code> <p>aconfig.Config The compiled backend config for this reconciliation</p> required <code>deploy_manager</code> <code>DeployManagerBase</code> <p>DeployManagerBase The preconfigured DeployManager in charge of running the actual deploy operations for this deployment</p> required <code>temporary_patches</code> <code>Optional[List[dict]]</code> <p>list(dict) List of temporary patch object to apply to resources managed by this rollout</p> <code>None</code> Source code in <code>oper8/session.py</code> <pre><code>def __init__(  # pylint: disable=too-many-arguments\n    self,\n    reconciliation_id: str,\n    cr_manifest: aconfig.Config,\n    config: aconfig.Config,\n    deploy_manager: DeployManagerBase,\n    temporary_patches: Optional[List[dict]] = None,\n):\n    \"\"\"Construct a session object to hold the state for a reconciliation\n\n    Args:\n        reconciliation_id:  str\n            The unique ID for this reconciliation\n        cr_manifest:  aconfig.Config\n            The full value of the CR manifest that triggered this\n            reconciliation\n        config:  aconfig.Config\n            The compiled backend config for this reconciliation\n        deploy_manager:  DeployManagerBase\n            The preconfigured DeployManager in charge of running the actual\n            deploy operations for this deployment\n        temporary_patches:  list(dict)\n            List of temporary patch object to apply to resources managed by\n            this rollout\n    \"\"\"\n\n    ##################################################################\n    # Private Members: These members will be hidden from client code #\n    ##################################################################\n\n    # Mapping from component name to Component instance\n    self.__graph = Graph()\n\n    ###################################################\n    # Properties: These properties will be exposed as #\n    # @property members to be used by client code     #\n    ###################################################\n\n    self.__id = reconciliation_id\n    if not isinstance(cr_manifest, aconfig.Config):\n        cr_manifest = aconfig.Config(cr_manifest, override_env_vars=False)\n    self._validate_cr(cr_manifest)\n    self.__cr_manifest = cr_manifest\n    if not isinstance(config, aconfig.Config):\n        config = aconfig.Config(config, override_env_vars=False)\n    self.__config = config\n    self.__temporary_patches = temporary_patches or []\n\n    # The deploy manager that will be used to manage interactions with the\n    # cluster\n    self.__deploy_manager = deploy_manager\n\n    # Get the current status and version so that it can be referenced by the\n    # Application and Components that use it\n    self.__status = self.get_status()\n    self.__current_version = get_version(self.status)\n</code></pre>"},{"location":"API%20References/#oper8.session.Session.add_component","title":"<code>add_component(component)</code>","text":"<p>Add a component to this deploy associated with a specific application</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>COMPONENT_INSTANCE_TYPE</code> <p>Component The component to add to this deploy</p> required <code>disabled</code> <p>bool Whether or not the component is disabled in this deploy</p> required Source code in <code>oper8/session.py</code> <pre><code>@alog.logged_function(log.debug2)\ndef add_component(self, component: COMPONENT_INSTANCE_TYPE):\n    \"\"\"Add a component to this deploy associated with a specific application\n\n    Args:\n        component:  Component\n            The component to add to this deploy\n        disabled:  bool\n            Whether or not the component is disabled in this deploy\n    \"\"\"\n    self.graph.add_node(component)\n</code></pre>"},{"location":"API%20References/#oper8.session.Session.add_component_dependency","title":"<code>add_component_dependency(component, upstream_component, verify_function=None)</code>","text":"<p>Add a dependency indicating that one component requires an upstream component to be deployed before it can be deployed.</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>Union[str, COMPONENT_INSTANCE_TYPE]</code> <p>str or Component The component or name of component in the deploy that must wait for the upstream</p> required <code>upstream_component</code> <code>Union[str, COMPONENT_INSTANCE_TYPE]</code> <p>str or Component The upstream component or name of upstream that must be deployed before component</p> required <code>verify_function</code> <code>Optional[COMPONENT_VERIFY_FUNCTION]</code> <p>callable A callable function of the form <code>def verify(session) -&gt; bool:</code> to use to verify that the dependency has been satisfied. This will be used to block deployment of the component beyond requiring that the upstream has been deployed successfully.</p> <code>None</code> Source code in <code>oper8/session.py</code> <pre><code>def add_component_dependency(\n    self,\n    component: Union[str, COMPONENT_INSTANCE_TYPE],\n    upstream_component: Union[str, COMPONENT_INSTANCE_TYPE],\n    verify_function: Optional[COMPONENT_VERIFY_FUNCTION] = None,\n):\n    \"\"\"Add a dependency indicating that one component requires an upstream\n    component to be deployed before it can be deployed.\n\n    Args:\n        component:  str or Component\n            The component or name of component in the deploy that must wait for the upstream\n        upstream_component:  str or Component\n            The upstream component or name of upstream that must be deployed before component\n        verify_function:  callable\n            A callable function of the form `def verify(session) -&gt; bool:`\n            to use to verify that the dependency has been satisfied. This\n            will be used to block deployment of the component beyond\n            requiring that the upstream has been deployed successfully.\n    \"\"\"\n    # Get component obj if name was provided\n    component_node = component\n    if isinstance(component, str):\n        component_node = self.get_component(component)\n\n    upstream_component_node = upstream_component\n    if isinstance(upstream_component, str):\n        upstream_component_node = self.get_component(upstream_component)\n\n    if not component_node or not upstream_component_node:\n        raise ValueError(\n            f\"Cannot add dependency [{component} -&gt; {upstream_component}]\",\n            \" for unknown component(s)\",\n        )\n\n    if component_node.disabled or upstream_component_node.disabled:\n        raise ValueError(\n            f\"Cannot add dependency [{component} -&gt; {upstream_component}]\",\n            \" for with disabled component(s)\",\n        )\n\n    # Add session parameter to verify function if one was provided\n    if verify_function:\n        verify_function = partial(verify_function, self)\n    self.graph.add_node_dependency(\n        component_node, upstream_component_node, verify_function\n    )\n</code></pre>"},{"location":"API%20References/#oper8.session.Session.filter_objects_current_state","title":"<code>filter_objects_current_state(kind, api_version=None, label_selector=None, field_selector=None, namespace=_SESSION_NAMESPACE)</code>","text":"<p>Get the current state of the given object in the namespace of this session</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>str The kind of the object to fetch</p> required <code>label_selector</code> <code>Optional[str]</code> <p>str The label selector to filter the results by</p> <code>None</code> <code>field_selector</code> <code>Optional[str]</code> <p>str The field selector to filter the results by</p> <code>None</code> <code>api_version</code> <code>Optional[str]</code> <p>str The api_version of the resource kind to fetch</p> <code>None</code> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool Whether or not the state fetch operation succeeded</p> <code>current_state</code> <code>List[dict]</code> <p>List[Dict] The list of resources in dict representation, or [] if none match</p> Source code in <code>oper8/session.py</code> <pre><code>def filter_objects_current_state(  # pylint: disable=too-many-arguments\n    self,\n    kind: str,\n    api_version: Optional[str] = None,\n    label_selector: Optional[str] = None,\n    field_selector: Optional[str] = None,\n    namespace: Optional[str] = _SESSION_NAMESPACE,\n) -&gt; Tuple[bool, List[dict]]:\n    \"\"\"Get the current state of the given object in the namespace of this\n    session\n\n    Args:\n        kind:  str\n            The kind of the object to fetch\n        label_selector:  str\n            The label selector to filter the results by\n        field_selector:  str\n            The field selector to filter the results by\n        api_version:  str\n            The api_version of the resource kind to fetch\n\n    Returns:\n        success:  bool\n            Whether or not the state fetch operation succeeded\n        current_state:  List[Dict]\n            The list of resources in dict representation,\n            or [] if none match\n    \"\"\"\n    namespace = namespace if namespace != _SESSION_NAMESPACE else self.namespace\n    return self.deploy_manager.filter_objects_current_state(\n        kind=kind,\n        namespace=namespace,\n        api_version=api_version,\n        label_selector=label_selector,\n        field_selector=field_selector,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.session.Session.get_component","title":"<code>get_component(name, disabled=None)</code>","text":"<p>Get an individual component by name</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>str Name of component to return</p> required <code>disabled</code> <code>Optional[bool]</code> <p>Optional[bool] Option on wether to return disabled components. If this option is not supplied then the referenced component will be returned irregardless whether its disabled or enabled</p> <code>None</code> <p>Returns:</p> Name Type Description <code>component</code> <code>Optional[COMPONENT_INSTANCE_TYPE]</code> <p>Optional[Component] The component with the given name or None if component does not exit or does not match disabled arg</p> Source code in <code>oper8/session.py</code> <pre><code>def get_component(\n    self, name: str, disabled: Optional[bool] = None\n) -&gt; Optional[COMPONENT_INSTANCE_TYPE]:\n    \"\"\"Get an individual component by name\n\n    Args:\n        name: str\n            Name of component to return\n        disabled: Optional[bool]\n            Option on wether to return disabled components. If this option is not supplied then\n            the referenced component will be returned irregardless whether its disabled\n            or enabled\n\n    Returns:\n        component: Optional[Component]\n            The component with the given name or None if component does not exit or does\n            not match disabled arg\n    \"\"\"\n    comp = self.graph.get_node(name)\n\n    # Only filter disabled/enabled components if the option was passed in.\n    if isinstance(disabled, bool):\n        if disabled:\n            return comp if comp.disabled else None\n        return comp if not comp.disabled else None\n\n    return comp\n</code></pre>"},{"location":"API%20References/#oper8.session.Session.get_component_dependencies","title":"<code>get_component_dependencies(component)</code>","text":"<p>Get the list of (upstream_name, verify_function) tuples for a given component.</p> This is primarily for use inside of the RolloutManager. Do not use <p>this method in user code unless you know what you're doing!</p> <p>Parameters:</p> Name Type Description Default <code>component_name</code> <p>str The name of the component to lookup dependencies for</p> required <p>Returns:</p> Name Type Description <code>upstreams</code> <code>List[Tuple[COMPONENT_INSTANCE_TYPE, Optional[COMPONENT_VERIFY_FUNCTION]]]</code> <p>List[Tuple[str, Optional[VERIFY_FUNCTION]]] The list of upstream (name, verify_fn) pairs</p> Source code in <code>oper8/session.py</code> <pre><code>def get_component_dependencies(\n    self,\n    component: Union[str, COMPONENT_INSTANCE_TYPE],\n) -&gt; List[Tuple[COMPONENT_INSTANCE_TYPE, Optional[COMPONENT_VERIFY_FUNCTION]]]:\n    \"\"\"Get the list of (upstream_name, verify_function) tuples for a given\n    component.\n\n    NOTE: This is primarily for use inside of the RolloutManager. Do not use\n        this method in user code unless you know what you're doing!\n\n    Args:\n        component_name:  str\n            The name of the component to lookup dependencies for\n\n    Returns:\n        upstreams:  List[Tuple[str, Optional[VERIFY_FUNCTION]]]\n            The list of upstream (name, verify_fn) pairs\n    \"\"\"\n    component_node = component\n    if isinstance(component, str):\n        component_node = self.get_component(component)\n\n    return component_node.get_children()\n</code></pre>"},{"location":"API%20References/#oper8.session.Session.get_components","title":"<code>get_components(disabled=False)</code>","text":"<p>Get all components associated with an application</p> <p>Parameters:</p> Name Type Description Default <code>disabled</code> <code>bool</code> <p>bool Whether to return disabled or enabled components</p> <code>False</code> <p>Returns:</p> Name Type Description <code>components</code> <code>List[COMPONENT_INSTANCE_TYPE]</code> <p>list(Component) The list of Component objects associated with the given application</p> Source code in <code>oper8/session.py</code> <pre><code>def get_components(self, disabled: bool = False) -&gt; List[COMPONENT_INSTANCE_TYPE]:\n    \"\"\"Get all components associated with an application\n\n    Args:\n        disabled:  bool\n            Whether to return disabled or enabled components\n\n    Returns:\n        components:  list(Component)\n            The list of Component objects associated with the given\n            application\n    \"\"\"\n    assert isinstance(\n        disabled, bool\n    ), \"Disabled flag must be a bool. You may be using the old function signature!\"\n\n    # Get list of all components.\n    comp_list = self.graph.get_all_nodes()\n\n    # Filter out disabled/enabled components using get_component\n    filtered_list = [\n        comp for comp in comp_list if self.get_component(comp.get_name(), disabled)\n    ]\n\n    return filtered_list\n</code></pre>"},{"location":"API%20References/#oper8.session.Session.get_object_current_state","title":"<code>get_object_current_state(kind, name, api_version=None, namespace=_SESSION_NAMESPACE)</code>","text":"<p>Get the current state of the given object in the namespace of this session</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>str The kind of the object to fetch</p> required <code>name</code> <code>str</code> <p>str The full name of the object to fetch</p> required <code>api_version</code> <code>Optional[str]</code> <p>str The api_version of the resource kind to fetch</p> <code>None</code> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool Whether or not the state fetch operation succeeded</p> <code>current_state</code> <code>Optional[dict]</code> <p>dict or None The dict representation of the current object's configuration, or None if not present</p> Source code in <code>oper8/session.py</code> <pre><code>def get_object_current_state(\n    self,\n    kind: str,\n    name: str,\n    api_version: Optional[str] = None,\n    namespace: Optional[str] = _SESSION_NAMESPACE,\n) -&gt; Tuple[bool, Optional[dict]]:\n    \"\"\"Get the current state of the given object in the namespace of this\n    session\n\n    Args:\n        kind:  str\n            The kind of the object to fetch\n        name:  str\n            The full name of the object to fetch\n        api_version:  str\n            The api_version of the resource kind to fetch\n\n    Returns:\n        success:  bool\n            Whether or not the state fetch operation succeeded\n        current_state:  dict or None\n            The dict representation of the current object's configuration,\n            or None if not present\n    \"\"\"\n    namespace = namespace if namespace != _SESSION_NAMESPACE else self.namespace\n    return self.deploy_manager.get_object_current_state(\n        kind=kind,\n        name=name,\n        namespace=namespace,\n        api_version=api_version,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.session.Session.get_scoped_name","title":"<code>get_scoped_name(name)</code>","text":"<p>Get a name that is scoped to the application instance</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>str The name of a resource that will be managed by this operator which should have instance name scoping applied</p> required <p>Returns:</p> Name Type Description <code>scoped_name</code> <code>str</code> <p>str The scoped and truncated version of the input name</p> Source code in <code>oper8/session.py</code> <pre><code>def get_scoped_name(self, name: str) -&gt; str:\n    \"\"\"Get a name that is scoped to the application instance\n\n    Args:\n        name:  str\n            The name of a resource that will be managed by this operator\n            which should have instance name scoping applied\n\n    Returns:\n        scoped_name:  str\n            The scoped and truncated version of the input name\n    \"\"\"\n    scoped_name = self.get_truncated_name(f\"{self.name}-{name}\")\n    log.debug3(\"Scoped name [%s] -&gt; [%s]\", name, scoped_name)\n    return scoped_name\n</code></pre>"},{"location":"API%20References/#oper8.session.Session.get_status","title":"<code>get_status()</code>","text":"<p>Get the status of the resource being managed by this session or an empty dict if not available</p> <p>Returns:</p> Name Type Description <code>current_status</code> <code>dict</code> <p>dict The dict representation of the status subresource for the CR being managed by this session</p> Source code in <code>oper8/session.py</code> <pre><code>@alog.logged_function(log.debug2)\n@alog.timed_function(log.debug2)\ndef get_status(self) -&gt; dict:\n    \"\"\"Get the status of the resource being managed by this session or an\n    empty dict if not available\n\n    Returns:\n        current_status:  dict\n            The dict representation of the status subresource for the CR\n            being managed by this session\n    \"\"\"\n\n    # Pull the kind, name, and namespace\n    kind = self.cr_manifest.get(\"kind\")\n    name = self.name\n    api_version = self.api_version\n    log.debug3(\"Getting status for %s.%s/%s\", api_version, kind, name)\n\n    # Fetch the current status\n    success, content = self.get_object_current_state(\n        kind=kind,\n        name=name,\n        api_version=api_version,\n    )\n    assert_cluster(\n        success, f\"Failed to fetch status for [{api_version}/{kind}/{name}]\"\n    )\n    if content:\n        return content.get(\"status\", {})\n    return {}\n</code></pre>"},{"location":"API%20References/#oper8.session.Session.get_truncated_name","title":"<code>get_truncated_name(name)</code>  <code>staticmethod</code>","text":"<p>Perform truncation on a cluster name to make it conform to kubernetes limits while remaining unique.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>str The name of the resource that should be truncated and made unique</p> required <p>Returns:</p> Name Type Description <code>truncated_name</code> <code>str</code> <p>str A version of name that has been truncated and made unique</p> Source code in <code>oper8/session.py</code> <pre><code>@staticmethod\ndef get_truncated_name(name: str) -&gt; str:\n    \"\"\"Perform truncation on a cluster name to make it conform to kubernetes\n    limits while remaining unique.\n\n    Args:\n        name:  str\n            The name of the resource that should be truncated and made\n            unique\n\n    Returns:\n        truncated_name:  str\n            A version of name that has been truncated and made unique\n    \"\"\"\n    if len(name) &gt; MAX_NAME_LEN:\n        sha = hashlib.sha256()\n        sha.update(name.encode(\"utf-8\"))\n        trunc_name = name[: MAX_NAME_LEN - 4] + sha.hexdigest()[:4]\n        log.debug2(\"Truncated name [%s] -&gt; [%s]\", name, trunc_name)\n        name = trunc_name\n    return name\n</code></pre>"},{"location":"API%20References/#oper8.setup_vcs","title":"<code>setup_vcs</code>","text":"<p>This module uses VCS to create a trimmed down repo with a selection of local branches and tags and a fully flattened history.</p>"},{"location":"API%20References/#oper8.setup_vcs.VCSRepoInitializer","title":"<code>VCSRepoInitializer</code>","text":"<p>This class encapsulates the key attributes of the VCS repo initialization process</p> Source code in <code>oper8/setup_vcs.py</code> <pre><code>class VCSRepoInitializer:\n    \"\"\"This class encapsulates the key attributes of the VCS repo initialization\n    process\n    \"\"\"\n\n    # The git repo that is being compressed for VCS versioning\n    source_repo: VCS\n    # The git repo where the VCS versioning repo is going to be created\n    dest_repo: VCS\n    # The remote within the destination repo that refers to the source repo\n    source_remote: str\n    # The reference to the root empty commit in the destination repo\n    root_ref: str\n\n    # Default branch name used when creating the repo\n    DEFAULT_BRANCH_NAME = \"__root__\"\n\n    # The name of the source remote\n    SOURCE_REMOTE = \"__source__\"\n\n    def __init__(self, source: str, destination: str, force: bool):\n        \"\"\"Initialize and set up the repos and common attributes\"\"\"\n\n        # Make sure the source is a git repo\n        try:\n            self.source_repo = VCS(source)\n        except VCSConfigError as err:\n            msg = f\"Invalid source git repo: {source}\"\n            log.error(msg)\n            raise ValueError(msg) from err\n        log.debug(\"Source Repo: %s\", source)\n\n        # Set up the dest and make sure it's empty\n        if os.path.isfile(destination):\n            msg = f\"Invalid destination: {destination} is a file\"\n            log.error(msg)\n            raise ValueError(msg)\n        os.makedirs(destination, exist_ok=True)\n        contents = os.listdir(destination)\n        if contents:\n            if not force:\n                msg = f\"Invalid destination: {destination} is not empty\"\n                log.error(msg)\n                raise ValueError(msg)\n            log.debug(\"Force cleaning dest %s\", destination)\n            for entry in contents:\n                full_path = os.path.join(destination, entry)\n                if os.path.isdir(full_path):\n                    log.debug3(\"Removing dir: %s\", full_path)\n                    shutil.rmtree(full_path)\n                else:\n                    log.debug3(\"Removing file: %s\", full_path)\n                    os.remove(full_path)\n\n        # Initialize the dest as an empty repo\n        log.info(\"Initializing dest repo: %s\", destination)\n        self.dest_repo = VCS(\n            destination, create_if_needed=True, initial_head=self.DEFAULT_BRANCH_NAME\n        )\n        self.dest_repo.create_commit(\"root\")\n        self.dest_repo.add_remote(self.SOURCE_REMOTE, source)\n        self.root_ref = self.dest_repo.head\n\n    def initialize_branches(\n        self,\n        branch_expr: Optional[List[str]],\n        tag_expr: Optional[List[str]],\n    ):\n        \"\"\"Perform the initialize of all branches in the destination repo from\n        the branches and tags that match the given expressions.\n        \"\"\"\n        # Get all tags and branches\n        tags = self._list_tags(self.source_repo)\n        branches = self._list_branches(self.source_repo)\n        log.debug2(\"All Tags: %s\", tags)\n        log.debug2(\"All Branches: %s\", branches)\n\n        # Filter the tags and branches by the filter arguments\n        keep_tags = self._filter_refs(tags, tag_expr)\n        keep_branches = self._filter_refs(branches, branch_expr)\n        log.debug2(\"Keep Tags: %s\", keep_tags)\n        log.debug2(\"Keep Branches: %s\", keep_branches)\n\n        # For each retained reference, fetch the ref from the source, check out the\n        # files to the dest, and make a fresh commit\n        for keep_ref in keep_tags:\n            log.debug(\"Making destination branch [%s] from tag\", keep_ref)\n            self._make_dest_branch(keep_ref, False)\n        for keep_ref in keep_branches:\n            log.debug(\"Making destination branch [%s] from branch\", keep_ref)\n            self._make_dest_branch(keep_ref, True)\n\n    def clean_up(self):\n        \"\"\"Clean out all unnecessary content from the destination repo\"\"\"\n\n        # Check the root back out\n        log.debug3(\"Checking out root\")\n        self.dest_repo.checkout_ref(self.root_ref)\n\n        # Delete the source remote\n        self.dest_repo.delete_remote(self.SOURCE_REMOTE)\n\n        # Remove all tags\n        for tag_name in self._list_tags(self.dest_repo):\n            self.dest_repo.delete_tag(tag_name)\n\n        # Remove the root branch and leave HEAD detached\n        self.dest_repo.checkout_detached_head()\n        self.dest_repo.delete_branch(self.DEFAULT_BRANCH_NAME)\n\n        # Compress the references to remove orphaned refs and objects\n        self.dest_repo.compress_references()\n\n    ## Impl ##\n\n    def _get_all_checkout_files(self, keep_ref: str) -&gt; List[str]:\n        \"\"\"Get all of the file paths in the given ref relative to the dest repo\n\n        # NOTE: This relies on pygit2 syntax!\n        \"\"\"\n        commit, _ = self.dest_repo.get_ref(keep_ref)\n        diff = commit.tree.diff_to_workdir()\n        return [delta.new_file.path for delta in diff.deltas]\n\n    def _make_dest_branch(self, keep_ref: str, is_branch: bool):\n        \"\"\"This is the function that does the main work of copying code from the\n        source to the destination and creating a clean commit.\n        \"\"\"\n        # Make sure the root is checked out in the destination repo\n        log.debug3(\"Checking out root\")\n        self.dest_repo.checkout_ref(self.root_ref)\n\n        # Fetch the ref to keep from the source in the dest\n        log.debug3(\"Fetching %s\", keep_ref)\n        self.dest_repo.fetch_remote(self.SOURCE_REMOTE, {keep_ref})\n\n        # Check out the files\n        remote_ref_name = keep_ref\n        if is_branch:\n            remote_ref_name = f\"refs/remotes/{self.SOURCE_REMOTE}/{keep_ref}\"\n        log.debug3(\"Checking out files for %s\", remote_ref_name)\n        self.dest_repo.checkout_ref(\n            remote_ref_name, paths=self._get_all_checkout_files(remote_ref_name)\n        )\n\n        # Make a new branch named with this ref's shorthand name with any remote\n        # information removed\n        branch_name = keep_ref\n        log.debug2(\"Dest branch name: %s\", branch_name)\n        root_commit, _ = self.dest_repo.get_ref(self.root_ref)\n        branch = self.dest_repo.create_branch(branch_name, root_commit)\n        self.dest_repo.checkout_ref(branch.name)\n\n        # Make a commit with these files\n        self.dest_repo.create_commit(keep_ref, parents=[root_commit.id])\n\n        # Check the root branch back out\n        self.dest_repo.checkout_ref(self.DEFAULT_BRANCH_NAME)\n\n    ## Static Helpers ##\n\n    @staticmethod\n    def _list_branches(repo: VCS) -&gt; List[str]:\n        \"\"\"List all of the local branches\n\n        Args:\n            repo (VCS): The repo to list\n\n        Returns\n            refs (List[str]): A set of all branch references\n        \"\"\"\n        refs = set()\n        for ref in repo.list_refs():\n            _, repo_ref = repo.get_ref(ref)\n            name_parts = repo_ref.name.split(\"/\")\n            if (\n                \"tags\" not in name_parts\n                and \"HEAD\" not in name_parts\n                and \"remotes\" not in name_parts\n                and repo_ref.name != \"refs/stash\"\n            ):\n                refs.add(ref)\n        return sorted(sorted(refs))\n\n    @staticmethod\n    def _list_tags(repo: VCS) -&gt; List[str]:\n        \"\"\"List all of the tags and references in the repo\n\n        Args:\n            repo (VCS): The repo to list\n\n        Returns\n            refs (List[str]): A set of all tag references\n        \"\"\"\n        return list(\n            sorted(\n                {\n                    ref\n                    for ref in repo.list_refs()\n                    if \"refs/tags\" in repo.get_ref(ref)[1].name\n                }\n            )\n        )\n\n    @staticmethod\n    def _filter_refs(refs: List[str], exprs: Optional[List[str]]) -&gt; List[str]:\n        \"\"\"Keep all refs that match at least one of the expressions\"\"\"\n        return [ref for ref in refs if any(re.match(expr, ref) for expr in exprs or [])]\n</code></pre>"},{"location":"API%20References/#oper8.setup_vcs.VCSRepoInitializer.__init__","title":"<code>__init__(source, destination, force)</code>","text":"<p>Initialize and set up the repos and common attributes</p> Source code in <code>oper8/setup_vcs.py</code> <pre><code>def __init__(self, source: str, destination: str, force: bool):\n    \"\"\"Initialize and set up the repos and common attributes\"\"\"\n\n    # Make sure the source is a git repo\n    try:\n        self.source_repo = VCS(source)\n    except VCSConfigError as err:\n        msg = f\"Invalid source git repo: {source}\"\n        log.error(msg)\n        raise ValueError(msg) from err\n    log.debug(\"Source Repo: %s\", source)\n\n    # Set up the dest and make sure it's empty\n    if os.path.isfile(destination):\n        msg = f\"Invalid destination: {destination} is a file\"\n        log.error(msg)\n        raise ValueError(msg)\n    os.makedirs(destination, exist_ok=True)\n    contents = os.listdir(destination)\n    if contents:\n        if not force:\n            msg = f\"Invalid destination: {destination} is not empty\"\n            log.error(msg)\n            raise ValueError(msg)\n        log.debug(\"Force cleaning dest %s\", destination)\n        for entry in contents:\n            full_path = os.path.join(destination, entry)\n            if os.path.isdir(full_path):\n                log.debug3(\"Removing dir: %s\", full_path)\n                shutil.rmtree(full_path)\n            else:\n                log.debug3(\"Removing file: %s\", full_path)\n                os.remove(full_path)\n\n    # Initialize the dest as an empty repo\n    log.info(\"Initializing dest repo: %s\", destination)\n    self.dest_repo = VCS(\n        destination, create_if_needed=True, initial_head=self.DEFAULT_BRANCH_NAME\n    )\n    self.dest_repo.create_commit(\"root\")\n    self.dest_repo.add_remote(self.SOURCE_REMOTE, source)\n    self.root_ref = self.dest_repo.head\n</code></pre>"},{"location":"API%20References/#oper8.setup_vcs.VCSRepoInitializer.clean_up","title":"<code>clean_up()</code>","text":"<p>Clean out all unnecessary content from the destination repo</p> Source code in <code>oper8/setup_vcs.py</code> <pre><code>def clean_up(self):\n    \"\"\"Clean out all unnecessary content from the destination repo\"\"\"\n\n    # Check the root back out\n    log.debug3(\"Checking out root\")\n    self.dest_repo.checkout_ref(self.root_ref)\n\n    # Delete the source remote\n    self.dest_repo.delete_remote(self.SOURCE_REMOTE)\n\n    # Remove all tags\n    for tag_name in self._list_tags(self.dest_repo):\n        self.dest_repo.delete_tag(tag_name)\n\n    # Remove the root branch and leave HEAD detached\n    self.dest_repo.checkout_detached_head()\n    self.dest_repo.delete_branch(self.DEFAULT_BRANCH_NAME)\n\n    # Compress the references to remove orphaned refs and objects\n    self.dest_repo.compress_references()\n</code></pre>"},{"location":"API%20References/#oper8.setup_vcs.VCSRepoInitializer.initialize_branches","title":"<code>initialize_branches(branch_expr, tag_expr)</code>","text":"<p>Perform the initialize of all branches in the destination repo from the branches and tags that match the given expressions.</p> Source code in <code>oper8/setup_vcs.py</code> <pre><code>def initialize_branches(\n    self,\n    branch_expr: Optional[List[str]],\n    tag_expr: Optional[List[str]],\n):\n    \"\"\"Perform the initialize of all branches in the destination repo from\n    the branches and tags that match the given expressions.\n    \"\"\"\n    # Get all tags and branches\n    tags = self._list_tags(self.source_repo)\n    branches = self._list_branches(self.source_repo)\n    log.debug2(\"All Tags: %s\", tags)\n    log.debug2(\"All Branches: %s\", branches)\n\n    # Filter the tags and branches by the filter arguments\n    keep_tags = self._filter_refs(tags, tag_expr)\n    keep_branches = self._filter_refs(branches, branch_expr)\n    log.debug2(\"Keep Tags: %s\", keep_tags)\n    log.debug2(\"Keep Branches: %s\", keep_branches)\n\n    # For each retained reference, fetch the ref from the source, check out the\n    # files to the dest, and make a fresh commit\n    for keep_ref in keep_tags:\n        log.debug(\"Making destination branch [%s] from tag\", keep_ref)\n        self._make_dest_branch(keep_ref, False)\n    for keep_ref in keep_branches:\n        log.debug(\"Making destination branch [%s] from branch\", keep_ref)\n        self._make_dest_branch(keep_ref, True)\n</code></pre>"},{"location":"API%20References/#oper8.setup_vcs.setup_vcs","title":"<code>setup_vcs(source, destination=None, branch_expr=None, tag_expr=__UNSET__, force=False)</code>","text":"<p>This utility will initialize an operator's VCS directory for use with oper8's VCS versioning.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the source repository on disk</p> required <code>destination</code> <code>Optional[str]</code> <p>The path where the VCS repo should be created</p> <code>None</code> <code>branch_expr</code> <code>Optional[List[str]]</code> <p>Regular expression(s) to use to identify branches to retain in the VCS repo</p> <code>None</code> <code>tag_expr</code> <code>Optional[List[str]]</code> <p>Regular expression(s) to use to identify tags to retain in the VCS repo</p> <code>__UNSET__</code> <code>force</code> <code>bool</code> <p>Force overwrite existing destination</p> <code>False</code> Source code in <code>oper8/setup_vcs.py</code> <pre><code>def setup_vcs(\n    source: str,\n    destination: Optional[str] = None,\n    branch_expr: Optional[List[str]] = None,\n    tag_expr: Optional[List[str]] = __UNSET__,\n    force: bool = False,\n):\n    \"\"\"This utility will initialize an operator's VCS directory for use with\n    oper8's VCS versioning.\n\n    Args:\n        source (str): The path to the source repository on disk\n        destination (Optional[str]): The path where the VCS repo should be\n            created\n        branch_expr (Optional[List[str]]): Regular expression(s) to use to\n            identify branches to retain in the VCS repo\n        tag_expr (Optional[List[str]]): Regular expression(s) to use to\n            identify tags to retain in the VCS repo\n        force (bool): Force overwrite existing destination\n    \"\"\"\n    initializer = VCSRepoInitializer(\n        source=source, destination=destination or DEFAULT_DEST, force=force\n    )\n    initializer.initialize_branches(\n        branch_expr=branch_expr,\n        tag_expr=tag_expr if tag_expr is not __UNSET__ else [DEFAULT_TAG_EXPR],\n    )\n    initializer.clean_up()\n</code></pre>"},{"location":"API%20References/#oper8.status","title":"<code>status</code>","text":"<p>This module holds the common functionality used to represent the status of resources managed by oper8</p> <p>Oper8 supports the following orthogonal status conditions:</p> <ul> <li>Ready: True if the service is able to take traffic</li> <li>Updating: True if a modification is being actively applied to the application</li> </ul> <p>Additionally, oper8 supports a top-level status element to report the detailed status of the managed components. The schema is: {     \"componentStatus\": {         \"allComponents\": [list of all node names],         \"deployedComponents\": [list of nodes that have successfully deployed],         \"verifiedComponents\": [list of nodes that have successfully verified],         \"failedComponents\": [list of nodes that have successfully verified],         \"deployed\": \"N/M\",         \"verified\": \"N/M\",     } }</p>"},{"location":"API%20References/#oper8.status.ReadyReason","title":"<code>ReadyReason</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Nested class to hold reason constants for the Ready condition</p> Source code in <code>oper8/status.py</code> <pre><code>class ReadyReason(Enum):\n    \"\"\"Nested class to hold reason constants for the Ready condition\"\"\"\n\n    # The application is stable and ready for traffic\n    STABLE = \"Stable\"\n\n    # The application is rolling out for the first time\n    INITIALIZING = \"Initializing\"\n\n    # The application rollout is in progress and will continue\n    # the next reconcile\n    IN_PROGRESS = \"InProgress\"\n\n    # The application has hit an unrecoverable config error during rollout\n    CONFIG_ERROR = \"ConfigError\"\n\n    # The application has hit an unrecoverable error during rollout\n    ERRORED = \"Errored\"\n</code></pre>"},{"location":"API%20References/#oper8.status.ServiceStatus","title":"<code>ServiceStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Nested class to hold status constants for the service status</p> Source code in <code>oper8/status.py</code> <pre><code>class ServiceStatus(Enum):\n    \"\"\"Nested class to hold status constants for the service status\"\"\"\n\n    # Installation or Update reconciliation is in-progress\n    IN_PROGRESS = \"InProgress\"\n\n    # Installation or Update failed with error\n    FAILED = \"Failed\"\n\n    # Service is in stable state\n    COMPLETED = \"Completed\"\n</code></pre>"},{"location":"API%20References/#oper8.status.UpdatingReason","title":"<code>UpdatingReason</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Nested class to hold reason constants for the Updating condition</p> Source code in <code>oper8/status.py</code> <pre><code>class UpdatingReason(Enum):\n    \"\"\"Nested class to hold reason constants for the Updating condition\"\"\"\n\n    # There are no updates to apply to the application\n    STABLE = \"Stable\"\n\n    # A required precondition was not met\n    PRECONDITION_WAIT = \"PreconditionWait\"\n\n    # A required deployment verification condition was not met\n    VERIFY_WAIT = \"VerifyWait\"\n\n    # The application attempted to perform an operation against the cluster that\n    # failed unexpectedly\n    CLUSTER_ERROR = \"ClusterError\"\n\n    # An error occurred, so the application is not attempting to update\n    ERRORED = \"Errored\"\n\n    # Version upgrade is initiated\n    VERSION_CHANGE = \"VersionChange\"\n</code></pre>"},{"location":"API%20References/#oper8.status.get_condition","title":"<code>get_condition(type_name, current_status)</code>","text":"<p>Extract the given condition type from a status object</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <p>str The condition type to fetch</p> required <code>current_status</code> <code>dict</code> <p>dict The dict representation of the status for a given application</p> required <p>Returns:</p> Name Type Description <code>condition</code> <code>dict</code> <p>dict The condition object if found, empty dict otherwise</p> Source code in <code>oper8/status.py</code> <pre><code>def get_condition(type_name: str, current_status: dict) -&gt; dict:\n    \"\"\"Extract the given condition type from a status object\n\n    Args:\n        type:  str\n            The condition type to fetch\n        current_status:  dict\n            The dict representation of the status for a given application\n\n    Returns:\n        condition:  dict\n            The condition object if found, empty dict otherwise\n    \"\"\"\n    cond = [\n        cond\n        for cond in current_status.get(\"conditions\", [])\n        if cond.get(\"type\") == type_name\n    ]\n    if cond:\n        assert len(cond) == 1, f\"Found multiple condition entries for {type_name}\"\n        return cond[0]\n    return {}\n</code></pre>"},{"location":"API%20References/#oper8.status.get_version","title":"<code>get_version(current_status)</code>","text":"<p>Extract the current version (not desired version) from a status object</p> <p>Parameters:</p> Name Type Description Default <code>current_status</code> <code>dict</code> <p>dict The dict representation of the status for a given application</p> required <p>Returns:</p> Name Type Description <code>version</code> <code>Optional[str]</code> <p>Optional[dict] The current version if found in a status object, None otherwise.</p> Source code in <code>oper8/status.py</code> <pre><code>def get_version(current_status: dict) -&gt; Optional[str]:\n    \"\"\"Extract the current version (not desired version) from a status object\n\n    Args:\n        current_status: dict\n            The dict representation of the status for a given application\n\n    Returns:\n        version: Optional[dict]\n            The current version if found in a status object, None otherwise.\n\n    \"\"\"\n    return nested_get(current_status, VERSIONS_FIELD_CURRENT_VERSION)\n</code></pre>"},{"location":"API%20References/#oper8.status.make_application_status","title":"<code>make_application_status(ready_reason=None, ready_message='', updating_reason=None, updating_message='', component_state=None, external_conditions=None, external_status=None, version=None, supported_versions=None, operator_version=None, kind=None)</code>","text":"<p>Create a full status object for an application</p> <p>Parameters:</p> Name Type Description Default <code>ready_reason</code> <code>Optional[Union[ReadyReason, str]]</code> <p>Optional[ReadyReason or str] The reason enum for the Ready condition</p> <code>None</code> <code>ready_message</code> <code>str</code> <p>str Plain-text message explaining the Ready condition value</p> <code>''</code> <code>updating_reason</code> <code>Optional[Union[UpdatingReason, str]]</code> <p>Optional[UpdatingReason or str] The reason enum for the Updating condition</p> <code>None</code> <code>updating_message</code> <code>str</code> <p>str Plain-text message explaining the Updating condition value</p> <code>''</code> <code>component_state</code> <code>Optional[CompletionState]</code> <p>Optional[CompletionState] The terminal state of components in the latest rollout</p> <code>None</code> <code>external_conditions</code> <code>Optional[List[dict]]</code> <p>Optional[List[dict]] Additional conditions to include in the update</p> <code>None</code> <code>external_status</code> <code>Optional[dict]</code> <p>Optional[dict] Additional key/value status elements besides \"conditions\" that should be preserved through the update</p> <code>None</code> <code>version</code> <code>Optional[str]</code> <p>Optional[str] The verified version of the application</p> <code>None</code> <code>supported_versions</code> <code>Optional[List[str]]</code> <p>Optional[List[str]] The list of supported versions for this application</p> <code>None</code> <code>operator_version</code> <code>Optional[str]</code> <p>Optional[str] The operator version for this application</p> <code>None</code> <code>kind</code> <code>Optional[str]</code> <p>Optional[str] The kind of reconciling CR. If specified, this function adds service status field which is compliant with IBM Cloud Pak requirements.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>current_status</code> <code>dict</code> <p>dict Dict representation of the status for the application</p> Source code in <code>oper8/status.py</code> <pre><code>def make_application_status(  # pylint: disable=too-many-arguments,too-many-locals\n    ready_reason: Optional[Union[ReadyReason, str]] = None,\n    ready_message: str = \"\",\n    updating_reason: Optional[Union[UpdatingReason, str]] = None,\n    updating_message: str = \"\",\n    component_state: Optional[CompletionState] = None,\n    external_conditions: Optional[List[dict]] = None,\n    external_status: Optional[dict] = None,\n    version: Optional[str] = None,\n    supported_versions: Optional[List[str]] = None,\n    operator_version: Optional[str] = None,\n    kind: Optional[str] = None,\n) -&gt; dict:\n    \"\"\"Create a full status object for an application\n\n    Args:\n        ready_reason:  Optional[ReadyReason or str]\n            The reason enum for the Ready condition\n        ready_message:  str\n            Plain-text message explaining the Ready condition value\n        updating_reason:  Optional[UpdatingReason or str]\n            The reason enum for the Updating condition\n        updating_message:  str\n            Plain-text message explaining the Updating condition value\n        component_state:  Optional[CompletionState]\n            The terminal state of components in the latest rollout\n        external_conditions:  Optional[List[dict]]\n            Additional conditions to include in the update\n        external_status:  Optional[dict]\n            Additional key/value status elements besides \"conditions\" that\n            should be preserved through the update\n        version:  Optional[str]\n            The verified version of the application\n        supported_versions:  Optional[List[str]]\n            The list of supported versions for this application\n        operator_version:  Optional[str]\n            The operator version for this application\n        kind: Optional[str]\n            The kind of reconciling CR. If specified, this function adds\n            service status field which is compliant with IBM Cloud Pak\n            requirements.\n\n    Returns:\n        current_status:  dict\n            Dict representation of the status for the application\n    \"\"\"\n    now = datetime.now()\n    conditions = []\n    if ready_reason is not None:\n        conditions.append(_make_ready_condition(ready_reason, ready_message, now))\n    if updating_reason is not None:\n        conditions.append(\n            _make_updating_condition(updating_reason, updating_message, now)\n        )\n    conditions.extend(external_conditions or [])\n    status = external_status or {}\n    status[\"conditions\"] = conditions\n\n    # If a component_state is given, create the top-level status elements to\n    # track which components have deployed and verified\n    if component_state is not None:\n        log.debug2(\"Adding component state to status\")\n        status[COMPONENT_STATUS] = _make_component_state(component_state)\n        log.debug3(status[COMPONENT_STATUS])\n\n    # Create the versions section\n    if version is not None:\n        nested_set(status, VERSIONS_FIELD_CURRENT_VERSION, version)\n    if supported_versions is not None:\n        nested_set(\n            status,\n            VERSIONS_FIELD_AVAILABLE_VERSIONS,\n            [_make_available_version(version) for version in supported_versions],\n        )\n    if operator_version is not None:\n        nested_set(status, OPERATOR_VERSION, operator_version)\n\n    # Create service status section\n    if kind:\n        # make field name follow k8s naming convention\n        service_status_field = kind[0].lower()\n        if len(kind) &gt; 1:\n            service_status_field += kind[1:]\n        service_status_field += \"Status\"\n\n        # Only update service status if the current value is set by oper8. This\n        # allows services to override the service status section\n        current_service_status = status.get(service_status_field)\n        managed_service_values = [status.value for status in ServiceStatus]\n        if (\n            not current_service_status\n            or current_service_status in managed_service_values\n        ):\n            current_service_status = _make_service_status(\n                ready_reason, updating_reason\n            ).value\n\n        status[service_status_field] = current_service_status\n\n    return status\n</code></pre>"},{"location":"API%20References/#oper8.status.status_changed","title":"<code>status_changed(current_status, new_status)</code>","text":"<p>Compare two status objects to determine if there is a meaningful change between the current status and the proposed new status. A meaningful change is defined as any change besides a timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>current_status</code> <code>dict</code> <p>dict The raw status dict from the current CR</p> required <code>new_status</code> <code>dict</code> <p>dict The proposed new status</p> required <p>Returns:</p> Name Type Description <code>status_changed</code> <code>bool</code> <p>bool True if there is a meaningful change between the current status and the new status</p> Source code in <code>oper8/status.py</code> <pre><code>def status_changed(current_status: dict, new_status: dict) -&gt; bool:\n    \"\"\"Compare two status objects to determine if there is a meaningful change\n    between the current status and the proposed new status. A meaningful change\n    is defined as any change besides a timestamp.\n\n    Args:\n        current_status:  dict\n            The raw status dict from the current CR\n        new_status:  dict\n            The proposed new status\n\n    Returns:\n        status_changed:  bool\n            True if there is a meaningful change between the current status and\n            the new status\n    \"\"\"\n    # Status objects must be dicts\n    if not isinstance(current_status, dict) or not isinstance(new_status, dict):\n        return True\n\n    # Perform a deep diff, excluding timestamps\n    return bool(\n        DeepDiff(\n            current_status,\n            new_status,\n            exclude_obj_callback=lambda _, path: path.endswith(f\"{TIMESTAMP_KEY}']\"),\n        )\n    )\n</code></pre>"},{"location":"API%20References/#oper8.status.update_application_status","title":"<code>update_application_status(current_status, **kwargs)</code>","text":"<p>Create an updated status based on the values in the current status</p> <p>Parameters:</p> Name Type Description Default <code>current_status</code> <code>dict</code> <p>dict The dict representation of the status for a given application</p> required <code>**kwargs</code> <p>Additional keyword args to pass to make_application_status</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>updated_status</code> <code>dict</code> <p>dict Updated dict representation of the status for the application</p> Source code in <code>oper8/status.py</code> <pre><code>def update_application_status(current_status: dict, **kwargs) -&gt; dict:\n    \"\"\"Create an updated status based on the values in the current status\n\n    Args:\n        current_status:  dict\n            The dict representation of the status for a given application\n        **kwargs:\n            Additional keyword args to pass to make_application_status\n\n    Returns:\n        updated_status:  dict\n            Updated dict representation of the status for the application\n    \"\"\"\n    # Make a deep copy of current_status so that we aren't accidentally\n    # modifying the current status object. This prevents a bug where status\n    # changes are not detected\n    current_status = copy.deepcopy(current_status)\n\n    # Make a dict of type -&gt; condition. This is necessary because other\n    # conditions may be applied by ansible\n    current_conditions = current_status.get(\"conditions\", [])\n    current_condition_map = {cond[\"type\"]: cond for cond in current_conditions}\n    ready_cond = current_condition_map.get(READY_CONDITION, {})\n    updating_cond = current_condition_map.get(UPDATING_CONDITION, {})\n\n    # Setup the kwargs for the status call\n    ready_reason = ready_cond.get(\"reason\")\n    updating_reason = updating_cond.get(\"reason\")\n    if ready_reason:\n        kwargs.setdefault(\"ready_reason\", ReadyReason(ready_reason))\n    if updating_reason:\n        kwargs.setdefault(\"updating_reason\", UpdatingReason(updating_reason))\n    kwargs.setdefault(\"ready_message\", ready_cond.get(\"message\", \"\"))\n    kwargs.setdefault(\"updating_message\", updating_cond.get(\"message\", \"\"))\n\n    # Extract external conditions managed by other portions of the operator\n    external_conditions = [\n        cond\n        for cond in current_conditions\n        if cond.get(\"type\") not in [READY_CONDITION, UPDATING_CONDITION]\n    ]\n    log.debug3(\"External conditions: %s\", external_conditions)\n    kwargs[\"external_conditions\"] = external_conditions\n    log.debug3(\"Merged status kwargs: %s\", kwargs)\n\n    # Extract external status elements (besides conditions) managed by other\n    # portions of the operator\n    external_status = {\n        key: val for key, val in current_status.items() if key != \"conditions\"\n    }\n    kwargs[\"external_status\"] = external_status\n    kwargs[\"operator_version\"] = config.operator_version\n\n    return make_application_status(**kwargs)\n</code></pre>"},{"location":"API%20References/#oper8.status.update_resource_status","title":"<code>update_resource_status(deploy_manager, kind, api_version, name, namespace, **kwargs)</code>","text":"<p>Create an updated status based on the values in the current status</p> <p>Parameters:</p> Name Type Description Default <code>deploy_manager</code> <code>DeployManagerBase</code> <p>DeployManagerBase The deploymanager used to get and set status</p> required <code>kind</code> <code>str</code> <p>str The kind of the resource</p> required <code>api_version</code> <code>str</code> <p>str The api_version of the resource</p> required <code>name</code> <code>str</code> <p>str The name of the resource</p> required <code>namespace</code> <code>str</code> <p>str The namespace the resource is located in</p> required <code>**kwargs</code> <code>dict</code> <p>Dict Any additional keyword arguments to be passed to update_application_status</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>status_object</code> <code>dict</code> <p>Dict The applied status if successful</p> Source code in <code>oper8/status.py</code> <pre><code>def update_resource_status(\n    deploy_manager: \"DeployManagerBase\",  # noqa: F821\n    kind: str,\n    api_version: str,\n    name: str,\n    namespace: str,\n    **kwargs: dict,\n) -&gt; dict:\n    \"\"\"Create an updated status based on the values in the current status\n\n    Args:\n        deploy_manager: DeployManagerBase\n            The deploymanager used to get and set status\n        kind: str\n            The kind of the resource\n        api_version: str\n            The api_version of the resource\n        name: str\n            The name of the resource\n        namespace: str\n            The namespace the resource is located in\n        **kwargs: Dict\n            Any additional keyword arguments to be passed to update_application_status\n\n    Returns:\n        status_object: Dict\n            The applied status if successful\n\n    \"\"\"\n    log.debug3(\n        \"Updating status for %s/%s.%s/%s\",\n        namespace,\n        api_version,\n        kind,\n        name,\n    )\n\n    # Fetch the current status from the cluster\n    success, current_state = deploy_manager.get_object_current_state(\n        api_version=api_version,\n        kind=kind,\n        name=name,\n        namespace=namespace,\n    )\n    if not success:\n        log.warning(\"Failed to fetch current state for %s/%s/%s\", namespace, kind, name)\n        return {}\n    current_status = (current_state or {}).get(\"status\", {})\n    log.debug3(\"Pre-update status: %s\", current_status)\n\n    # Merge in the given status\n    status_object = update_application_status(current_status, kind=kind, **kwargs)\n    log.debug3(\"Updated status: %s\", status_object)\n\n    # Check to see if the status values of any conditions have changed and\n    # only update the status if it has changed\n    if status_changed(current_status, status_object):\n        log.debug(\"Found meaningful change. Updating status\")\n        log.debug2(\"(current) %s != (updated) %s\", current_status, status_object)\n\n        # Do the update\n        success, _ = deploy_manager.set_status(\n            kind=kind,\n            name=name,\n            namespace=namespace,\n            api_version=api_version,\n            status=status_object,\n        )\n\n        # Since this is just a status update, we don't fail if the update fails,\n        # but we do throw a warning\n        if not success:\n            log.warning(\"Failed to update status for [%s/%s/%s]\", namespace, kind, name)\n            return {}\n\n    return status_object\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers","title":"<code>test_helpers</code>","text":""},{"location":"API%20References/#oper8.test_helpers.helpers","title":"<code>helpers</code>","text":"<p>This module holds common helper functions for making testing easy</p>"},{"location":"API%20References/#oper8.test_helpers.helpers.DummyComponentBase","title":"<code>DummyComponentBase</code>","text":"<p>               Bases: <code>Component</code></p> <p>This base class provides all of the common functionality for DummyComponent and DummyLegacyComponent</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>class DummyComponentBase(Component):\n    \"\"\"This base class provides all of the common functionality for\n    DummyComponent and DummyLegacyComponent\n    \"\"\"\n\n    def __init__(\n        self,\n        name=None,\n        session=None,\n        api_objects=None,\n        api_object_deps=None,\n        render_chart_fail=False,\n        deploy_fail=False,\n        disable_fail=False,\n        verify_fail=False,\n        build_chart_fail=False,\n        disabled=False,\n        **kwargs,\n    ):\n        # Hang onto config inputs\n        self.api_objects = api_objects or []\n        self.api_object_deps = api_object_deps or {}\n\n        # Mock passthroughs to the base class\n        self.render_chart_fail = render_chart_fail\n        self.render_chart = mock.Mock(\n            side_effect=get_failable_method(self.render_chart_fail, self.render_chart)\n        )\n        self.deploy_fail = deploy_fail\n        self.deploy = mock.Mock(\n            side_effect=get_failable_method(self.deploy_fail, super().deploy)\n        )\n        self.disable_fail = disable_fail\n        self.disable = mock.Mock(\n            side_effect=get_failable_method(self.disable_fail, super().disable)\n        )\n        self.verify_fail = verify_fail\n        self.verify = mock.Mock(\n            side_effect=get_failable_method(self.verify_fail, super().verify)\n        )\n        self.build_chart_fail = build_chart_fail\n        self.build_chart = mock.Mock(\n            side_effect=get_failable_method(self.build_chart_fail, self.build_chart)\n        )\n\n        # Initialize Component\n        if name is None:\n            super().__init__(session=session, disabled=disabled)\n        else:\n            super().__init__(name=name, session=session, disabled=disabled)\n\n    @alog.logged_function(log.debug2)\n    def _add_resources(self, scope, session):\n        \"\"\"This will be called in both implementations in their respective\n        places\n        \"\"\"\n        api_objs = self._gather_dummy_resources(scope, session)\n\n        # Add dependencies between objects\n        for downstream_name, upstreams in self.api_object_deps.items():\n            assert downstream_name in api_objs, \"Bad test config\"\n            downstream = api_objs[downstream_name]\n            for upstream_name in upstreams:\n                assert upstream_name in api_objs, \"Bad test config\"\n                upstream = api_objs[upstream_name]\n                downstream.add_dependency(upstream)\n\n    def _gather_dummy_resources(self, scope, session):\n        api_objs = {}\n        for api_obj in self.api_objects:\n            log.debug3(\"Creating api object: %s\", api_obj)\n\n            # Create api object from tuple or generate one if callable\n            if isinstance(api_obj, tuple):\n                object_name, object_def = api_obj\n\n                object_def = merge_configs(\n                    {\n                        \"apiVersion\": \"v1\",\n                        \"metadata\": {\"name\": object_name, \"namespace\": TEST_NAMESPACE},\n                    },\n                    object_def,\n                )\n            elif isinstance(api_obj, dict):\n                object_def = api_obj\n                object_name = api_obj.get(\"metadata\", {}).get(\"name\")\n            else:\n                object_def = api_obj(self, session)\n                object_name = api_obj.name\n\n            resource_node = self.add_resource(object_name, object_def)\n            if resource_node is not None:\n                api_objs[resource_node.get_name()] = resource_node\n        return api_objs\n\n    def get_rendered_configs(self):\n        configs = []\n        for obj in self.managed_objects:\n            configs.append(aconfig.Config(obj.definition))\n        return configs\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.DummyController","title":"<code>DummyController</code>","text":"<p>               Bases: <code>Controller</code></p> <p>Configurable implementation of a controller that can be used in unit tests to simulate Controller behavior</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>class DummyController(Controller):\n    \"\"\"Configurable implementation of a controller that can be used in unit\n    tests to simulate Controller behavior\n    \"\"\"\n\n    ##################\n    ## Construction ##\n    ##################\n\n    group = \"foo.bar.com\"\n    version = \"v42\"\n    kind = \"Foo\"\n\n    def __init__(\n        self,\n        components=None,\n        after_deploy_fail=False,\n        after_deploy_unsuccessful_fail=False,\n        after_verify_fail=False,\n        after_verify_unsuccessful_fail=False,\n        setup_components_fail=False,\n        finalize_components_fail=False,\n        should_requeue_fail=False,\n        component_type=DummyNodeComponent,\n        **kwargs,\n    ):\n        # Initialize parent\n        super().__init__(**kwargs)\n\n        # Set up objects that this controller will manage directly\n        self.component_specs = components or []\n        self.component_type = component_type\n\n        # Set up mocks\n        self.after_deploy_fail = after_deploy_fail\n        self.after_deploy_unsuccessful_fail = after_deploy_unsuccessful_fail\n        self.after_verify_fail = after_verify_fail\n        self.after_verify_unsuccessful_fail = after_verify_unsuccessful_fail\n        self.setup_components_fail = setup_components_fail\n        self.finalize_components_fail = finalize_components_fail\n        self.should_requeue_fail = should_requeue_fail\n        self.after_deploy = mock.Mock(\n            side_effect=get_failable_method(\n                self.after_deploy_fail, super().after_deploy\n            )\n        )\n        self.after_deploy_unsuccessful = mock.Mock(\n            side_effect=get_failable_method(\n                self.after_deploy_unsuccessful_fail, super().after_deploy_unsuccessful\n            )\n        )\n        self.after_verify = mock.Mock(\n            side_effect=get_failable_method(\n                self.after_verify_fail, super().after_verify\n            )\n        )\n        self.after_verify_unsuccessful = mock.Mock(\n            side_effect=get_failable_method(\n                self.after_verify_unsuccessful_fail, super().after_verify_unsuccessful\n            )\n        )\n        self.setup_components = mock.Mock(\n            side_effect=get_failable_method(\n                self.setup_components_fail, self.setup_components\n            )\n        )\n        self.finalize_components = mock.Mock(\n            side_effect=get_failable_method(\n                self.finalize_components_fail, self.finalize_components\n            )\n        )\n        self.should_requeue = mock.Mock(\n            side_effect=get_failable_method(\n                self.should_requeue_fail, super().should_requeue\n            )\n        )\n\n    ##############################\n    ## Interface Implementation ##\n    ##############################\n\n    def setup_components(self, session: Session):\n        \"\"\"Set up the components based on the component specs passed in\"\"\"\n\n        # Add the components\n        for component in self.component_specs:\n            name = component[\"name\"]\n            log.debug2(\"Adding component %s (kwargs: %s)\", name, component)\n            comp = self._make_dummy_component(\n                session=session,\n                **component,\n            )\n            log.debug2(\"Component name: %s\", comp.name)\n            log.debug2(\n                \"Components in session [%s]: %s\",\n                session.id,\n                [\n                    comp.name\n                    for comp in session.get_components()\n                    + session.get_components(disabled=True)\n                ],\n            )\n\n        # Add the dependencies after the nodes (so that we can look up by name)\n        component_map = {\n            comp.name: comp\n            for comp in session.get_components() + session.get_components(disabled=True)\n        }\n        for component in self.component_specs:\n            comp = component_map[component[\"name\"]]\n            upstreams = component.get(\"upstreams\", [])\n            for upstream in upstreams:\n                session.add_component_dependency(comp, upstream)\n\n        # Hang onto the components so that they can be checked\n        self.components = component_map\n\n    ############################\n    ## Implementation Details ##\n    ############################\n\n    def _make_dummy_component(self, name=\"dummy\", session=None, **kwargs):\n        \"\"\"This helper wraps any DummyComponent class so that the name class\n        attribute is not overwritten by the next instance.\n        \"\"\"\n\n        class WrappedDummyComponent(self.component_type):\n            pass\n\n        WrappedDummyComponent.name = name\n        return WrappedDummyComponent(session=session, **kwargs)\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.DummyController.setup_components","title":"<code>setup_components(session)</code>","text":"<p>Set up the components based on the component specs passed in</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>def setup_components(self, session: Session):\n    \"\"\"Set up the components based on the component specs passed in\"\"\"\n\n    # Add the components\n    for component in self.component_specs:\n        name = component[\"name\"]\n        log.debug2(\"Adding component %s (kwargs: %s)\", name, component)\n        comp = self._make_dummy_component(\n            session=session,\n            **component,\n        )\n        log.debug2(\"Component name: %s\", comp.name)\n        log.debug2(\n            \"Components in session [%s]: %s\",\n            session.id,\n            [\n                comp.name\n                for comp in session.get_components()\n                + session.get_components(disabled=True)\n            ],\n        )\n\n    # Add the dependencies after the nodes (so that we can look up by name)\n    component_map = {\n        comp.name: comp\n        for comp in session.get_components() + session.get_components(disabled=True)\n    }\n    for component in self.component_specs:\n        comp = component_map[component[\"name\"]]\n        upstreams = component.get(\"upstreams\", [])\n        for upstream in upstreams:\n            session.add_component_dependency(comp, upstream)\n\n    # Hang onto the components so that they can be checked\n    self.components = component_map\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.DummyNodeComponent","title":"<code>DummyNodeComponent</code>","text":"<p>               Bases: <code>DummyComponentBase</code></p> <p>Configurable dummy component which will create an arbitrary set of resource node instances.</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>class DummyNodeComponent(DummyComponentBase):\n    \"\"\"\n    Configurable dummy component which will create an arbitrary set of\n    resource node instances.\n    \"\"\"\n\n    def __init__(self, session, *args, **kwargs):\n        \"\"\"Construct with the additional option to fail build_chart\"\"\"\n        super().__init__(*args, session=session, **kwargs)\n        self._add_resources(self, session)\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.DummyNodeComponent.__init__","title":"<code>__init__(session, *args, **kwargs)</code>","text":"<p>Construct with the additional option to fail build_chart</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>def __init__(self, session, *args, **kwargs):\n    \"\"\"Construct with the additional option to fail build_chart\"\"\"\n    super().__init__(*args, session=session, **kwargs)\n    self._add_resources(self, session)\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.FailOnce","title":"<code>FailOnce</code>","text":"<p>Helper callable that will fail once on the N'th call</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>class FailOnce:\n    \"\"\"Helper callable that will fail once on the N'th call\"\"\"\n\n    def __init__(self, fail_val, fail_number=1):\n        self.call_count = 0\n        self.fail_number = fail_number\n        self.fail_val = fail_val\n\n    def __call__(self, *_, **__):\n        self.call_count += 1\n        if self.call_count == self.fail_number:\n            log.debug(\"Failing on call %d with %s\", self.call_count, self.fail_val)\n            if isinstance(self.fail_val, type) and issubclass(self.fail_val, Exception):\n                raise self.fail_val(\"Raising!\")\n            return self.fail_val\n        log.debug(\"Not failing on call %d\", self.call_count)\n        return\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.MockComponent","title":"<code>MockComponent</code>","text":"<p>               Bases: <code>DummyNodeComponent</code></p> <p>Dummy component with a valid mock name</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>class MockComponent(DummyNodeComponent):\n    \"\"\"Dummy component with a valid mock name\"\"\"\n\n    name = \"mock\"\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.MockDeployManager","title":"<code>MockDeployManager</code>","text":"<p>               Bases: <code>DryRunDeployManager</code></p> <p>The MockDeployManager wraps a standard DryRunDeployManager and adds configuration options to simulate failures in each of its operations.</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>class MockDeployManager(DryRunDeployManager):\n    \"\"\"The MockDeployManager wraps a standard DryRunDeployManager and adds\n    configuration options to simulate failures in each of its operations.\n    \"\"\"\n\n    def __init__(\n        self,\n        deploy_fail=False,\n        deploy_raise=False,\n        disable_fail=False,\n        disable_raise=False,\n        get_state_fail=False,\n        get_state_raise=False,\n        watch_fail=False,\n        watch_raise=False,\n        generate_resource_version=True,\n        set_status_fail=False,\n        set_status_raise=False,\n        auto_enable=True,\n        resources=None,\n        resource_dir=None,\n        **kwargs,\n    ):\n        \"\"\"This DeployManager can be configured to have various failure cases\n        and will mock the state of the cluster so that get_object_current_state\n        will pull its information from the local dict.\n        \"\"\"\n\n        # Add apiVersion to resources that are missing it, then initialize the\n        # dry run manager\n\n        resources = resources or []\n        # Parse pre-populated resources if needed\n        resources = resources + (RunOperatorCmd._parse_resource_dir(resource_dir))\n\n        for resource in resources:\n            resource.setdefault(\"apiVersion\", \"v1\")\n        super().__init__(\n            resources, generate_resource_version=generate_resource_version, **kwargs\n        )\n\n        self.watch_fail = \"assert\" if watch_raise else watch_fail\n        self.deploy_fail = \"assert\" if deploy_raise else deploy_fail\n        self.disable_fail = \"assert\" if disable_raise else disable_fail\n        self.get_state_fail = \"assert\" if get_state_raise else get_state_fail\n        self.set_status_fail = \"assert\" if set_status_raise else set_status_fail\n\n        # If auto-enabling, turn the mocks on now\n        if auto_enable:\n            self.enable_mocks()\n\n    #######################\n    ## Helpers for Tests ##\n    #######################\n\n    def enable_mocks(self):\n        \"\"\"Turn the mocks on\"\"\"\n        self.deploy = mock.Mock(\n            side_effect=get_failable_method(\n                self.deploy_fail, super().deploy, (False, False)\n            )\n        )\n        self.disable = mock.Mock(\n            side_effect=get_failable_method(\n                self.disable_fail, super().disable, (False, False)\n            )\n        )\n        self.get_object_current_state = mock.Mock(\n            side_effect=get_failable_method(\n                self.get_state_fail, super().get_object_current_state, (False, None)\n            )\n        )\n        self.set_status = mock.Mock(\n            side_effect=get_failable_method(\n                self.set_status_fail, super().set_status, (False, False)\n            )\n        )\n        self.watch_objects = mock.Mock(\n            side_effect=get_failable_method(self.watch_fail, super().watch_objects, [])\n        )\n\n    def get_obj(self, kind, name, namespace=None, api_version=None):\n        return self.get_object_current_state(kind, name, namespace, api_version)[1]\n\n    def has_obj(self, *args, **kwargs):\n        return self.get_obj(*args, **kwargs) is not None\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.MockDeployManager.__init__","title":"<code>__init__(deploy_fail=False, deploy_raise=False, disable_fail=False, disable_raise=False, get_state_fail=False, get_state_raise=False, watch_fail=False, watch_raise=False, generate_resource_version=True, set_status_fail=False, set_status_raise=False, auto_enable=True, resources=None, resource_dir=None, **kwargs)</code>","text":"<p>This DeployManager can be configured to have various failure cases and will mock the state of the cluster so that get_object_current_state will pull its information from the local dict.</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>def __init__(\n    self,\n    deploy_fail=False,\n    deploy_raise=False,\n    disable_fail=False,\n    disable_raise=False,\n    get_state_fail=False,\n    get_state_raise=False,\n    watch_fail=False,\n    watch_raise=False,\n    generate_resource_version=True,\n    set_status_fail=False,\n    set_status_raise=False,\n    auto_enable=True,\n    resources=None,\n    resource_dir=None,\n    **kwargs,\n):\n    \"\"\"This DeployManager can be configured to have various failure cases\n    and will mock the state of the cluster so that get_object_current_state\n    will pull its information from the local dict.\n    \"\"\"\n\n    # Add apiVersion to resources that are missing it, then initialize the\n    # dry run manager\n\n    resources = resources or []\n    # Parse pre-populated resources if needed\n    resources = resources + (RunOperatorCmd._parse_resource_dir(resource_dir))\n\n    for resource in resources:\n        resource.setdefault(\"apiVersion\", \"v1\")\n    super().__init__(\n        resources, generate_resource_version=generate_resource_version, **kwargs\n    )\n\n    self.watch_fail = \"assert\" if watch_raise else watch_fail\n    self.deploy_fail = \"assert\" if deploy_raise else deploy_fail\n    self.disable_fail = \"assert\" if disable_raise else disable_fail\n    self.get_state_fail = \"assert\" if get_state_raise else get_state_fail\n    self.set_status_fail = \"assert\" if set_status_raise else set_status_fail\n\n    # If auto-enabling, turn the mocks on now\n    if auto_enable:\n        self.enable_mocks()\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.MockDeployManager.enable_mocks","title":"<code>enable_mocks()</code>","text":"<p>Turn the mocks on</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>def enable_mocks(self):\n    \"\"\"Turn the mocks on\"\"\"\n    self.deploy = mock.Mock(\n        side_effect=get_failable_method(\n            self.deploy_fail, super().deploy, (False, False)\n        )\n    )\n    self.disable = mock.Mock(\n        side_effect=get_failable_method(\n            self.disable_fail, super().disable, (False, False)\n        )\n    )\n    self.get_object_current_state = mock.Mock(\n        side_effect=get_failable_method(\n            self.get_state_fail, super().get_object_current_state, (False, None)\n        )\n    )\n    self.set_status = mock.Mock(\n        side_effect=get_failable_method(\n            self.set_status_fail, super().set_status, (False, False)\n        )\n    )\n    self.watch_objects = mock.Mock(\n        side_effect=get_failable_method(self.watch_fail, super().watch_objects, [])\n    )\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.MockTopApp","title":"<code>MockTopApp</code>","text":"<p>               Bases: <code>Controller</code></p> <p>Mock implementation of a top-level Controller to allow subsystems to be tested as \"children\"</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>@controller(\n    group=\"unit.test.com\",\n    version=\"v42\",\n    kind=\"MockTopApp\",\n)\nclass MockTopApp(Controller):\n    \"\"\"Mock implementation of a top-level Controller to allow subsystems to be\n    tested as \"children\"\n    \"\"\"\n\n    def __init__(self, config_defaults=None, component_types=None):\n        super().__init__(config_defaults=config_defaults)\n        self.component_types = component_types or []\n\n    def setup_components(self, session: Session):\n        for component_type in self.component_types:\n            component_type(session=session)\n\n    def do_rollout(self, session):\n        try:\n            return self.run_reconcile(session)\n        except Exception as err:\n            log.debug(\"Caught error in rollout: %s\", err, exc_info=True)\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.MockedOpenshiftDeployManager","title":"<code>MockedOpenshiftDeployManager</code>","text":"<p>               Bases: <code>OpenshiftDeployManager</code></p> <p>Override class that uses the mocked client</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>class MockedOpenshiftDeployManager(OpenshiftDeployManager):\n    \"\"\"Override class that uses the mocked client\"\"\"\n\n    def __init__(self, manage_ansible_status=False, owner_cr=None, *args, **kwargs):\n        self._mock_args = args\n        self._mock_kwargs = kwargs\n        super().__init__(manage_ansible_status, owner_cr)\n\n    def _setup_client(self):\n        mock_client = MockKubClient(*self._mock_args, **self._mock_kwargs)\n        return DynamicClient(mock_client)\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.ModuleExit","title":"<code>ModuleExit</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception we'll use to break out when sys.exit was called</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>class ModuleExit(Exception):\n    \"\"\"Exception we'll use to break out when sys.exit was called\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.deep_merge","title":"<code>deep_merge(a, b)</code>","text":"<p>NOTE: This should really be eliminated in favor of just using merge_configs</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>def deep_merge(a, b):\n    \"\"\"NOTE: This should really be eliminated in favor of just using\n    merge_configs\n    \"\"\"\n    return merge_configs(a, b)\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.default_branch_name","title":"<code>default_branch_name()</code>  <code>cached</code>","text":"<p>Helper to get the current git context's default branch name</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>@lru_cache(maxsize=1)\ndef default_branch_name() -&gt; str:\n    \"\"\"Helper to get the current git context's default branch name\"\"\"\n    try:\n        return (\n            subprocess.run(\n                [\"git\", \"config\", \"--get\", \"init.defaultBranch\"],\n                check=True,\n                stdout=subprocess.PIPE,\n            )\n            .stdout.decode(\"utf-8\")\n            .strip()\n        )\n    except subprocess.CalledProcessError:\n        return \"master\"\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.library_config","title":"<code>library_config(**config_overrides)</code>","text":"<p>This context manager sets library config values temporarily and reverts them on completion</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>@contextmanager\ndef library_config(**config_overrides):\n    \"\"\"This context manager sets library config values temporarily and reverts\n    them on completion\n    \"\"\"\n    # Override the configs and hang onto the old values\n    old_vals = {}\n    for key, val in config_overrides.items():\n        if key in config_detail_dict:\n            old_vals[key] = config_detail_dict[key]\n        config_detail_dict[key] = val\n\n    # Yield to the context\n    yield\n\n    # Revert to the old values\n    for key in config_overrides:\n        if key in old_vals:\n            config_detail_dict[key] = old_vals[key]\n        else:\n            del config_detail_dict[key]\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.make_patch","title":"<code>make_patch(patch_type, body, name='test', target=None, namespace=TEST_NAMESPACE, api_version='org.oper8/v1', kind='TemporaryPatch')</code>","text":"<p>Make a sample TemporaryPatch resource body</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>def make_patch(\n    patch_type,\n    body,\n    name=\"test\",\n    target=None,\n    namespace=TEST_NAMESPACE,\n    api_version=\"org.oper8/v1\",\n    kind=\"TemporaryPatch\",\n):\n    \"\"\"Make a sample TemporaryPatch resource body\"\"\"\n    target = target or {}\n    patch_obj = {\n        \"apiVersion\": api_version,\n        \"kind\": kind,\n        \"metadata\": {\"name\": name},\n        \"spec\": {\n            \"apiVersion\": target.get(\"apiVersion\", \"fake\"),\n            \"kind\": target.get(\"kind\", \"fake\"),\n            \"name\": target.get(\"metadata\", {}).get(\"name\", \"fake\"),\n            \"patchType\": patch_type,\n            \"patch\": body,\n        },\n    }\n    if namespace is not None:\n        patch_obj[\"metadata\"][\"namespace\"] = namespace\n    return aconfig.Config(\n        patch_obj,\n        override_env_vars=False,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.mock_config_file","title":"<code>mock_config_file(config_object)</code>","text":"<p>Yuck! Ansible makes it tough to actually inject parameters in since it expects that modules will only be run by its parent runner.</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>@contextmanager\ndef mock_config_file(config_object):\n    \"\"\"Yuck! Ansible makes it tough to actually inject parameters in since it\n    expects that modules will only be run by its parent runner.\n    \"\"\"\n    # Third Party\n    import ansible.module_utils.basic\n\n    ansible.module_utils.basic._ANSIBLE_ARGS = json.dumps(config_object).encode(\"utf-8\")\n    yield\n    ansible.module_utils.basic._ANSIBLE_ARGS = None\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.helpers.setup_session_ctx","title":"<code>setup_session_ctx(*args, **kwargs)</code>","text":"<p>Context manager wrapper around setup_session. This simplifies the porting process from WA and really provides no functional benefit.</p> Source code in <code>oper8/test_helpers/helpers.py</code> <pre><code>@contextmanager\ndef setup_session_ctx(*args, **kwargs):\n    \"\"\"Context manager wrapper around setup_session. This simplifies the porting\n    process from WA and really provides no functional benefit.\n    \"\"\"\n    yield setup_session(*args, **kwargs)\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.kub_mock","title":"<code>kub_mock</code>","text":"<p>This module implements a mock of the kubernetes client library which can be used to patch the api client in an ansible module.</p> <p>We attempt to emulate the internals of the kubernetes api_client, but this is based on code inspection of the current implementation and is certainly subject to change!</p>"},{"location":"API%20References/#oper8.test_helpers.kub_mock.MockKubClient","title":"<code>MockKubClient</code>","text":"<p>               Bases: <code>ApiClient</code></p> <p>Mocked version of kubernetes.client.ApiClient which swaps out the implementation of call_api() to use preconfigured responses</p> Source code in <code>oper8/test_helpers/kub_mock.py</code> <pre><code>class MockKubClient(kubernetes.client.ApiClient):\n    \"\"\"Mocked version of kubernetes.client.ApiClient which swaps out the\n    implementation of call_api() to use preconfigured responses\n    \"\"\"\n\n    def __init__(self, cluster_state=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # Save the current cluster state\n        self.__cluster_state = cluster_state or {}\n        log.debug3(\"Cluster State: %s\", self.__cluster_state)\n\n        # Setup listener variables\n        self.__queue_lock = RLock()\n        self.__watch_queues = set()\n\n        # Save a list of kinds for each api group\n        self._api_group_kinds = {}\n\n        # Canned handlers\n        self._handlers = {\n            \"/apis\": {\"GET\": self.apis},\n            \"/version\": {\"GET\": lambda *_, **__: self._make_response({})},\n            \"/api/v1\": {\"GET\": self.api_v1},\n        }\n        for namespace, ns_entries in self.__cluster_state.items():\n            for kind, kind_entries in ns_entries.items():\n                for api_version, version_entries in kind_entries.items():\n                    # Add handlers common to this kind\n                    self._add_handlers_for_kind(namespace, kind, api_version)\n\n                    # Add handlers for the individual pre-existing instances\n                    for name, obj_state in version_entries.items():\n                        # If this is a static object state, make sure the\n                        # metadata aligns correctly\n                        if isinstance(obj_state, dict):\n                            self._add_resource_defaults(\n                                obj_state=obj_state,\n                                namespace=namespace,\n                                kind=kind,\n                                api_version=api_version,\n                                name=name,\n                            )\n\n                        # Add the endpoints for this resource\n                        self._add_handlers_for_resource(\n                            namespace=namespace,\n                            kind=kind,\n                            api_version=api_version,\n                            name=name,\n                        )\n\n        log.debug(\"Configured Handlers: %s\", list(self._handlers.keys()))\n\n    def call_api(\n        self,\n        resource_path,\n        method,\n        path_params=None,\n        query_params=None,\n        header_params=None,\n        body=None,\n        **kwargs,\n    ):\n        \"\"\"Mocked out call function to return preconfigured responses\n\n        NOTE: this is set up to work with how openshift.dynamic.DynamicClient\n            calls self.client.call_api. It (currently) passes some as positional\n            args and some as kwargs.\n        \"\"\"\n        for key, value in query_params:\n            if key == \"watch\" and value:\n                method = \"WATCH\"\n                break\n\n        log.debug2(\"Mock [%s] request to [%s]\", method, resource_path)\n        log.debug4(\"Path Params: %s\", path_params)\n        log.debug4(\"Query Params: %s\", query_params)\n        log.debug4(\"Header Params: %s\", header_params)\n        log.debug4(\"Body: %s\", body)\n\n        # Find the right handler and execute it\n        return self._get_handler(resource_path, method)(\n            resource_path=resource_path,\n            path_params=path_params,\n            query_params=query_params,\n            header_params=header_params,\n            body=body,\n            **kwargs,\n        )\n\n    ## Implementation Helpers ##################################################\n\n    @staticmethod\n    def _make_response(body, status_code=200):\n        log.debug2(\"Making response with code: %d\", status_code)\n        log.debug4(body)\n        resp = MockKubRestResponse(\n            aconfig.Config(\n                {\n                    \"status\": status_code,\n                    \"reason\": \"MOCK\",\n                    \"data\": json.dumps(body).encode(\"utf8\"),\n                }\n            )\n        )\n        if not 200 &lt;= status_code &lt;= 299:\n            raise kubernetes.client.rest.ApiException(http_resp=resp)\n        return resp\n\n    def _get_handler(self, resource_path, method):\n        # Look for a configured handler that matches the path exactly\n        handler = self._handlers.get(resource_path, {}).get(method)\n\n        # If no handler found, start looking for '*' handlers\n        path_parts = resource_path.split(\"/\")\n        while handler is None and path_parts:\n            star_path = \"/\".join(path_parts + [\"*\"])\n            log.debug4(\"Looking for [%s]\", star_path)\n            handler = self._handlers.get(star_path, {}).get(method)\n            path_parts.pop()\n\n        # Return whatever we've found or not_found\n        return handler or self.not_found\n\n    @staticmethod\n    def _get_kind_variants(kind):\n        return [kind.lower(), f\"{kind.lower()}s\", kind]\n\n    @staticmethod\n    def _get_version_parts(api_version):\n        parts = api_version.split(\"/\", 1)\n        if len(parts) == 2:\n            group_name, version = parts\n            api_endpoint = f\"/apis/{group_name}/{version}\"\n        else:\n            group_name = None\n            version = api_version\n            api_endpoint = f\"/api/{api_version}\"\n        return group_name, version, api_endpoint\n\n    def _add_handlers_for_kind(self, namespace, kind, api_version):\n        # Set up the information needed for the apis and crds calls\n        group_name, version, api_endpoint = self._get_version_parts(api_version)\n        self._api_group_kinds.setdefault(group_name, {}).setdefault(version, []).append(\n            kind\n        )\n\n        # Add a configured handler for this group type\n        log.debug(\"Adding resource handler for: %s\", api_endpoint)\n        self._handlers[api_endpoint] = {\n            \"GET\": lambda *_, **__: (self.current_state_crds(group_name, version))\n        }\n\n        # Add POST handlers for this type\n        for kind_variant in self._get_kind_variants(kind):\n            # Add different endpoints based on namespace\n            if namespace:\n                endpoint = f\"{api_endpoint}/namespaces/{namespace}/{kind_variant}/*\"\n            else:\n                endpoint = f\"{api_endpoint}/{kind_variant}/*\"\n\n            log.debug2(\n                \"Adding POST &amp; PUT &amp; GET &amp; WATCH &amp; PATCH handler for (%s: %s)\",\n                kind,\n                endpoint,\n            )\n            self._handlers[endpoint] = {\n                \"WATCH\": lambda resource_path, body, *_, **__: (\n                    self.current_state_watch(\n                        resource_path, api_version, kind, resourced=False\n                    )\n                ),\n                \"PATCH\": lambda resource_path, body, *_, **__: (\n                    self.current_state_patch(resource_path, api_version, kind, body)\n                ),\n                \"PUT\": lambda resource_path, body, *_, **__: (\n                    self.current_state_patch(resource_path, api_version, kind, body)\n                ),\n                \"POST\": lambda resource_path, body, *_, **__: (\n                    self.current_state_post(resource_path, api_version, kind, body)\n                ),\n                \"GET\": lambda resource_path, *_, **__: (\n                    self.current_state_list(resource_path, api_version, kind)\n                ),\n            }\n\n    def _remove_handlers_for_resource(self, namespace, kind, api_version, name):\n        # Get crucial API information out of the object\n        _, __, api_endpoint = self._get_version_parts(api_version)\n        for kind_variant in self._get_kind_variants(kind):\n            if namespace:\n                resource_api_endpoint = (\n                    f\"{api_endpoint}/namespaces/{namespace}/{kind_variant}/{name}\"\n                )\n            else:\n                resource_api_endpoint = f\"{api_endpoint}/{kind_variant}/{name}\"\n\n            status_resource_api_endpoint = \"/\".join([resource_api_endpoint, \"status\"])\n            del self._handlers[resource_api_endpoint]\n            del self._handlers[status_resource_api_endpoint]\n\n    def _add_handlers_for_resource(self, namespace, kind, api_version, name):\n        # Get crucial API information out of the object\n        _, __, api_endpoint = self._get_version_parts(api_version)\n\n        # Add configured handlers for GET/PUT on this resource\n        for kind_variant in self._get_kind_variants(kind):\n            # The endpoint that will be used to hit this specific\n            # resource\n            if namespace:\n                resource_api_endpoint = (\n                    f\"{api_endpoint}/namespaces/{namespace}/{kind_variant}/{name}\"\n                )\n            else:\n                resource_api_endpoint = f\"{api_endpoint}/{kind_variant}/{name}\"\n\n            # Add the handlers\n            log.debug(\"Adding GET handler for: %s\", resource_api_endpoint)\n            self._handlers[resource_api_endpoint] = {\n                \"GET\": lambda *_, x=resource_api_endpoint, **__: (\n                    self.current_state_get(x, api_version, kind)\n                ),\n                \"PUT\": lambda body, *_, x=resource_api_endpoint, **__: (\n                    self.current_state_put(x, api_version, kind, body)\n                ),\n                \"PATCH\": lambda body, *_, x=resource_api_endpoint, **__: (\n                    self.current_state_patch(x, api_version, kind, body)\n                ),\n                \"DELETE\": lambda *_, x=resource_api_endpoint, **__: (\n                    self.current_state_delete(x, api_version, kind)\n                ),\n                \"WATCH\": lambda resource_path, body, *_, **__: (\n                    self.current_state_watch(\n                        resource_path, api_version, kind, resourced=True\n                    )\n                ),\n            }\n\n            # Add status PUT\n            status_resource_api_endpoint = \"/\".join([resource_api_endpoint, \"status\"])\n            self._handlers[status_resource_api_endpoint] = {\n                \"PUT\": lambda body, *_, x=status_resource_api_endpoint, **__: (\n                    self.current_state_put(\n                        x,\n                        api_version,\n                        kind,\n                        body,\n                        is_status=True,\n                    )\n                )\n            }\n\n    def _get_object_state(self, method, namespace, kind, api_version, name):\n        create = method in [\"PUT\", \"PATCH\", \"POST\"]\n        if create:\n            content = (\n                self.__cluster_state.setdefault(namespace, {})\n                .setdefault(kind, {})\n                .setdefault(api_version, {})\n                .get(name, {})\n            )\n        else:\n            content = (\n                self.__cluster_state.get(namespace, {})\n                .get(kind, {})\n                .get(api_version, {})\n                .get(name)\n            )\n\n        # If it's a callable, call it!\n        if callable(content):\n            log.debug2(\"Making callable resource content\")\n            content = content(\n                method=method,\n                namespace=namespace,\n                kind=kind,\n                api_version=api_version,\n                name=name,\n            )\n\n            # Add the defaults and handle the case where it's a tuple with a\n            # status code\n            if isinstance(content, tuple):\n                body, status = content\n                log.debug3(\"Handling tuple content with status [%s]\", status)\n                self._add_resource_defaults(body, namespace, kind, api_version, name)\n                content = (body, status)\n            else:\n                self._add_resource_defaults(content, namespace, kind, api_version, name)\n            log.debug3(\"Content: %s\", content)\n        return content\n\n    def _list_object_state(self, method, namespace, kind, api_version):\n        if method != \"GET\":\n            return ([], 405)\n\n        content = (\n            self.__cluster_state.setdefault(namespace, {})\n            .setdefault(kind, {})\n            .setdefault(api_version, {})\n        )\n\n        resource_list = []\n        return_status = 200\n        for resource_name in content:\n            resource = content[resource_name]\n\n            # If it's a callable, call it!\n            if callable(resource):\n                log.debug2(\"Making callable resource content\")\n                resource = resource(\n                    method=method,\n                    namespace=namespace,\n                    kind=kind,\n                    api_version=api_version,\n                    name=resource_name,\n                )\n\n            # Add the defaults and handle the case where it's a tuple with a\n            # status code\n            if isinstance(resource, tuple):\n                resource, status = resource\n                if status == 403:\n                    return_status = 403\n                    break\n                self._add_resource_defaults(\n                    resource, namespace, kind, api_version, resource_name\n                )\n            else:\n                self._add_resource_defaults(\n                    resource, namespace, kind, api_version, resource_name\n                )\n\n            log.debug3(\"Resource: %s\", content)\n            resource_list.append(resource)\n\n        content = {\"apiVersion\": \"v1\", \"kind\": \"List\", \"items\": resource_list}\n        return (content, return_status)\n\n    def _update_object_current_state(self, namespace, kind, api_version, name, state):\n        \"\"\"Helper function to update a resource in the cluster and update all watch queues\"\"\"\n        self.__cluster_state.setdefault(namespace, {}).setdefault(kind, {}).setdefault(\n            api_version, {}\n        )\n\n        # Get the event type based on if name already exists\n        event_type = KubeEventType.ADDED\n        if name in self.__cluster_state[namespace][kind][api_version]:\n            event_type = KubeEventType.MODIFIED\n\n        # Update the cluster state and add resource_path if it doesn't already exist\n        self.__cluster_state[namespace][kind][api_version][name] = state\n        self._add_handlers_for_resource(\n            namespace=namespace, kind=kind, api_version=api_version, name=name\n        )\n        self._update_watch_queues(event_type, state)\n\n    def _delete_object_state(self, namespace, kind, api_version, name):\n        \"\"\"Helper function to delete a resource in the cluster and update all watch queues\"\"\"\n\n        original_object = self.__cluster_state[namespace][kind][api_version][name]\n\n        del self.__cluster_state[namespace][kind][api_version][name]\n        if not self.__cluster_state[namespace][kind][api_version]:\n            del self.__cluster_state[namespace][kind][api_version]\n        if not self.__cluster_state[namespace][kind]:\n            del self.__cluster_state[namespace][kind]\n        if not self.__cluster_state[namespace]:\n            del self.__cluster_state[namespace]\n\n        self._update_watch_queues(KubeEventType.DELETED, original_object)\n\n        # Remove any endpoint handlers for this resource\n        self._remove_handlers_for_resource(namespace, kind, api_version, name)\n\n        return True\n\n    def _add_watch_queue(self, queue):\n        with self.__queue_lock:\n            log.debug3(\"Adding watch queue %s\", queue)\n            self.__watch_queues.add(queue)\n\n    def _remove_watch_queue(self, queue):\n        with self.__queue_lock:\n            log.debug3(\"Removing watch queue %s\", queue)\n            self.__watch_queues.remove(queue)\n\n    def _update_watch_queues(self, event, object):\n        with self.__queue_lock:\n            log.debug2(\"Updating watch queues with %s event\", event)\n            for queue in self.__watch_queues:\n                queue.put((event, object))\n\n    @staticmethod\n    def _add_resource_defaults(obj_state, namespace, kind, api_version, name):\n        obj_state[\"apiVersion\"] = api_version\n        obj_state[\"kind\"] = kind\n        md = obj_state.setdefault(\"metadata\", {})\n        if namespace:\n            md[\"namespace\"] = namespace\n        md[\"name\"] = name\n        last_applied_annotation = annotate_last_applied(obj_state)\n        md.setdefault(\"annotations\", {}).update(\n            last_applied_annotation[\"metadata\"][\"annotations\"]\n        )\n\n    @classmethod\n    def _patch_resource(cls, base, overrides):\n        \"\"\"Merge helper that supports removing elements when the override is set\n        to None\n        \"\"\"\n        for key, value in overrides.items():\n            if value is None and key in base:\n                del base[key]\n\n            elif (\n                key not in base\n                or not isinstance(base[key], dict)\n                or not isinstance(value, dict)\n            ):\n                base[key] = value\n            else:\n                base[key] = cls._patch_resource(base[key], value)\n\n        return base\n\n    ## Handlers ################################################################\n\n    @classmethod\n    def not_found(cls, *_, **__):\n        log.debug3(\"Not Found\")\n        return cls._make_response(\n            {\n                \"kind\": \"Status\",\n                \"apiVersion\": \"v1\",\n                \"metadata\": {},\n                \"status\": \"Failure\",\n                \"message\": \"the server could not find the requested resource\",\n                \"reason\": \"NotFound\",\n                \"details\": {},\n                \"code\": 404,\n            },\n            404,\n        )\n\n    def apis(self, *_, **__):\n        api_group_list = {\"kind\": \"APIGroupList\", \"apiVersion\": \"v1\", \"groups\": []}\n        for group_name, api_versions in self._api_group_kinds.items():\n            if group_name is None:\n                continue\n            group = {\"name\": group_name, \"versions\": []}\n            for api_version in api_versions:\n                group[\"versions\"].append(\n                    {\n                        \"groupVersion\": f\"{group_name}/{api_version}\",\n                        \"version\": api_version,\n                    }\n                )\n            group[\"preferredVersion\"] = group[\"versions\"][0]\n            api_group_list[\"groups\"].append(group)\n        return self._make_response(api_group_list)\n\n    def api_v1(self, *_, **__):\n        return self.current_state_crds(None, \"v1\")\n\n    def current_state_crds(self, group_name, api_version):\n        resource_list = {\n            \"kind\": \"APIResourceList\",\n            \"apiVersion\": \"v1\",\n            \"groupVersion\": f\"{group_name}/{api_version}\",\n            \"resources\": [],\n        }\n        for kind in self._api_group_kinds.get(group_name, {}).get(api_version, []):\n            resource_list[\"resources\"].append(\n                {\n                    \"name\": f\"{kind.lower()}s\",\n                    \"singularName\": kind.lower(),\n                    \"namespaced\": True,\n                    \"kind\": kind,\n                    \"verbs\": [\n                        \"delete\",\n                        \"deletecollection\",\n                        \"get\",\n                        \"list\",\n                        \"patch\",\n                        \"create\",\n                        \"update\",\n                        \"watch\",\n                    ],\n                    \"storageVersionHash\": base64.b64encode(kind.encode(\"utf-8\")).decode(\n                        \"utf-8\"\n                    ),\n                }\n            )\n            resource_list[\"resources\"].append(\n                {\n                    \"name\": f\"{kind.lower()}s/status\",\n                    \"singularName\": \"\",\n                    \"namespaced\": True,\n                    \"kind\": kind,\n                    \"verbs\": [\"get\", \"patch\", \"update\"],\n                }\n            )\n        return self._make_response(resource_list)\n\n    def current_state_watch(\n        self, api_endpoint, api_version, kind, resourced=False, query_params=None\n    ):\n        # Parse the endpoint for the namespace and name\n        endpoint_parts = api_endpoint.split(\"/\")\n        namespace = None\n        name = None\n        if \"namespaces\" in endpoint_parts:\n            namespace = endpoint_parts[endpoint_parts.index(\"namespaces\") + 1]\n\n        if resourced:\n            name = endpoint_parts[-1]\n\n        # Return Watch Stream Response\n        return MockWatchStreamResponse(\n            api_client=self,\n            api_version=api_version,\n            kind=kind,\n            namespace=namespace,\n            name=name,\n            timeout=(query_params or {}).get(\"timeoutSeconds\"),\n        )\n\n    def current_state_get(self, api_endpoint, api_version, kind):\n        # Parse the endpoint for the namespace and name\n        endpoint_parts = api_endpoint.split(\"/\")\n        namespace = \"\"\n        if \"namespaces\" in endpoint_parts:\n            namespace = endpoint_parts[endpoint_parts.index(\"namespaces\") + 1]\n        name = endpoint_parts[-1]\n\n        # Look up the resources in the cluster state\n        log.debug2(\n            \"Looking for current state of [%s/%s/%s/%s]\",\n            namespace,\n            kind,\n            api_version,\n            name,\n        )\n        content = self._get_object_state(\n            method=\"GET\",\n            namespace=namespace,\n            kind=kind,\n            api_version=api_version,\n            name=name,\n        )\n        log.debug4(\"Content: %s\", content)\n        if content is not None:\n            # If the content includes a status code, make the response with it\n            if isinstance(content, tuple):\n                return self._make_response(*content)\n            return self._make_response(content)\n        return self.not_found()\n\n    def current_state_list(self, api_endpoint, api_version, kind):\n        # Parse the endpoint for the namespace and name and where the kind is located\n        # in endpoint_parts\n        endpoint_parts = api_endpoint.split(\"/\")\n        namespace = \"\"\n        kind_loc = 1\n        if \"namespaces\" in endpoint_parts:\n            kind_loc = endpoint_parts.index(\"namespaces\") + 2\n            namespace = endpoint_parts[kind_loc - 1]\n        else:\n            version_split = api_version.split(\"/\")\n            # 2 for [\"\",\"api\"] and then add length of api_version split which would be\n            # 2 for resources with a group and 1 without e.g. v1 = 1 and foo.bar.com/v1 would be 2\n            kind_loc = 2 + len(version_split)\n\n        # If Api was trying to get a specific resource and not list then return 404\n        # as object must not have been found. This is checked by seeing if the kind\n        # is at the end of the endpoint_parts\n        if kind_loc != len(endpoint_parts) - 1:\n            return self.not_found()\n\n        # Look up the resources in the cluster state\n        log.debug2(\n            \"Listing current state of [%s/%s/%s]\",\n            namespace,\n            kind,\n            api_version,\n        )\n        content = self._list_object_state(\n            method=\"GET\",\n            namespace=namespace,\n            kind=kind,\n            api_version=api_version,\n        )\n        log.debug4(\"Content: %s\", content)\n        if content is not None:\n            # If the content includes a status code, make the response with it\n            if isinstance(content, tuple):\n                return self._make_response(*content)\n            return self._make_response(content)\n        return self.not_found()\n\n    def current_state_put(self, api_endpoint, api_version, kind, body, is_status=False):\n        # Parse the endpoint for the namespace and name\n        endpoint_parts = api_endpoint.split(\"/\")\n        namespace = \"\"\n        if \"namespaces\" in endpoint_parts:\n            namespace = endpoint_parts[endpoint_parts.index(\"namespaces\") + 1]\n        name = endpoint_parts[-1] if not is_status else endpoint_parts[-2]\n\n        # Look up the resources in the cluster state\n        log.debug2(\n            \"Looking for current state of [%s/%s/%s/%s]\",\n            namespace,\n            kind,\n            api_version,\n            name,\n        )\n        content = self._get_object_state(\n            method=\"PUT\",\n            namespace=namespace,\n            kind=kind,\n            api_version=api_version,\n            name=name,\n        )\n        log.debug3(\"Current Content: %s\", content)\n\n        # If the content has a status code, unpack it\n        status_code = 200\n        if isinstance(content, tuple):\n            content, status_code = content\n\n        # If it's a non-200 status code, don't make the update\n        if status_code != 200:\n            return self._make_response(content, status_code)\n\n        # If this is a status, we are only updating the status and keeping the\n        # existing content\n        if is_status:\n            content.update({\"status\": body.get(\"status\", {})})\n            updated_content = content\n        else:\n            if \"status\" in body:\n                del body[\"status\"]\n            updated_content = body\n        log.debug3(\n            \"Updating [%s/%s/%s/%s] with body: %s\",\n            namespace,\n            kind,\n            api_version,\n            name,\n            updated_content,\n        )\n        self._update_object_current_state(\n            namespace, kind, api_version, name, updated_content\n        )\n\n        return self._make_response(updated_content, status_code)\n\n    def current_state_patch(self, api_endpoint, api_version, kind, body):\n        # Parse the endpoint for the namespace and name\n        endpoint_parts = api_endpoint.split(\"/\")\n        namespace = \"\"\n        if \"namespaces\" in endpoint_parts:\n            namespace = endpoint_parts[endpoint_parts.index(\"namespaces\") + 1]\n        name = endpoint_parts[-1]\n\n        # Look up the resources in the cluster state\n        log.debug2(\n            \"Looking for current state of [%s/%s/%s/%s]\",\n            namespace,\n            kind,\n            api_version,\n            name,\n        )\n        content = self._get_object_state(\n            method=\"PATCH\",\n            namespace=namespace,\n            kind=kind,\n            api_version=api_version,\n            name=name,\n        )\n        log.debug3(\"Current Content: %s\", content)\n        log.debug3(\"Update body: %s\", body)\n\n        # If the content has a status code, unpack it\n        status_code = 200\n        if isinstance(content, tuple):\n            content, status_code = content\n\n        # If it's a non-200 status code, don't make the update\n        if status_code != 200:\n            return self._make_response(content, status_code)\n\n        # Merge in the new body\n        if \"status\" in body:\n            del body[\"status\"]\n        log.debug3(\n            \"Updating [%s/%s/%s/%s] with body: %s\",\n            namespace,\n            kind,\n            api_version,\n            name,\n            body,\n        )\n        updated_content = self._patch_resource(content, body)\n        log.debug3(\"Updated content: %s\", updated_content)\n        self._update_object_current_state(\n            namespace, kind, api_version, name, updated_content\n        )\n\n        return self._make_response(updated_content, status_code)\n\n    def current_state_post(self, api_endpoint, api_version, kind, body):\n        log.debug2(\"Creating current state for [%s]\", api_endpoint)\n\n        # Parse the endpoint and body for the namespace and name\n        endpoint_parts = api_endpoint.split(\"/\")\n        namespace = \"\"\n        if \"namespaces\" in endpoint_parts:\n            namespace = endpoint_parts[endpoint_parts.index(\"namespaces\") + 1]\n        name = body.get(\"metadata\", {}).get(\"name\")\n\n        # Look up the resources in the cluster state\n        log.debug2(\n            \"Looking for current state of [%s/%s/%s/%s]\",\n            namespace,\n            kind,\n            api_version,\n            name,\n        )\n        content = self._get_object_state(\n            method=\"POST\",\n            namespace=namespace,\n            kind=kind,\n            api_version=api_version,\n            name=name,\n        )\n        log.debug3(\"Current Content: %s\", content)\n\n        # If the content has a status code, unpack it\n        status_code = 200\n        if isinstance(content, tuple):\n            content, status_code = content\n\n        # If it's a non-200 status code, don't make the update\n        if status_code != 200:\n            return self._make_response(content, status_code)\n\n        # Overwrite the body\n        log.debug3(\"Overwrite content: %s\", body)\n        self._update_object_current_state(namespace, kind, api_version, name, body)\n\n        return self._make_response(body, status_code)\n\n    def current_state_delete(self, api_endpoint, api_version, kind):\n        # Parse the endpoint for the namespace and name\n        endpoint_parts = api_endpoint.split(\"/\")\n        namespace = \"\"\n        if \"namespaces\" in endpoint_parts:\n            namespace = endpoint_parts[endpoint_parts.index(\"namespaces\") + 1]\n        name = endpoint_parts[-1]\n\n        # Look up the resources in the cluster state\n        log.debug2(\n            \"Looking for current state of [%s/%s/%s/%s]\",\n            namespace,\n            kind,\n            api_version,\n            name,\n        )\n\n        content = self._get_object_state(\"DELETE\", namespace, kind, api_version, name)\n        response_code = 200\n        if isinstance(content, tuple):\n            content, response_code = content\n        deleted = content is not None\n\n        if content is not None:\n            self._delete_object_state(\n                namespace=namespace,\n                kind=kind,\n                api_version=api_version,\n                name=name,\n            )\n        response_content = {\n            \"kind\": \"status\",\n            \"apiVersion\": \"v1\",\n            \"metadata\": {},\n            \"details\": {\n                \"name\": name,\n                \"kind\": kind,\n            },\n        }\n\n        if deleted:\n            response_content[\"status\"] = \"Success\"\n            response_content[\"details\"][\"uid\"] = \"Hope nothing uses this\"\n        else:\n            response_content[\"status\"] = \"Failure\"\n            response_content[\"message\"] = f'{kind} \"{name}\" not found'\n            response_code = 404\n        return self._make_response(response_content, response_code)\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.kub_mock.MockKubClient.call_api","title":"<code>call_api(resource_path, method, path_params=None, query_params=None, header_params=None, body=None, **kwargs)</code>","text":"<p>Mocked out call function to return preconfigured responses</p> this is set up to work with how openshift.dynamic.DynamicClient <p>calls self.client.call_api. It (currently) passes some as positional args and some as kwargs.</p> Source code in <code>oper8/test_helpers/kub_mock.py</code> <pre><code>def call_api(\n    self,\n    resource_path,\n    method,\n    path_params=None,\n    query_params=None,\n    header_params=None,\n    body=None,\n    **kwargs,\n):\n    \"\"\"Mocked out call function to return preconfigured responses\n\n    NOTE: this is set up to work with how openshift.dynamic.DynamicClient\n        calls self.client.call_api. It (currently) passes some as positional\n        args and some as kwargs.\n    \"\"\"\n    for key, value in query_params:\n        if key == \"watch\" and value:\n            method = \"WATCH\"\n            break\n\n    log.debug2(\"Mock [%s] request to [%s]\", method, resource_path)\n    log.debug4(\"Path Params: %s\", path_params)\n    log.debug4(\"Query Params: %s\", query_params)\n    log.debug4(\"Header Params: %s\", header_params)\n    log.debug4(\"Body: %s\", body)\n\n    # Find the right handler and execute it\n    return self._get_handler(resource_path, method)(\n        resource_path=resource_path,\n        path_params=path_params,\n        query_params=query_params,\n        header_params=header_params,\n        body=body,\n        **kwargs,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.kub_mock.MockWatchStreamResponse","title":"<code>MockWatchStreamResponse</code>","text":"<p>Helper class used to stream resources from the MockKubeClient using a queue. When the streaming method is called the MockWatchStreamResponse registers a threading queue with the MockKubClient and yields all events</p> Source code in <code>oper8/test_helpers/kub_mock.py</code> <pre><code>class MockWatchStreamResponse:\n    \"\"\"Helper class used to stream resources from the MockKubeClient using a queue.\n    When the streaming method is called the MockWatchStreamResponse registers a\n    threading queue with the MockKubClient and yields all events\"\"\"\n\n    def __init__(\n        self,\n        api_client: \"MockKubClient\",\n        api_version: str,\n        kind: str,\n        namespace: Optional[str] = None,\n        name: Optional[str] = None,\n        timeout: Optional[int] = None,\n    ):\n        self.api_client = api_client\n        self.watch_queue = Queue()\n        self.timeout = timeout or 250\n\n        self.kind = kind\n        self.api_version = api_version\n        self.namespace = namespace\n        self.name = name\n\n        # Shutdown flag\n        self.shutdown = Event()\n\n    def __del__(self):\n        self.api_client._remove_watch_queue(self.watch_queue)\n\n    def stream(self, *args, **kwargs):\n        \"\"\"Continuously yield events from the cluster until the shutdown or timeout\"\"\"\n\n        # Get the current resource state\n        current_resources = []\n        if self.name:\n            current_obj = self.api_client._get_object_state(\n                method=\"GET\",\n                namespace=self.namespace,\n                kind=self.kind,\n                api_version=self.api_version,\n                name=self.name,\n            )\n            if current_obj:\n                current_resources.append(current_obj)\n        else:\n            response, code = self.api_client._list_object_state(\n                method=\"GET\",\n                namespace=self.namespace,\n                kind=self.kind,\n                api_version=self.api_version,\n            )\n            current_resources = response.get(\"items\")\n\n        # yield back the resources\n        for resource in current_resources:\n            log.debug2(\"Yielding initial state event\")\n            yield self._make_watch_response(KubeEventType.ADDED, resource)\n\n        log.debug2(\"Yielded initial state. Starting watch\")\n        # Create a watch queue and add it to the api_client\n        self.api_client._add_watch_queue(self.watch_queue)\n\n        # Configure the timeout and end times\n        timeout_delta = timedelta(seconds=self.timeout)\n        end_time = datetime.now() + timeout_delta\n        while True:\n            timeout = (end_time - datetime.now()).total_seconds() or 1\n            try:\n                event_type, resource = self.watch_queue.get(timeout=timeout)\n            except Empty:\n                return\n\n            if self._check_end_conditions(end_time):\n                return\n\n            resource_metadata = resource.get(\"metadata\", {})\n\n            # Ensure the kind/apiversion/namespace match the requested\n            if (\n                resource.get(\"kind\") == self.kind\n                and resource.get(\"apiVersion\") == self.api_version\n                and resource_metadata.get(\"namespace\") == self.namespace\n            ):\n                # If resourced then ensure the name matches\n                if self.name and resource_metadata.get(\"name\") != self.name:\n                    continue\n\n                log.debug2(\"Yielding watch event\")\n                yield self._make_watch_response(event_type, resource)\n\n    def close(self):\n        pass\n\n    def release_conn(self):\n        self.shutdown.set()\n\n    def _check_end_conditions(self, end_time):\n        log.debug3(\"Checking shutdown and endtime conditions\")\n        if self.shutdown.is_set():\n            return True\n\n        return end_time &lt; datetime.now()\n\n    def _make_watch_response(self, event, object):\n        # Add new line to watch response\n        response = json.dumps({\"type\": event.value, \"object\": object}) + \"\\n\"\n        return response\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.kub_mock.MockWatchStreamResponse.stream","title":"<code>stream(*args, **kwargs)</code>","text":"<p>Continuously yield events from the cluster until the shutdown or timeout</p> Source code in <code>oper8/test_helpers/kub_mock.py</code> <pre><code>def stream(self, *args, **kwargs):\n    \"\"\"Continuously yield events from the cluster until the shutdown or timeout\"\"\"\n\n    # Get the current resource state\n    current_resources = []\n    if self.name:\n        current_obj = self.api_client._get_object_state(\n            method=\"GET\",\n            namespace=self.namespace,\n            kind=self.kind,\n            api_version=self.api_version,\n            name=self.name,\n        )\n        if current_obj:\n            current_resources.append(current_obj)\n    else:\n        response, code = self.api_client._list_object_state(\n            method=\"GET\",\n            namespace=self.namespace,\n            kind=self.kind,\n            api_version=self.api_version,\n        )\n        current_resources = response.get(\"items\")\n\n    # yield back the resources\n    for resource in current_resources:\n        log.debug2(\"Yielding initial state event\")\n        yield self._make_watch_response(KubeEventType.ADDED, resource)\n\n    log.debug2(\"Yielded initial state. Starting watch\")\n    # Create a watch queue and add it to the api_client\n    self.api_client._add_watch_queue(self.watch_queue)\n\n    # Configure the timeout and end times\n    timeout_delta = timedelta(seconds=self.timeout)\n    end_time = datetime.now() + timeout_delta\n    while True:\n        timeout = (end_time - datetime.now()).total_seconds() or 1\n        try:\n            event_type, resource = self.watch_queue.get(timeout=timeout)\n        except Empty:\n            return\n\n        if self._check_end_conditions(end_time):\n            return\n\n        resource_metadata = resource.get(\"metadata\", {})\n\n        # Ensure the kind/apiversion/namespace match the requested\n        if (\n            resource.get(\"kind\") == self.kind\n            and resource.get(\"apiVersion\") == self.api_version\n            and resource_metadata.get(\"namespace\") == self.namespace\n        ):\n            # If resourced then ensure the name matches\n            if self.name and resource_metadata.get(\"name\") != self.name:\n                continue\n\n            log.debug2(\"Yielding watch event\")\n            yield self._make_watch_response(event_type, resource)\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.kub_mock.mock_kub_client_constructor","title":"<code>mock_kub_client_constructor(*args, **kwargs)</code>","text":"<p>Context manager to patch the api client</p> Source code in <code>oper8/test_helpers/kub_mock.py</code> <pre><code>@contextmanager\ndef mock_kub_client_constructor(*args, **kwargs):\n    \"\"\"Context manager to patch the api client\"\"\"\n    log.debug(\"Getting mocked client\")\n    client = MockKubClient(*args, **kwargs)\n    log.debug(\"Mock client complete\")\n    with mock.patch(\n        \"kubernetes.config.new_client_from_config\",\n        return_value=client,\n    ):\n        yield client\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.oper8x_helpers","title":"<code>oper8x_helpers</code>","text":"<p>This module holds helpers that rely on oper8.x</p>"},{"location":"API%20References/#oper8.test_helpers.oper8x_helpers.set_tls_ca_secret","title":"<code>set_tls_ca_secret(session)</code>","text":"<p>Set the key/cert content for the shared CA secret. This function returns the pem-encoded values for convenience in other tests</p> Source code in <code>oper8/test_helpers/oper8x_helpers.py</code> <pre><code>def set_tls_ca_secret(session):\n    \"\"\"Set the key/cert content for the shared CA secret. This function returns\n    the pem-encoded values for convenience in other tests\n    \"\"\"\n    with open(os.path.join(TEST_DATA_DIR, \"test_ca.key\")) as f:\n        key_pem = f.read()\n    with open(os.path.join(TEST_DATA_DIR, \"test_ca.crt\")) as f:\n        crt_pem = f.read()\n    set_secret_data(\n        session,\n        InternalCaComponent.CA_SECRET_NAME,\n        data={\n            InternalCaComponent.CA_KEY_FILENAME: common.b64_secret(key_pem),\n            InternalCaComponent.CA_CRT_FILENAME: common.b64_secret(crt_pem),\n        },\n    )\n\n    return key_pem, crt_pem\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.pwm_helpers","title":"<code>pwm_helpers</code>","text":"<p>Utils and common classes for the python watch manager tests</p>"},{"location":"API%20References/#oper8.test_helpers.pwm_helpers.DisabledLeadershipManager","title":"<code>DisabledLeadershipManager</code>","text":"<p>               Bases: <code>LeadershipManagerBase</code></p> <p>Leadership Manager that is always disabled</p> Source code in <code>oper8/test_helpers/pwm_helpers.py</code> <pre><code>class DisabledLeadershipManager(LeadershipManagerBase):\n    \"\"\"Leadership Manager that is always disabled\"\"\"\n\n    def __init__(self):\n        self.shutdown_event = Event()\n\n    def acquire_resource(self, resource):\n        return False\n\n    def acquire(self, force: bool = False) -&gt; bool:\n        if force:\n            self.shutdown_event.set()\n        return self.shutdown_event.wait()\n\n    def release(self):\n        raise NotImplementedError()\n\n    def release_resource(self, resource=None):\n        raise NotImplementedError()\n\n    def is_leader(self):\n        return False\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.pwm_helpers.MockedReconcileThread","title":"<code>MockedReconcileThread</code>","text":"<p>               Bases: <code>ReconcileThread</code></p> <p>Subclass of ReconcileThread that mocks the subprocess. This was more reliable than using unittest.mock</p> Source code in <code>oper8/test_helpers/pwm_helpers.py</code> <pre><code>class MockedReconcileThread(ReconcileThread):\n    \"\"\"Subclass of ReconcileThread that mocks the subprocess. This was more\n    reliable than using unittest.mock\"\"\"\n\n    _disable_singleton = True\n\n    def __init__(\n        self,\n        deploy_manager=None,\n        leadership_manager=None,\n        subprocess_wait_time=0.1,\n        returned_messages=None,\n    ):\n        self.requests = Queue()\n        self.timer_events = Queue()\n        self.processes_started = 0\n        self.processes_finished = 0\n        self.watch_threads_created = 0\n        self.subprocess_wait_time = subprocess_wait_time\n        self.returned_messages = returned_messages or [[]]\n        super().__init__(deploy_manager, leadership_manager)\n\n    def push_request(self, request: ReconcileRequest):\n        self.requests.put(request)\n        super().push_request(request)\n\n    def get_request(self) -&gt; ReconcileRequest:\n        return self.requests.get()\n\n    def _handle_watch_request(self, request: WatchRequest):\n        self.watch_threads_created += 1\n        return super()._handle_watch_request(request)\n\n    def _handle_process_end(self, reconcile_process: ReconcileProcess):\n        self.processes_finished += 1\n        return super()._handle_process_end(reconcile_process)\n\n    def _start_reconcile_process(\n        self, request: ReconcileRequest, pipe: Connection\n    ) -&gt; multiprocessing.Process:\n        self.processes_started += 1\n\n        returned_messages = []\n        if len(self.returned_messages) &gt; 0:\n            returned_messages = self.returned_messages.pop(0)\n\n        # Create and start a mocked reconcile process\n        process = self.spawn_ctx.Process(\n            target=mocked_create_and_start_entrypoint,\n            args=[\n                self.logging_queue,\n                request,\n                pipe,\n                self.subprocess_wait_time,\n                returned_messages,\n            ],\n        )\n        process.start()\n        log.debug3(f\"Started child process with pid: {process.pid}\")\n\n        return process\n\n    def _create_timer_event_for_request(\n        self, request: ReconcileRequest, result: ReconciliationResult = None\n    ):\n        timer_event = super()._create_timer_event_for_request(request, result)\n        self.timer_events.put(timer_event)\n        return timer_event\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.pwm_helpers.mocked_create_and_start_entrypoint","title":"<code>mocked_create_and_start_entrypoint(logging_queue, request, result_pipe, wait_time=0.5, returned_messages=None)</code>","text":"Source code in <code>oper8/test_helpers/pwm_helpers.py</code> <pre><code>def mocked_create_and_start_entrypoint(\n    logging_queue: multiprocessing.Queue,\n    request: ReconcileRequest,\n    result_pipe: Connection,\n    wait_time=0.5,\n    returned_messages=None,\n):\n    \"\"\"\"\"\"\n    time.sleep(wait_time)\n    for message in returned_messages or []:\n        result_pipe.send(message)\n</code></pre>"},{"location":"API%20References/#oper8.test_helpers.pwm_helpers.read_heartbeat_file","title":"<code>read_heartbeat_file(hb_file)</code>","text":"<p>Parse a heartbeat file into a datetime</p> Source code in <code>oper8/test_helpers/pwm_helpers.py</code> <pre><code>def read_heartbeat_file(hb_file: str) -&gt; datetime:\n    \"\"\"Parse a heartbeat file into a datetime\"\"\"\n    with open(hb_file) as handle:\n        hb_str = handle.read()\n\n    return datetime.strptime(hb_str, HeartbeatThread._DATE_FORMAT)\n</code></pre>"},{"location":"API%20References/#oper8.utils","title":"<code>utils</code>","text":"<p>Common utilities shared across components in the library</p>"},{"location":"API%20References/#oper8.utils.abstractclassproperty","title":"<code>abstractclassproperty</code>","text":"<p>This decorator implements a classproperty that will raise when accessed</p> Source code in <code>oper8/utils.py</code> <pre><code>class abstractclassproperty:  # pylint: disable=invalid-name,too-few-public-methods\n    \"\"\"This decorator implements a classproperty that will raise when accessed\"\"\"\n\n    def __init__(self, func):\n        self.prop_name = func.__name__\n\n    def __get__(self, *args):\n        # If this is being called by __setattr__, we're ok because it's\n        # attempting to set the attribute on the class\n        curframe = inspect.currentframe()\n        callframe = inspect.getouterframes(curframe, 2)[1]\n        caller_name = callframe[3]\n        if caller_name == \"__setattr__\":\n            return None\n\n        # If this is a help() call or a pdoc documentation request, return an\n        # object with a docstring indicating that the property is abstract\n        if (\n            \"help\" in callframe.frame.f_code.co_names\n            or callframe.frame.f_globals[\"__name__\"] == \"pdoc\"\n        ):\n\n            class AbstractClassProperty:  # pylint: disable=missing-class-docstring\n                __slots__ = []\n                __doc__ = f\"\"\"The &lt;{self.prop_name}&gt; property is an abstract class property\n                that must be overwritten in derived children\n                \"\"\"\n\n            return AbstractClassProperty\n\n        raise NotImplementedError(\n            f\"Cannot access abstractclassproperty {self.prop_name}\"\n        )\n</code></pre>"},{"location":"API%20References/#oper8.utils.classproperty","title":"<code>classproperty</code>","text":"<p>@classmethod+@property CITE: https://stackoverflow.com/a/22729414</p> Source code in <code>oper8/utils.py</code> <pre><code>class classproperty:  # pylint: disable=invalid-name,too-few-public-methods\n    \"\"\"@classmethod+@property\n    CITE: https://stackoverflow.com/a/22729414\n    \"\"\"\n\n    def __init__(self, func):\n        self.func = classmethod(func)\n\n    def __get__(self, *args):\n        return self.func.__get__(*args)()\n</code></pre>"},{"location":"API%20References/#oper8.utils.add_finalizer","title":"<code>add_finalizer(session, finalizer)</code>","text":"<p>This helper adds a finalizer to current session CR</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SESSION_TYPE</code> <p>Session The session for the current deploy</p> required <code>finalizer</code> <code>str</code> <p>str The finalizer to be added</p> required Source code in <code>oper8/utils.py</code> <pre><code>def add_finalizer(session: SESSION_TYPE, finalizer: str):\n    \"\"\"This helper adds a finalizer to current session CR\n\n    Args:\n        session:  Session\n            The session for the current deploy\n        finalizer: str\n            The finalizer to be added\n    \"\"\"\n    if finalizer in session.finalizers:\n        return\n\n    log.debug(\"Adding finalizer: %s\", finalizer)\n\n    manifest = {\n        \"kind\": session.kind,\n        \"apiVersion\": session.api_version,\n        \"metadata\": copy.deepcopy(session.metadata),\n    }\n    manifest[\"metadata\"].setdefault(\"finalizers\", []).append(finalizer)\n    success, _ = session.deploy_manager.deploy([manifest])\n\n    # Once successfully added to cluster than add it to session\n    assert_cluster(success, f\"Failed add finalizer {finalizer}\")\n    session.finalizers.append(finalizer)\n</code></pre>"},{"location":"API%20References/#oper8.utils.get_manifest_version","title":"<code>get_manifest_version(cr_manifest)</code>","text":"<p>Get the version for a given custom resource or from the config if version override provided</p> <p>Parameters:</p> Name Type Description Default <code>cr_manifest</code> <code>Config</code> <p>aconfig.Config The manifest to pull the version from</p> required <p>Returns:</p> Name Type Description <code>version</code> <code>str</code> <p>str The current version</p> Source code in <code>oper8/utils.py</code> <pre><code>def get_manifest_version(cr_manifest: aconfig.Config) -&gt; str:\n    \"\"\"Get the version for a given custom resource or from the config\n    if version override provided\n\n    Args:\n        cr_manifest: aconfig.Config\n            The manifest to pull the version from\n\n    Returns:\n        version: str\n            The current version\n    \"\"\"\n    if config.vcs.version_override:\n        return config.vcs.version_override\n    return nested_get(cr_manifest, config.vcs.field)\n</code></pre>"},{"location":"API%20References/#oper8.utils.get_passthrough_annotations","title":"<code>get_passthrough_annotations(session)</code>","text":"<p>This helper gets the set of annotations that should be passed from a parent CR to a child subsystem CR.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <p>DeploySession The session for the current deploy</p> required <p>Returns:</p> Name Type Description <code>annotations</code> <p>Dict[str, str] The dict mapping of annotations that should be passed through</p> Source code in <code>oper8/utils.py</code> <pre><code>def get_passthrough_annotations(session):\n    \"\"\"This helper gets the set of annotations that should be passed from a\n    parent CR to a child subsystem CR.\n\n    Args:\n        session:  DeploySession\n            The session for the current deploy\n\n    Returns:\n        annotations:  Dict[str, str]\n            The dict mapping of annotations that should be passed through\n    \"\"\"\n    annotations = session.metadata.get(\"annotations\", {})\n    passthrough_annotations = {\n        k: v for k, v in annotations.items() if k in constants.ALL_ANNOTATIONS\n    }\n\n    log.debug2(\"Oper8 passthrough annotations: %s\", passthrough_annotations)\n    return passthrough_annotations\n</code></pre>"},{"location":"API%20References/#oper8.utils.merge_configs","title":"<code>merge_configs(base, overrides)</code>","text":"<p>Helper to perform a deep merge of the overrides into the base. The merge is done in place, but the resulting dict is also returned for convenience.</p> <p>The merge logic is quite simple: If both the base and overrides have a key and the type of the key for both is a dict, recursively merge, otherwise set the base value to the override value.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <p>dict The base config that will be updated with the overrides</p> required <code>overrides</code> <p>dict The override config</p> required <p>Returns:</p> Name Type Description <code>merged</code> <code>dict</code> <p>dict The merged results of overrides merged onto base</p> Source code in <code>oper8/utils.py</code> <pre><code>def merge_configs(base, overrides) -&gt; dict:\n    \"\"\"Helper to perform a deep merge of the overrides into the base. The merge\n    is done in place, but the resulting dict is also returned for convenience.\n\n    The merge logic is quite simple: If both the base and overrides have a key\n    and the type of the key for both is a dict, recursively merge, otherwise\n    set the base value to the override value.\n\n    Args:\n        base:  dict\n            The base config that will be updated with the overrides\n        overrides:  dict\n            The override config\n\n    Returns:\n        merged:  dict\n            The merged results of overrides merged onto base\n    \"\"\"\n    for key, value in overrides.items():\n        if (\n            key not in base\n            or not isinstance(base[key], dict)\n            or not isinstance(value, dict)\n        ):\n            base[key] = value\n        else:\n            base[key] = merge_configs(base[key], value)\n\n    return base\n</code></pre>"},{"location":"API%20References/#oper8.utils.nested_get","title":"<code>nested_get(dct, key, dflt=None)</code>","text":"<p>Helper to get values from a dict using 'foo.bar' key notation</p> <p>Parameters:</p> Name Type Description Default <code>dct</code> <code>dict</code> <p>dict The dict into which the key will be set</p> required <code>key</code> <code>str</code> <p>str Key that may contain '.' notation indicating dict nesting</p> required <p>Returns:</p> Name Type Description <code>val</code> <code>Any</code> <p>Any Whatever is found at the given key or None if the key is not found. This includes missing intermediate dicts.</p> Source code in <code>oper8/utils.py</code> <pre><code>def nested_get(dct: dict, key: str, dflt=None) -&gt; Any:\n    \"\"\"Helper to get values from a dict using 'foo.bar' key notation\n\n    Args:\n        dct:  dict\n            The dict into which the key will be set\n        key:  str\n            Key that may contain '.' notation indicating dict nesting\n\n    Returns:\n        val:  Any\n            Whatever is found at the given key or None if the key is not found.\n            This includes missing intermediate dicts.\n    \"\"\"\n    parts = key.split(constants.NESTED_DICT_DELIM)\n    for i, part in enumerate(parts[:-1]):\n        dct = dct.get(part, __MISSING__)\n        if dct is __MISSING__:\n            return dflt\n        if not isinstance(dct, dict):\n            raise TypeError(\n                \"Intermediate key {} is not a dict\".format(  # pylint: disable=consider-using-f-string\n                    constants.NESTED_DICT_DELIM.join(parts[:i])\n                )\n            )\n    return dct.get(parts[-1], dflt)\n</code></pre>"},{"location":"API%20References/#oper8.utils.nested_set","title":"<code>nested_set(dct, key, val)</code>","text":"<p>Helper to set values in a dict using 'foo.bar' key notation</p> <p>Parameters:</p> Name Type Description Default <code>dct</code> <code>dict</code> <p>dict The dict into which the key will be set</p> required <code>key</code> <code>str</code> <p>str Key that may contain '.' notation indicating dict nesting</p> required <code>val</code> <code>Any</code> <p>Any The value to place at the nested key</p> required Source code in <code>oper8/utils.py</code> <pre><code>def nested_set(dct: dict, key: str, val: Any):\n    \"\"\"Helper to set values in a dict using 'foo.bar' key notation\n\n    Args:\n        dct:  dict\n            The dict into which the key will be set\n        key:  str\n            Key that may contain '.' notation indicating dict nesting\n        val:  Any\n            The value to place at the nested key\n    \"\"\"\n    parts = key.split(constants.NESTED_DICT_DELIM)\n    for i, part in enumerate(parts[:-1]):\n        dct = dct.setdefault(part, {})\n        if not isinstance(dct, dict):\n            raise TypeError(\n                \"Intermediate key {} is not a dict\".format(  # pylint: disable=consider-using-f-string\n                    constants.NESTED_DICT_DELIM.join(parts[:i])\n                )\n            )\n    dct[parts[-1]] = val\n</code></pre>"},{"location":"API%20References/#oper8.utils.remove_finalizer","title":"<code>remove_finalizer(session, finalizer)</code>","text":"<p>This helper gets removes a finalizer from the current session controller</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SESSION_TYPE</code> <p>Session The session for the current deploy</p> required <code>finalizer</code> <code>str</code> <p>str The finalizer to remove</p> required <p>Returns:</p> Name Type Description <code>annotations</code> <p>Dict[str, str] The dict mapping of annotations that should be passed through</p> Source code in <code>oper8/utils.py</code> <pre><code>def remove_finalizer(session: SESSION_TYPE, finalizer: str):\n    \"\"\"This helper gets removes a finalizer from the current session controller\n\n    Args:\n        session:  Session\n            The session for the current deploy\n        finalizer: str\n            The finalizer to remove\n\n    Returns:\n        annotations:  Dict[str, str]\n            The dict mapping of annotations that should be passed through\n    \"\"\"\n    if finalizer not in session.finalizers:\n        return\n\n    log.debug(\"Removing finalizer: %s\", finalizer)\n\n    # Create manifest with only required fields\n    manifest = {\n        \"kind\": session.kind,\n        \"apiVersion\": session.api_version,\n        \"metadata\": copy.deepcopy(session.metadata),\n    }\n\n    # Check to see if the object exists in the cluster\n    success, found = session.get_object_current_state(\n        kind=session.kind,\n        api_version=session.api_version,\n        name=session.name,\n    )\n    assert_cluster(success, \"Failed to look up CR for self\")\n\n    # If still present in the cluster, update it without the finalizer\n    if found:\n        manifest[\"metadata\"][\"finalizers\"].remove(finalizer)\n        success, _ = session.deploy_manager.deploy([manifest])\n\n        # Once successfully removed from cluster than remove from session\n        assert_cluster(success, f\"Failed remove finalizer {finalizer}\")\n\n    # If the finalizer has been confirmed to not be there, remove it from the\n    # in-memory finalizers\n    session.finalizers.remove(finalizer)\n</code></pre>"},{"location":"API%20References/#oper8.utils.sanitize_for_serialization","title":"<code>sanitize_for_serialization(obj)</code>","text":"<p>Builds a JSON POST object. If obj is None, return None. If obj is str, int, long, float, bool, return directly. If obj is datetime.datetime, datetime.date     convert to string in iso8601 format. If obj is list, sanitize each element in the list. If obj is dict, return the dict. If obj is OpenAPI model, return the properties dict. :param obj: The data to serialize. :return: The serialized form of data.</p> Source code in <code>oper8/utils.py</code> <pre><code>def sanitize_for_serialization(obj):  # pylint: disable=too-many-return-statements\n    \"\"\"Builds a JSON POST object.\n    If obj is None, return None.\n    If obj is str, int, long, float, bool, return directly.\n    If obj is datetime.datetime, datetime.date\n        convert to string in iso8601 format.\n    If obj is list, sanitize each element in the list.\n    If obj is dict, return the dict.\n    If obj is OpenAPI model, return the properties dict.\n    :param obj: The data to serialize.\n    :return: The serialized form of data.\n    \"\"\"\n    if obj is None:  # pylint: disable=no-else-return\n        return None\n    elif isinstance(obj, (float, bool, bytes, six.text_type) + six.integer_types):\n        return obj\n    elif isinstance(obj, list):\n        return [sanitize_for_serialization(sub_obj) for sub_obj in obj]\n    elif isinstance(obj, tuple):\n        return tuple(sanitize_for_serialization(sub_obj) for sub_obj in obj)\n    elif isinstance(obj, (datetime.datetime, datetime.date)):\n        return obj.isoformat()\n    elif isinstance(obj, ResourceNode):\n        return sanitize_for_serialization(obj.manifest)\n    elif isinstance(obj, property):\n        return sanitize_for_serialization(obj.fget())\n\n    if isinstance(obj, dict):\n        obj_dict = obj\n    elif hasattr(obj, \"attribute_map\"):\n        # Convert model obj to dict except\n        # `openapi_types` and `attribute_map`.\n        # Convert attribute name to json key in\n        # model definition for request.\n        obj_dict = {}\n        for attr, name in six.iteritems(obj.attribute_map):\n            if hasattr(obj, attr):\n                obj_dict[name] = getattr(obj, attr)\n\n    # Prune fields which are None but keep\n    # empty arrays or dictionaries\n    return_dict = {}\n    for key, val in six.iteritems(obj_dict):\n        updated_obj = sanitize_for_serialization(val)\n        if updated_obj is not None:\n            return_dict[key] = updated_obj\n    return return_dict\n</code></pre>"},{"location":"API%20References/#oper8.vcs","title":"<code>vcs</code>","text":"<p>Version Control System class manages a specific git directory.</p>"},{"location":"API%20References/#oper8.vcs.VCS","title":"<code>VCS</code>","text":"<p>Generic class for handling a git repository. This class contains helper functions to get, list, and checkout references. Each instance of this class corresponds to a different git directory</p> Source code in <code>oper8/vcs.py</code> <pre><code>class VCS:\n    \"\"\"Generic class for handling a git repository. This class contains helper functions\n    to get, list, and checkout references. Each instance of this class corresponds to a\n    different git directory\n    \"\"\"\n\n    def __init__(self, directory: str, create_if_needed: bool = False, **kwargs):\n        \"\"\"Initialize the pygit2 Repository reference\n\n        Args:\n            directory: str\n                The git directory\n            create_if_needed: bool\n                If True, the repo will be initialized if it doesn't already\n                exist\n            **kwargs:\n                Passthrough args to the repository setup\n        \"\"\"\n        # Get repo reference\n        try:\n            # Check for global file and create one if needed. This\n            # is needed due to this issue: https://github.com/libgit2/pygit2/issues/915\n            config_file = (\n                pathlib.Path(option(GIT_OPT_GET_SEARCH_PATH, GIT_CONFIG_LEVEL_GLOBAL))\n                / \".gitconfig\"\n            )\n            if not config_file.exists():\n                config_file.touch(exist_ok=True)\n\n            # Disable safe git directories. This solves a common problem\n            # when running in openshift where the running user is different\n            # from the owner of the filesystem\n            global_config = Config.get_global_config()\n            global_config[\"safe.directory\"] = \"*\"\n\n            self.repo = Repository(directory)\n            log.debug2(\"Found repo: %s\", self.repo)\n        except GitError as err:\n            if create_if_needed:\n                self.repo = init_repository(directory, **kwargs)\n            else:\n                log.error(\"Invalid Repo: %s\", err, exc_info=True)\n                raise VCSConfigError(f\"Invalid Repo at {directory}\") from err\n\n    ### Accessors\n\n    @property\n    def head(self) -&gt; str:\n        \"\"\"Get a reference to the current HEAD\"\"\"\n        return str(self.repo.head.target)\n\n    def get_ref(self, refish: str) -&gt; Tuple[Commit, Reference]:\n        \"\"\"Get a git commit and reference from a shorthand string\n\n        Args:\n            refish: str\n                The human readable form of a git reference like branch name\n                or commit hash\n\n        Returns\n            commit_and_reference: Tuple[Commit,Reference]\n                Both a commit and reference for a given refish\n        \"\"\"\n        try:\n            return self.repo.resolve_refish(refish)\n        except KeyError as err:\n            log.error(\"Unable to find version %s in repo\", refish)\n            raise VCSConfigError(  # pylint: disable=raise-missing-from\n                f\"Version: '{refish}' not found in repo\"\n            ) from err\n\n    def list_refs(self) -&gt; Set[str]:\n        \"\"\"List all of the tags and references in the repo\n\n        Returns\n            ref_list: Set[str]\n                A set of all references' shorthand as strings\n        \"\"\"\n        # Loop through repo tags to get each tag's short name\n        refs_set = set()\n        for ref in self.repo.references.objects:\n            refs_set.add(ref.shorthand)\n\n        return refs_set\n\n    ### Mutators\n\n    def checkout_ref(\n        self,\n        refish: str,\n        dest_path: Optional[pathlib.Path] = None,\n        method: VCSCheckoutMethod = VCSCheckoutMethod.WORKTREE,\n        **kwargs,\n    ):\n        \"\"\"Checkout a refish to a given destination directory. This function\n        first attempts to create a worktree but on failure will do a traditional\n        clone\n\n        Args:\n            refish: str\n                The refish to be checked out in the dest_dir\n            dest_path: Optional[pathlib.Path]\n                The destination directory if not in-place\n            method: VCSCheckoutMethod=VCSCheckoutMethod.WORKTREE\n                The checkout method to use, either a git clone or worktree add\n            **kwargs\n                Kwargs to pass through to checkout\n        \"\"\"\n\n        # Get the commit and ref for a given refish\n        commit, ref = self.get_ref(refish)\n\n        # If in-place, check out directly\n        if not dest_path:\n            log.debug2(\"Checking out %s in place\", refish)\n            self.repo.checkout(ref, **kwargs)\n            return\n\n        # Check if dest directory already exists and if it has the correct\n        # commit\n        if dest_path.is_dir():\n            dest_vcs = VCS(dest_path)\n\n            # Check if the dest index file has been created. It is the last\n            # part of a checkout. If index has not been created than another\n            # process must be working on it\n            dest_index_file = pathlib.Path(dest_vcs.repo.path) / \"index\"\n            if not dest_index_file.is_file():\n                raise VCSMultiProcessError(\n                    \"Index file not found. Checkout already in progress \"\n                )\n\n            if dest_vcs.repo.head.peel(Commit) != commit:\n                raise VCSConfigError(\n                    f\"Destination directory {dest_path} already exists with incorrect branch\"\n                )\n            return\n\n        # Create the directory if it doesn't exist\n        dest_path.parents[0].mkdir(parents=True, exist_ok=True)\n\n        if method == VCSCheckoutMethod.WORKTREE:\n            # Create a unique branch for each worktree\n            cleaned_dest_dir = \"_\".join(dest_path.parts[1:])\n            branch_name = f\"{refish}_{cleaned_dest_dir}\"\n\n            branch = self.create_branch(branch_name, commit)\n            self._create_worktree(branch_name, dest_path, branch)\n        elif method == VCSCheckoutMethod.CLONE:\n            self._clone_ref(dest_path, ref, **kwargs)\n        else:\n            raise VCSConfigError(f\"Invalid checkout method: {method}\")\n\n    def create_commit(\n        self,\n        message: str,\n        parents: Optional[List[str]] = None,\n        committer_name: str = \"Oper8\",\n        committer_email: str = \"noreply@oper8.org\",\n    ):\n        \"\"\"Create a commit in the repo with the files currently in the index\n\n        Args:\n            message: str\n                The commit message\n            parents: Optional[List[str]]\n                Parent commit hashes\n            committer_name: str\n                The name of the committer\n            committer_email: str\n                Email address for this committer\n        \"\"\"\n        parents = parents or []\n        parent_commits = []\n        for parent in parents:\n            try:\n                parent_commits.append(self.repo.get(parent))\n            except ValueError as err:\n                raise ValueError(f\"Invalid parent commit: {parent}\") from err\n        signature = Signature(committer_name, committer_email)\n        self.repo.create_commit(\n            \"HEAD\", signature, signature, message, self.repo.index.write_tree(), parents\n        )\n\n    def add_remote(self, remote_name: str, remote_path: str):\n        \"\"\"Add a named remote to the repo\n\n        Args:\n            remote_name: str\n                The name of the remote\n            remote_path: str\n                The path on disk to the remote repo\n        \"\"\"\n        self.repo.remotes.create(remote_name, remote_path)\n\n    def delete_remote(self, remote_name: str):\n        \"\"\"Remove a remote from the repo\n\n        Args:\n            remote_name:  str\n                The name of the remote\n        \"\"\"\n        self.repo.remotes.delete(remote_name)\n\n    def fetch_remote(\n        self,\n        remote_name: str,\n        refs: Optional[Set[str]] = None,\n        wait: bool = True,\n    ):\n        \"\"\"Fetch content from the named remote. If no refs given, all refs are\n        fetched.\n\n        Args:\n            remote_name: str\n                The name of the remote to fetch\n            refs: Optional[Set[str]]\n                The refs to fetch (fetch all if not given)\n            wait: bool\n                If true, wait for fetch to complete\n        \"\"\"\n        remote = self.repo.remotes[remote_name]\n        progress = remote.fetch(list(refs or []))\n        while wait and progress.received_objects &lt; progress.total_objects:\n            time.sleep(0.1)  # pragma: no cover\n\n    def create_branch(self, branch_name: str, commit: Commit) -&gt; Branch:\n        \"\"\"Create branch given a name and commit\n\n        Args:\n            branch_name: str\n                The name to be created\n            commit: Commit\n                The commit for the branch to be created from\n\n        Returns:\n            branch: Branch\n                The created branch\"\"\"\n        if branch_name in self.repo.branches:\n            branch = self.repo.branches.get(branch_name)\n            if branch.peel(Commit) != commit:\n                raise VCSRuntimeError(\"Branch already exists with incorrect commit\")\n            return branch\n\n        try:\n            log.debug(\"Creating branch for %s\", branch_name)\n            return self.repo.branches.create(branch_name, commit)\n        except AlreadyExistsError as err:\n            # Branch must have been created by different processes\n            log.warning(\"Branch %s already exists\", branch_name)\n            raise VCSMultiProcessError(f\"Branch {branch_name} already exists\") from err\n\n        except OSError as err:\n            raise VCSRuntimeError(\"Unable to create branch\") from err\n\n    def delete_branch(self, branch_name: str):\n        \"\"\"Delete a branch from the repo\n\n        Args:\n            branch_name:  str\n                The name of the branch\n        \"\"\"\n        self.repo.branches.delete(branch_name)\n\n    def delete_tag(self, tag_name: str):\n        \"\"\"Delete a tag from the repo\n\n        Args:\n            tag_name:  str\n                The name of the tag\n        \"\"\"\n        self.repo.references.delete(f\"refs/tags/{tag_name}\")\n\n    def checkout_detached_head(self, refish: Optional[str] = None):\n        \"\"\"Check out the current HEAD commit as a detached head\n\n        Args:\n            refish:  Optional[str]\n                The ref to check out. If not given, the current HEAD is used\n        \"\"\"\n        refish = refish or self.head\n\n        # Create a placeholder reference to a non-existent remote\n        dummy_ref = self.repo.references.create(\n            \"refs/remotes/doesnotexist/foobar\", refish\n        )\n        self.repo.checkout(dummy_ref)\n        self.repo.references.delete(dummy_ref.name)\n\n    def compress_references(self):\n        \"\"\"Compress unreachable references in the repo\"\"\"\n        self.repo.compress_references()\n\n    ### Implementation Details\n\n    def _clone_ref(self, dest_path: pathlib.Path, ref: Reference, **kwargs):\n        \"\"\"Clone a refish to a given destination directory\n\n        Args:\n            dest_path: pathlib.Path\n                The destination directory\n            refish: str\n                The branch or ref to be checked out\n            **kwargs\n                Kwargs to pass through to checkout\n        \"\"\"\n        try:\n            dest_repo = clone_repository(self.repo.path, dest_path)\n            dest_repo.checkout(refname=ref, **kwargs)\n        except (OSError, GitError, KeyError) as err:\n            log.error(\"Unable to clone refish: %s\", ref.shorthand, exc_info=True)\n            raise VCSRuntimeError(\"Unable to clone ref from repo\") from err\n\n    def _create_worktree(\n        self, worktree_name: str, dest_path: pathlib.Path, branch: Branch\n    ):\n        \"\"\"Create worktree for branch. This is better than a direct checkout\n        as it saves space on checkout and is faster. This is especially\n        beneficial on repositories with large git directories\n\n        Args:\n           worktree_name: str\n               The name of the worktree\n           dest_path: pathlib.Path\n               The destination directory\n           branch: Branch\n               The branch to be checked out in the worktree\n        \"\"\"\n        log.debug(\"Creating new worktree for %s\", worktree_name)\n        try:\n            self.repo.add_worktree(worktree_name, dest_path, branch)\n        except AlreadyExistsError as err:\n            # Worktree must have been created by different processes\n            log.warning(\"Worktree %s already exists\", worktree_name)\n            raise VCSMultiProcessError(\n                f\"Worktree {worktree_name} already exists\"\n            ) from err\n        except GitError as err:\n            # If reference is already checked out it must have been done by a different process\n            if \"is already checked out\" in str(err):\n                log.warning(\n                    \"Branch %s already checked out by other process\",\n                    worktree_name,\n                    exc_info=True,\n                )\n                raise VCSMultiProcessError(\n                    f\"Branch {worktree_name} already checked out by other process\"\n                ) from err\n\n            log.error(\n                \"Unexpected Git Error when adding worktree: %s\", err, exc_info=True\n            )\n            raise VCSRuntimeError(\n                \"Adding worktree failed with unexpected git error\"\n            ) from err\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCS.head","title":"<code>head</code>  <code>property</code>","text":"<p>Get a reference to the current HEAD</p>"},{"location":"API%20References/#oper8.vcs.VCS.__init__","title":"<code>__init__(directory, create_if_needed=False, **kwargs)</code>","text":"<p>Initialize the pygit2 Repository reference</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>str The git directory</p> required <code>create_if_needed</code> <code>bool</code> <p>bool If True, the repo will be initialized if it doesn't already exist</p> <code>False</code> <code>**kwargs</code> <p>Passthrough args to the repository setup</p> <code>{}</code> Source code in <code>oper8/vcs.py</code> <pre><code>def __init__(self, directory: str, create_if_needed: bool = False, **kwargs):\n    \"\"\"Initialize the pygit2 Repository reference\n\n    Args:\n        directory: str\n            The git directory\n        create_if_needed: bool\n            If True, the repo will be initialized if it doesn't already\n            exist\n        **kwargs:\n            Passthrough args to the repository setup\n    \"\"\"\n    # Get repo reference\n    try:\n        # Check for global file and create one if needed. This\n        # is needed due to this issue: https://github.com/libgit2/pygit2/issues/915\n        config_file = (\n            pathlib.Path(option(GIT_OPT_GET_SEARCH_PATH, GIT_CONFIG_LEVEL_GLOBAL))\n            / \".gitconfig\"\n        )\n        if not config_file.exists():\n            config_file.touch(exist_ok=True)\n\n        # Disable safe git directories. This solves a common problem\n        # when running in openshift where the running user is different\n        # from the owner of the filesystem\n        global_config = Config.get_global_config()\n        global_config[\"safe.directory\"] = \"*\"\n\n        self.repo = Repository(directory)\n        log.debug2(\"Found repo: %s\", self.repo)\n    except GitError as err:\n        if create_if_needed:\n            self.repo = init_repository(directory, **kwargs)\n        else:\n            log.error(\"Invalid Repo: %s\", err, exc_info=True)\n            raise VCSConfigError(f\"Invalid Repo at {directory}\") from err\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCS.add_remote","title":"<code>add_remote(remote_name, remote_path)</code>","text":"<p>Add a named remote to the repo</p> <p>Parameters:</p> Name Type Description Default <code>remote_name</code> <code>str</code> <p>str The name of the remote</p> required <code>remote_path</code> <code>str</code> <p>str The path on disk to the remote repo</p> required Source code in <code>oper8/vcs.py</code> <pre><code>def add_remote(self, remote_name: str, remote_path: str):\n    \"\"\"Add a named remote to the repo\n\n    Args:\n        remote_name: str\n            The name of the remote\n        remote_path: str\n            The path on disk to the remote repo\n    \"\"\"\n    self.repo.remotes.create(remote_name, remote_path)\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCS.checkout_detached_head","title":"<code>checkout_detached_head(refish=None)</code>","text":"<p>Check out the current HEAD commit as a detached head</p> <p>Parameters:</p> Name Type Description Default <code>refish</code> <code>Optional[str]</code> <p>Optional[str] The ref to check out. If not given, the current HEAD is used</p> <code>None</code> Source code in <code>oper8/vcs.py</code> <pre><code>def checkout_detached_head(self, refish: Optional[str] = None):\n    \"\"\"Check out the current HEAD commit as a detached head\n\n    Args:\n        refish:  Optional[str]\n            The ref to check out. If not given, the current HEAD is used\n    \"\"\"\n    refish = refish or self.head\n\n    # Create a placeholder reference to a non-existent remote\n    dummy_ref = self.repo.references.create(\n        \"refs/remotes/doesnotexist/foobar\", refish\n    )\n    self.repo.checkout(dummy_ref)\n    self.repo.references.delete(dummy_ref.name)\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCS.checkout_ref","title":"<code>checkout_ref(refish, dest_path=None, method=VCSCheckoutMethod.WORKTREE, **kwargs)</code>","text":"<p>Checkout a refish to a given destination directory. This function first attempts to create a worktree but on failure will do a traditional clone</p> <p>Parameters:</p> Name Type Description Default <code>refish</code> <code>str</code> <p>str The refish to be checked out in the dest_dir</p> required <code>dest_path</code> <code>Optional[Path]</code> <p>Optional[pathlib.Path] The destination directory if not in-place</p> <code>None</code> <code>method</code> <code>VCSCheckoutMethod</code> <p>VCSCheckoutMethod=VCSCheckoutMethod.WORKTREE The checkout method to use, either a git clone or worktree add</p> <code>WORKTREE</code> Source code in <code>oper8/vcs.py</code> <pre><code>def checkout_ref(\n    self,\n    refish: str,\n    dest_path: Optional[pathlib.Path] = None,\n    method: VCSCheckoutMethod = VCSCheckoutMethod.WORKTREE,\n    **kwargs,\n):\n    \"\"\"Checkout a refish to a given destination directory. This function\n    first attempts to create a worktree but on failure will do a traditional\n    clone\n\n    Args:\n        refish: str\n            The refish to be checked out in the dest_dir\n        dest_path: Optional[pathlib.Path]\n            The destination directory if not in-place\n        method: VCSCheckoutMethod=VCSCheckoutMethod.WORKTREE\n            The checkout method to use, either a git clone or worktree add\n        **kwargs\n            Kwargs to pass through to checkout\n    \"\"\"\n\n    # Get the commit and ref for a given refish\n    commit, ref = self.get_ref(refish)\n\n    # If in-place, check out directly\n    if not dest_path:\n        log.debug2(\"Checking out %s in place\", refish)\n        self.repo.checkout(ref, **kwargs)\n        return\n\n    # Check if dest directory already exists and if it has the correct\n    # commit\n    if dest_path.is_dir():\n        dest_vcs = VCS(dest_path)\n\n        # Check if the dest index file has been created. It is the last\n        # part of a checkout. If index has not been created than another\n        # process must be working on it\n        dest_index_file = pathlib.Path(dest_vcs.repo.path) / \"index\"\n        if not dest_index_file.is_file():\n            raise VCSMultiProcessError(\n                \"Index file not found. Checkout already in progress \"\n            )\n\n        if dest_vcs.repo.head.peel(Commit) != commit:\n            raise VCSConfigError(\n                f\"Destination directory {dest_path} already exists with incorrect branch\"\n            )\n        return\n\n    # Create the directory if it doesn't exist\n    dest_path.parents[0].mkdir(parents=True, exist_ok=True)\n\n    if method == VCSCheckoutMethod.WORKTREE:\n        # Create a unique branch for each worktree\n        cleaned_dest_dir = \"_\".join(dest_path.parts[1:])\n        branch_name = f\"{refish}_{cleaned_dest_dir}\"\n\n        branch = self.create_branch(branch_name, commit)\n        self._create_worktree(branch_name, dest_path, branch)\n    elif method == VCSCheckoutMethod.CLONE:\n        self._clone_ref(dest_path, ref, **kwargs)\n    else:\n        raise VCSConfigError(f\"Invalid checkout method: {method}\")\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCS.compress_references","title":"<code>compress_references()</code>","text":"<p>Compress unreachable references in the repo</p> Source code in <code>oper8/vcs.py</code> <pre><code>def compress_references(self):\n    \"\"\"Compress unreachable references in the repo\"\"\"\n    self.repo.compress_references()\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCS.create_branch","title":"<code>create_branch(branch_name, commit)</code>","text":"<p>Create branch given a name and commit</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>str The name to be created</p> required <code>commit</code> <code>Commit</code> <p>Commit The commit for the branch to be created from</p> required <p>Returns:</p> Name Type Description <code>branch</code> <code>Branch</code> <p>Branch The created branch</p> Source code in <code>oper8/vcs.py</code> <pre><code>def create_branch(self, branch_name: str, commit: Commit) -&gt; Branch:\n    \"\"\"Create branch given a name and commit\n\n    Args:\n        branch_name: str\n            The name to be created\n        commit: Commit\n            The commit for the branch to be created from\n\n    Returns:\n        branch: Branch\n            The created branch\"\"\"\n    if branch_name in self.repo.branches:\n        branch = self.repo.branches.get(branch_name)\n        if branch.peel(Commit) != commit:\n            raise VCSRuntimeError(\"Branch already exists with incorrect commit\")\n        return branch\n\n    try:\n        log.debug(\"Creating branch for %s\", branch_name)\n        return self.repo.branches.create(branch_name, commit)\n    except AlreadyExistsError as err:\n        # Branch must have been created by different processes\n        log.warning(\"Branch %s already exists\", branch_name)\n        raise VCSMultiProcessError(f\"Branch {branch_name} already exists\") from err\n\n    except OSError as err:\n        raise VCSRuntimeError(\"Unable to create branch\") from err\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCS.create_commit","title":"<code>create_commit(message, parents=None, committer_name='Oper8', committer_email='noreply@oper8.org')</code>","text":"<p>Create a commit in the repo with the files currently in the index</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>str The commit message</p> required <code>parents</code> <code>Optional[List[str]]</code> <p>Optional[List[str]] Parent commit hashes</p> <code>None</code> <code>committer_name</code> <code>str</code> <p>str The name of the committer</p> <code>'Oper8'</code> <code>committer_email</code> <code>str</code> <p>str Email address for this committer</p> <code>'noreply@oper8.org'</code> Source code in <code>oper8/vcs.py</code> <pre><code>def create_commit(\n    self,\n    message: str,\n    parents: Optional[List[str]] = None,\n    committer_name: str = \"Oper8\",\n    committer_email: str = \"noreply@oper8.org\",\n):\n    \"\"\"Create a commit in the repo with the files currently in the index\n\n    Args:\n        message: str\n            The commit message\n        parents: Optional[List[str]]\n            Parent commit hashes\n        committer_name: str\n            The name of the committer\n        committer_email: str\n            Email address for this committer\n    \"\"\"\n    parents = parents or []\n    parent_commits = []\n    for parent in parents:\n        try:\n            parent_commits.append(self.repo.get(parent))\n        except ValueError as err:\n            raise ValueError(f\"Invalid parent commit: {parent}\") from err\n    signature = Signature(committer_name, committer_email)\n    self.repo.create_commit(\n        \"HEAD\", signature, signature, message, self.repo.index.write_tree(), parents\n    )\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCS.delete_branch","title":"<code>delete_branch(branch_name)</code>","text":"<p>Delete a branch from the repo</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>str The name of the branch</p> required Source code in <code>oper8/vcs.py</code> <pre><code>def delete_branch(self, branch_name: str):\n    \"\"\"Delete a branch from the repo\n\n    Args:\n        branch_name:  str\n            The name of the branch\n    \"\"\"\n    self.repo.branches.delete(branch_name)\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCS.delete_remote","title":"<code>delete_remote(remote_name)</code>","text":"<p>Remove a remote from the repo</p> <p>Parameters:</p> Name Type Description Default <code>remote_name</code> <code>str</code> <p>str The name of the remote</p> required Source code in <code>oper8/vcs.py</code> <pre><code>def delete_remote(self, remote_name: str):\n    \"\"\"Remove a remote from the repo\n\n    Args:\n        remote_name:  str\n            The name of the remote\n    \"\"\"\n    self.repo.remotes.delete(remote_name)\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCS.delete_tag","title":"<code>delete_tag(tag_name)</code>","text":"<p>Delete a tag from the repo</p> <p>Parameters:</p> Name Type Description Default <code>tag_name</code> <code>str</code> <p>str The name of the tag</p> required Source code in <code>oper8/vcs.py</code> <pre><code>def delete_tag(self, tag_name: str):\n    \"\"\"Delete a tag from the repo\n\n    Args:\n        tag_name:  str\n            The name of the tag\n    \"\"\"\n    self.repo.references.delete(f\"refs/tags/{tag_name}\")\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCS.fetch_remote","title":"<code>fetch_remote(remote_name, refs=None, wait=True)</code>","text":"<p>Fetch content from the named remote. If no refs given, all refs are fetched.</p> <p>Parameters:</p> Name Type Description Default <code>remote_name</code> <code>str</code> <p>str The name of the remote to fetch</p> required <code>refs</code> <code>Optional[Set[str]]</code> <p>Optional[Set[str]] The refs to fetch (fetch all if not given)</p> <code>None</code> <code>wait</code> <code>bool</code> <p>bool If true, wait for fetch to complete</p> <code>True</code> Source code in <code>oper8/vcs.py</code> <pre><code>def fetch_remote(\n    self,\n    remote_name: str,\n    refs: Optional[Set[str]] = None,\n    wait: bool = True,\n):\n    \"\"\"Fetch content from the named remote. If no refs given, all refs are\n    fetched.\n\n    Args:\n        remote_name: str\n            The name of the remote to fetch\n        refs: Optional[Set[str]]\n            The refs to fetch (fetch all if not given)\n        wait: bool\n            If true, wait for fetch to complete\n    \"\"\"\n    remote = self.repo.remotes[remote_name]\n    progress = remote.fetch(list(refs or []))\n    while wait and progress.received_objects &lt; progress.total_objects:\n        time.sleep(0.1)  # pragma: no cover\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCS.get_ref","title":"<code>get_ref(refish)</code>","text":"<p>Get a git commit and reference from a shorthand string</p> <p>Parameters:</p> Name Type Description Default <code>refish</code> <code>str</code> <p>str The human readable form of a git reference like branch name or commit hash</p> required <p>Returns     commit_and_reference: Tuple[Commit,Reference]         Both a commit and reference for a given refish</p> Source code in <code>oper8/vcs.py</code> <pre><code>def get_ref(self, refish: str) -&gt; Tuple[Commit, Reference]:\n    \"\"\"Get a git commit and reference from a shorthand string\n\n    Args:\n        refish: str\n            The human readable form of a git reference like branch name\n            or commit hash\n\n    Returns\n        commit_and_reference: Tuple[Commit,Reference]\n            Both a commit and reference for a given refish\n    \"\"\"\n    try:\n        return self.repo.resolve_refish(refish)\n    except KeyError as err:\n        log.error(\"Unable to find version %s in repo\", refish)\n        raise VCSConfigError(  # pylint: disable=raise-missing-from\n            f\"Version: '{refish}' not found in repo\"\n        ) from err\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCS.list_refs","title":"<code>list_refs()</code>","text":"<p>List all of the tags and references in the repo</p> <p>Returns     ref_list: Set[str]         A set of all references' shorthand as strings</p> Source code in <code>oper8/vcs.py</code> <pre><code>def list_refs(self) -&gt; Set[str]:\n    \"\"\"List all of the tags and references in the repo\n\n    Returns\n        ref_list: Set[str]\n            A set of all references' shorthand as strings\n    \"\"\"\n    # Loop through repo tags to get each tag's short name\n    refs_set = set()\n    for ref in self.repo.references.objects:\n        refs_set.add(ref.shorthand)\n\n    return refs_set\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCSCheckoutMethod","title":"<code>VCSCheckoutMethod</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for available VCS checkout methods</p> Source code in <code>oper8/vcs.py</code> <pre><code>class VCSCheckoutMethod(Enum):\n    \"\"\"Enum for available VCS checkout methods\"\"\"\n\n    WORKTREE = \"worktree\"\n    CLONE = \"clone\"\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCSConfigError","title":"<code>VCSConfigError</code>","text":"<p>               Bases: <code>ConfigError</code></p> <p>Error for VCS Specific config exception</p> Source code in <code>oper8/vcs.py</code> <pre><code>class VCSConfigError(ConfigError):\n    \"\"\"Error for VCS Specific config exception\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCSMultiProcessError","title":"<code>VCSMultiProcessError</code>","text":"<p>               Bases: <code>PreconditionError</code></p> <p>VCS Error for when multiple git processes attempt to update the git directory at the same time</p> Source code in <code>oper8/vcs.py</code> <pre><code>class VCSMultiProcessError(PreconditionError):\n    \"\"\"VCS Error for when multiple git processes attempt to update the git directory\n    at the same time\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.vcs.VCSRuntimeError","title":"<code>VCSRuntimeError</code>","text":"<p>               Bases: <code>Oper8FatalError</code></p> <p>Error for general git exceptions</p> Source code in <code>oper8/vcs.py</code> <pre><code>class VCSRuntimeError(Oper8FatalError):\n    \"\"\"Error for general git exceptions\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.verify_resources","title":"<code>verify_resources</code>","text":"<p>This library holds common verification routines for individual kubernetes resources.</p>"},{"location":"API%20References/#oper8.verify_resources.verify_deployment","title":"<code>verify_deployment(object_state)</code>","text":"<p>Verify that all members of a deployment are ready and all members are rolled out to new version in case of update.</p> Source code in <code>oper8/verify_resources.py</code> <pre><code>def verify_deployment(object_state: dict) -&gt; bool:\n    \"\"\"Verify that all members of a deployment are ready\n    and all members are rolled out to new version in case of update.\n    \"\"\"\n    return _verify_condition(\n        object_state, AVAILABLE_CONDITION_KEY, True\n    ) and _verify_condition(\n        object_state,\n        PROGRESSING_CONDITION_KEY,\n        True,\n        expected_reason=NEW_RS_AVAILABLE_REASON,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.verify_resources.verify_job","title":"<code>verify_job(object_state)</code>","text":"<p>Verify that a job has completed successfully</p> Source code in <code>oper8/verify_resources.py</code> <pre><code>def verify_job(object_state: dict) -&gt; bool:\n    \"\"\"Verify that a job has completed successfully\"\"\"\n    # https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/job-v1/#JobStatus\n    return _verify_condition(object_state, COMPLETE_CONDITION_KEY, True)\n</code></pre>"},{"location":"API%20References/#oper8.verify_resources.verify_pod","title":"<code>verify_pod(object_state)</code>","text":"<p>Verify that a pod resources is ready</p> Source code in <code>oper8/verify_resources.py</code> <pre><code>def verify_pod(object_state: dict) -&gt; bool:\n    \"\"\"Verify that a pod resources is ready\"\"\"\n    return _verify_condition(object_state, \"Ready\", True)\n</code></pre>"},{"location":"API%20References/#oper8.verify_resources.verify_resource","title":"<code>verify_resource(kind, name, api_version, session, *, namespace=_SESSION_NAMESPACE, verify_function=None, is_subsystem=False, condition_type=None, timestamp_key=None)</code>","text":"<p>Verify a resource detailed in a ManagedObject.</p> we can't do type-hinting on the session because importing <p>DeploySession creates a circular dependency with Component. We should probably fix that...</p> <p>This function will run the appropriate verification function for the given resource kind.</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>str</code> <p>str The kind of the resource to look for</p> required <code>name</code> <code>str</code> <p>str The name of the resource to look for</p> required <code>api_version</code> <code>str</code> <p>str The api_version of the resource to look for</p> required <code>session</code> <p>DeploySession The current deployment session</p> required Kwargs <p>is_subsystem:  bool     Whether or not the given kind is an oper8 subsystem condition_type:  str     For non-standard types, this is the type name for the condition to     check for verification timestamp_key:  str     For non-standard types, this is the key in the condition to use to     sort by date</p> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool True on successful deployment verification, False on failure conditions</p> Source code in <code>oper8/verify_resources.py</code> <pre><code>def verify_resource(\n    kind: str,\n    name: str,\n    api_version: str,\n    session,\n    *,\n    # Use a predefined _SESSION_NAMESPACE default instead of None to differentiate between\n    # non-namespaced resources (which pass None) and those that use session.namespace\n    namespace: Optional[str] = _SESSION_NAMESPACE,\n    verify_function: Optional[RESOURCE_VERIFY_FUNCTION] = None,\n    is_subsystem: bool = False,\n    condition_type: Optional[str] = None,\n    timestamp_key: Optional[str] = None,\n) -&gt; bool:\n    \"\"\"Verify a resource detailed in a ManagedObject.\n\n    NOTE: we can't do type-hinting on the session because importing\n        DeploySession creates a circular dependency with Component. We should\n        probably fix that...\n\n    This function will run the appropriate verification function for the given\n    resource kind.\n\n    Args:\n        kind:  str\n            The kind of the resource to look for\n        name:  str\n            The name of the resource to look for\n        api_version:  str\n            The api_version of the resource to look for\n        session:  DeploySession\n            The current deployment session\n\n    Kwargs:\n        is_subsystem:  bool\n            Whether or not the given kind is an oper8 subsystem\n        condition_type:  str\n            For non-standard types, this is the type name for the condition to\n            check for verification\n        timestamp_key:  str\n            For non-standard types, this is the key in the condition to use to\n            sort by date\n\n    Returns:\n        success:  bool\n            True on successful deployment verification, False on failure\n            conditions\n    \"\"\"\n\n    # Configure namespace if it isn't set\n    namespace = namespace if namespace != _SESSION_NAMESPACE else session.namespace\n\n    # Get the state of the object\n    log.debug2(\"Fetching current content for [%s/%s] to verify it\", kind, name)\n    success, content = session.get_object_current_state(\n        kind=kind, name=name, api_version=api_version, namespace=namespace\n    )\n    assert success, f\"Failed to fetch state of [{kind}/{name}]\"\n\n    # If the object is not found, it is not verified\n    if not content:\n        log.debug(\"Could not find [%s/%s]. Not Ready.\", kind, name)\n        return False\n\n    # If a custom condition_type is given, then use the general condition\n    # verifier\n    if condition_type is not None:\n        log.debug(\n            \"Using custom verification for [%s/%s] with condition [%s]\",\n            kind,\n            name,\n            condition_type,\n        )\n        return _verify_condition(\n            content, condition_type, True, timestamp_key or DEFAULT_TIMESTAMP_KEY\n        )\n\n    # Run the appropriate verification function if there is one available\n    verify_fn = verify_function or _resource_verifiers.get(kind)\n    if not verify_fn and is_subsystem:\n        log.debug(\"Using oper8 subsystem verifier for [%s/%s]\", kind, name)\n        verify_fn = partial(\n            verify_subsystem,\n            desired_version=session.version,\n        )\n\n    # If a verifier was found, run it\n    if verify_fn:\n        log.debug2(\"Running [%s] verifier for [%s/%s]\", kind, kind, name)\n        return verify_fn(content)\n\n    # If no other verifier found, we consider it verified as long as it is\n    # present in the cluster\n    log.debug2(\"No kind-specific verifier for [%s/%s]\", kind, name)\n    return True\n</code></pre>"},{"location":"API%20References/#oper8.verify_resources.verify_statefulset","title":"<code>verify_statefulset(object_state)</code>","text":"<p>Verify that all desired replicas of a StatefulSet are ready</p> Source code in <code>oper8/verify_resources.py</code> <pre><code>def verify_statefulset(object_state: dict) -&gt; bool:\n    \"\"\"Verify that all desired replicas of a StatefulSet are ready\"\"\"\n    obj_status = object_state.get(\"status\", {})\n    expected_replicas = obj_status.get(\"replicas\")\n    if expected_replicas is None:\n        log.debug2(\"No replicas found in statefulset status. Not ready.\")\n        return False\n    ready_replicas = obj_status.get(\"readyReplicas\", 0)\n    return ready_replicas == expected_replicas\n</code></pre>"},{"location":"API%20References/#oper8.verify_resources.verify_subsystem","title":"<code>verify_subsystem(object_state, desired_version=None)</code>","text":"<p>Verify that an oper8-managed subsystem is ready</p> Source code in <code>oper8/verify_resources.py</code> <pre><code>def verify_subsystem(object_state: dict, desired_version: str = None) -&gt; bool:\n    \"\"\"Verify that an oper8-managed subsystem is ready\"\"\"\n\n    current_version = status.get_version(object_state.get(\"status\", {}))\n    # Once rollout finishes with verification, version status is added.\n    #   Until then, mark the subsystem as unverified.\n    if desired_version and not current_version:\n        log.debug2(\n            \"Reconciled version %s does not match desired: %s\",\n            current_version,\n            desired_version,\n        )\n        return False\n\n    return (\n        _verify_condition(\n            object_state, status.READY_CONDITION, True, status.TIMESTAMP_KEY\n        )\n        and _verify_condition(\n            object_state, status.UPDATING_CONDITION, False, status.TIMESTAMP_KEY\n        )\n        and current_version == desired_version\n    )\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager","title":"<code>watch_manager</code>","text":"<p>Top-level watch_manager imports</p>"},{"location":"API%20References/#oper8.watch_manager.ansible_watch_manager","title":"<code>ansible_watch_manager</code>","text":"<p>This module holds the ansible implementation of the WatchManager</p>"},{"location":"API%20References/#oper8.watch_manager.ansible_watch_manager.ansible_watch_manager","title":"<code>ansible_watch_manager</code>","text":"<p>Ansible-based implementation of the WatchManager</p>"},{"location":"API%20References/#oper8.watch_manager.ansible_watch_manager.ansible_watch_manager.AnsibleWatchManager","title":"<code>AnsibleWatchManager</code>","text":"<p>               Bases: <code>WatchManagerBase</code></p> <p>The AnsibleWatchManager uses the core of an ansible-based operator to manage watching resources. The key elements are:</p> <ol> <li>Manage a <code>watches.yaml</code> file for all watched resources</li> <li>Manage a playbook for each watched resource</li> <li>Manage the ansible operator's executable as a subprocess</li> </ol> Source code in <code>oper8/watch_manager/ansible_watch_manager/ansible_watch_manager.py</code> <pre><code>class AnsibleWatchManager(WatchManagerBase):\n    \"\"\"The AnsibleWatchManager uses the core of an ansible-based operator to\n    manage watching resources. The key elements are:\n\n    1. Manage a `watches.yaml` file for all watched resources\n    2. Manage a playbook for each watched resource\n    3. Manage the ansible operator's executable as a subprocess\n    \"\"\"\n\n    # Shared singleton process used to manage all watches via ansible\n    ANSIBLE_PROCESS = None\n\n    # Defaults for initialization args held separately to allow for override\n    # precedence order\n    _DEFAULT_INIT_KWARGS = {\n        \"ansible_base_path\": \"/opt/ansible\",\n        \"ansible_entrypoint\": DEFAULT_ENTRYPOINT,\n        \"ansible_args\": \"\",\n        \"manage_status\": False,\n        \"watch_dependent_resources\": False,\n        \"reconcile_period\": \"10m\",\n        \"playbook_parameters\": None,\n    }\n\n    def __init__(\n        self,\n        controller_type: Type[Controller],\n        *,\n        ansible_base_path: Optional[str] = None,\n        ansible_entrypoint: Optional[str] = None,\n        ansible_args: Optional[str] = None,\n        manage_status: Optional[bool] = None,\n        watch_dependent_resources: Optional[bool] = None,\n        reconcile_period: Optional[str] = None,\n        playbook_parameters: Optional[dict] = None,\n    ):\n        \"\"\"Construct with the core watch binding and configuration args for the\n        watches.yaml and playbook.yaml files.\n\n        NOTE: All args may be overridden in the `ansible_watch_manager` section\n            of the library config. The precedence order is:\n\n        1. Directly passed arguments\n        2. Config values\n        3. Default values from code\n\n        A passed None value in any of these is considered \"unset\"\n\n        Args:\n            controller_type:  Type[Controller],\n                The Controller type that will manage this group/version/kind\n\n        Kwargs:\n            ansible_base_path:  str\n                The base path where the ansible runtime will be run. This is\n                also used to determine where the watches.yaml and playbooks will\n                be managed.\n            ansible_entrypoint:  str\n                The command to use to run ansible\n            ansible_args: str\n                Additional flags to be passed to `ansible_entrypoint`\n            manage_status:  bool\n                Whether or not to let ansible manage status on the CR\n            watch_dependent_resources:  bool\n                Whether or not to trigger a reconciliation on change to\n                dependent resources.\n            reconcile_period:  str\n                String representation of the time duration to use for periodic\n                reconciliations\n            playbook_parameters:  dict\n                Parameters to use to configure the k8s_application module in the\n                playbook\n        \"\"\"\n        # Make sure that the shared ansible process is not already started\n        assert (\n            self.ANSIBLE_PROCESS is None\n        ), \"Cannot create an AnsibleWatchManager after starting another AnsibleWatchManager\"\n\n        # Set up the function arguments based on override precedence\n        ansible_base_path = self._init_arg(\"ansible_base_path\", ansible_base_path, str)\n        ansible_entrypoint = self._init_arg(\n            \"ansible_entrypoint\", ansible_entrypoint, str\n        )\n        ansible_args = self._init_arg(\"ansible_args\", ansible_args, str)\n        manage_status = self._init_arg(\"manage_status\", manage_status, bool)\n        watch_dependent_resources = self._init_arg(\n            \"watch_dependent_resources\", watch_dependent_resources, bool\n        )\n        reconcile_period = self._init_arg(\"reconcile_period\", reconcile_period, str)\n        playbook_parameters = self._init_arg(\n            \"playbook_parameters\", playbook_parameters, dict\n        )\n\n        super().__init__(controller_type)\n        self._ansible_base_path = ansible_base_path\n        self._ansible_entrypoint = ansible_entrypoint\n        self._ansible_args = ansible_args\n\n        # Create the playbook\n        playbook_path = self._add_playbook(playbook_parameters)\n\n        # Create the entry in the watches.yaml\n        self._add_watch_entry(\n            playbook_path=playbook_path,\n            manage_status=manage_status,\n            watch_dependent_resources=watch_dependent_resources,\n            reconcile_period=reconcile_period,\n            add_finalizer=controller_type.has_finalizer,\n            disable_vcs=getattr(controller_type, \"disable_vcs\", None),\n        )\n\n    ## Interface ###############################################################\n\n    def watch(self) -&gt; bool:\n        \"\"\"Start the global ansible process if not already started\n\n        NOTE: This is intentionally not thread safe! The watches should all be\n            managed from the primary entrypoint thread.\n\n        Returns:\n            success:  bool\n                True if the asible process is running correctly\n        \"\"\"\n        cls = self.__class__\n        if cls.ANSIBLE_PROCESS is None:\n            log.info(\"Starting ansible watch process\")\n            env = copy.deepcopy(os.environ)\n            env[\"ANSIBLE_LIBRARY\"] = self._ansible_library_path()\n            env[\"ANSIBLE_ROLES_PATH\"] = self._ansible_roles_path()\n            cls.ANSIBLE_PROCESS = (\n                subprocess.Popen(  # pylint: disable=consider-using-with\n                    shlex.split(\n                        \" \".join((self._ansible_entrypoint, self._ansible_args)).strip()\n                    ),\n                    cwd=self._ansible_base_path,\n                    env=env,\n                )\n            )\n\n        # If the process does not have a returncode on poll, it's up. This is a\n        # point-in-time statement. There's no way for this code to actually\n        # validate the state of the process since it may crash at any\n        # indeterminate time after starting.\n        return self.ANSIBLE_PROCESS.poll() is None\n\n    def wait(self):\n        \"\"\"Wait for the ansible process to terminate\"\"\"\n        if self.ANSIBLE_PROCESS is not None:\n            self.ANSIBLE_PROCESS.wait()\n\n    def stop(self):\n        \"\"\"Attempt to terminate the ansible process. This asserts that the\n        process has been created in order to avoid race conditions with a None\n        check.\n        \"\"\"\n        assert self.ANSIBLE_PROCESS is not None, \"Cannot stop before watching\"\n        log.info(\"Killing shared ansible process\")\n        self.ANSIBLE_PROCESS.terminate()\n        kill_start_time = time.time()\n        while (\n            self.ANSIBLE_PROCESS.poll() is None\n            and time.time() - kill_start_time\n            &lt; config.ansible_watch_manager.kill_max_wait\n        ):\n            time.sleep(0.001)\n        assert (\n            self.ANSIBLE_PROCESS.poll() is not None\n        ), \"The only way to shut down ansible is with a sledge hammer!\"\n\n    ## Implementation Details ##################################################\n\n    @classmethod\n    def _init_arg(cls, arg_name, passed_value, arg_type):\n        \"\"\"Helper to enforce init arg precedence\"\"\"\n        if passed_value is not None:\n            return passed_value\n        config_value = config.ansible_watch_manager.get(arg_name)\n        if config_value is not None:\n            if arg_type is not None and not isinstance(config_value, arg_type):\n                assert_config(\n                    isinstance(config_value, str),\n                    f\"Invalid type for ansible_watch_manager.{arg_name}: \"\n                    + \"{type(config_value)} should be {arg_type}\",\n                )\n                if arg_type is bool:\n                    config_value = config_value.lower() == \"true\"\n                elif arg_type is dict:\n                    config_value = json.loads(config_value)\n                assert_config(\n                    isinstance(config_value, arg_type),\n                    f\"Cannot convert ansible_watch_manager.{arg_name} from str to {arg_type}\",\n                )\n            return config_value\n        assert (\n            arg_name in cls._DEFAULT_INIT_KWARGS\n        ), f\"Programming Error: Unsupported init kwarg: {arg_name}\"\n        return cls._DEFAULT_INIT_KWARGS[arg_name]\n\n    def _add_playbook(self, playbook_parameters):\n        \"\"\"Create a playbook for this watch\"\"\"\n\n        # Open the base template for playbooks\n        playbook_base_path = os.path.join(\n            self._resources_path(),\n            \"playbook-base.yaml\",\n        )\n        with open(playbook_base_path, encoding=\"utf-8\") as handle:\n            playbook_base = yaml.safe_load(handle)\n\n        # Add the provided variables\n        module_vars = playbook_parameters or {}\n        module_vars.setdefault(\"strict_versioning\", False)\n        kind = self.controller_type.kind.lower()\n        log_file = f\"{kind}.{{{{ ansible_operator_meta.name }}}}.log\"\n        log_dir = config.ansible_watch_manager.log_file_dir\n        if log_dir is not None:\n            log.debug2(\"Adding log dir: %s\", log_dir)\n            log_file = os.path.join(log_dir, log_file)\n        module_vars.setdefault(\"log_file\", log_file)\n        playbook_base[0][\"tasks\"][0][\"vars\"] = module_vars\n\n        # Add the controller_class\n        controller_class = (\n            f\"{self.controller_type.__module__}.{self.controller_type.__name__}\"\n        )\n        log.debug3(\"controller_class: %s\", controller_class)\n        module_vars[\"controller_class\"] = controller_class\n\n        # Add the full_cr template\n        group_template = self.group.lower().replace(\".\", \"_\").replace(\"-\", \"_\")\n        cr_template = f\"{{{{ _{group_template}_{self.kind.lower()} }}}}\"\n        module_vars[\"full_cr\"] = cr_template\n\n        # Write it out to the right place\n        log.debug3(\n            \"%s/%s/%s playbook vars: %s\",\n            self.group,\n            self.version,\n            self.kind,\n            module_vars,\n        )\n        playbook_path = os.path.join(\n            self._ansible_base_path, f\"playbook-{self.kind.lower()}.yaml\"\n        )\n        with open(playbook_path, \"w\", encoding=\"utf-8\") as handle:\n            yaml.dump(playbook_base, handle)\n        return playbook_path\n\n    def _add_watch_entry(  # pylint: disable=too-many-arguments\n        self,\n        playbook_path: str,\n        manage_status: bool,\n        watch_dependent_resources: bool,\n        reconcile_period: str,\n        add_finalizer: bool,\n        disable_vcs: Optional[bool],\n    ):\n        \"\"\"Add an entry to the watches.yaml file, creating it if needed\"\"\"\n\n        # Load the current watches.yaml content, or start fresh\n        watches_path = os.path.join(self._ansible_base_path, \"watches.yaml\")\n        if os.path.exists(watches_path):\n            with open(watches_path, encoding=\"utf-8\") as handle:\n                watches = yaml.safe_load(handle)\n        else:\n            watches = []\n\n        # Make sure there is not already an entry for this watch\n        matches = [\n            (\n                watch_entry[\"group\"] == self.group\n                and watch_entry[\"version\"] == self.version\n                and watch_entry[\"kind\"] == self.kind\n            )\n            for watch_entry in watches\n        ]\n        assert True not in matches, (\n            \"Can't have multiple watch entries for the same group/version/kind! \"\n            + f\"{self.group}/{self.version}/{self.kind}\"\n        )\n        log.debug2(\"Adding new watch for %s\", self)\n        watch_entry = {\n            \"group\": self.group,\n            \"version\": self.version,\n            \"kind\": self.kind,\n            \"vars\": {\"operation\": \"add\"},\n        }\n        if disable_vcs is not None:\n            str_val = str(not disable_vcs).lower()\n            log.debug(\n                \"Adding watch variable [enable_ansible_vcs = '%s'] for %s/%s/%s\",\n                str_val,\n                self.group,\n                self.version,\n                self.kind,\n            )\n            watch_entry[\"vars\"][\"enable_ansible_vcs\"] = str_val\n        watches.append(watch_entry)\n\n        # Update the watch entry with the configuration for this watch\n        watch_entry[\"playbook\"] = playbook_path\n        watch_entry[\"manageStatus\"] = manage_status\n        watch_entry[\"watchDependentResources\"] = watch_dependent_resources\n        watch_entry[\"reconcilePeriod\"] = reconcile_period\n\n        # If requested, add a version of the watch that manages the finalizer\n        if add_finalizer:\n            finalizer_name = self.controller_type.finalizer\n            log.debug2(\"Adding finalizer: %s\", finalizer_name)\n            watch_entry[\"finalizer\"] = {\n                \"name\": finalizer_name,\n                \"vars\": {\"operation\": \"remove\"},\n            }\n\n        # Write the watches.yaml file back out\n        with open(watches_path, \"w\", encoding=\"utf-8\") as handle:\n            yaml.dump(watches, handle)\n\n    @staticmethod\n    def _resources_path():\n        \"\"\"Get the path to the static resources for ansible\"\"\"\n        return os.path.realpath(os.path.join(os.path.dirname(__file__), \"resources\"))\n\n    @staticmethod\n    def _ansible_library_path():\n        \"\"\"Get the absolute path to the ansible library with the k8s_applicaiton\n        module\n        \"\"\"\n        return os.path.realpath(os.path.join(os.path.dirname(__file__), \"modules\"))\n\n    @classmethod\n    def _ansible_roles_path(cls):\n        \"\"\"Get the absolute path to the ansible roles\"\"\"\n        return os.path.join(cls._resources_path(), \"roles\")\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.ansible_watch_manager.ansible_watch_manager.AnsibleWatchManager.__init__","title":"<code>__init__(controller_type, *, ansible_base_path=None, ansible_entrypoint=None, ansible_args=None, manage_status=None, watch_dependent_resources=None, reconcile_period=None, playbook_parameters=None)</code>","text":"<p>Construct with the core watch binding and configuration args for the watches.yaml and playbook.yaml files.</p> All args may be overridden in the <code>ansible_watch_manager</code> section <p>of the library config. The precedence order is:</p> <ol> <li>Directly passed arguments</li> <li>Config values</li> <li>Default values from code</li> </ol> <p>A passed None value in any of these is considered \"unset\"</p> <p>Parameters:</p> Name Type Description Default <code>controller_type</code> <code>Type[Controller]</code> <p>Type[Controller], The Controller type that will manage this group/version/kind</p> required Kwargs <p>ansible_base_path:  str     The base path where the ansible runtime will be run. This is     also used to determine where the watches.yaml and playbooks will     be managed. ansible_entrypoint:  str     The command to use to run ansible ansible_args: str     Additional flags to be passed to <code>ansible_entrypoint</code> manage_status:  bool     Whether or not to let ansible manage status on the CR watch_dependent_resources:  bool     Whether or not to trigger a reconciliation on change to     dependent resources. reconcile_period:  str     String representation of the time duration to use for periodic     reconciliations playbook_parameters:  dict     Parameters to use to configure the k8s_application module in the     playbook</p> Source code in <code>oper8/watch_manager/ansible_watch_manager/ansible_watch_manager.py</code> <pre><code>def __init__(\n    self,\n    controller_type: Type[Controller],\n    *,\n    ansible_base_path: Optional[str] = None,\n    ansible_entrypoint: Optional[str] = None,\n    ansible_args: Optional[str] = None,\n    manage_status: Optional[bool] = None,\n    watch_dependent_resources: Optional[bool] = None,\n    reconcile_period: Optional[str] = None,\n    playbook_parameters: Optional[dict] = None,\n):\n    \"\"\"Construct with the core watch binding and configuration args for the\n    watches.yaml and playbook.yaml files.\n\n    NOTE: All args may be overridden in the `ansible_watch_manager` section\n        of the library config. The precedence order is:\n\n    1. Directly passed arguments\n    2. Config values\n    3. Default values from code\n\n    A passed None value in any of these is considered \"unset\"\n\n    Args:\n        controller_type:  Type[Controller],\n            The Controller type that will manage this group/version/kind\n\n    Kwargs:\n        ansible_base_path:  str\n            The base path where the ansible runtime will be run. This is\n            also used to determine where the watches.yaml and playbooks will\n            be managed.\n        ansible_entrypoint:  str\n            The command to use to run ansible\n        ansible_args: str\n            Additional flags to be passed to `ansible_entrypoint`\n        manage_status:  bool\n            Whether or not to let ansible manage status on the CR\n        watch_dependent_resources:  bool\n            Whether or not to trigger a reconciliation on change to\n            dependent resources.\n        reconcile_period:  str\n            String representation of the time duration to use for periodic\n            reconciliations\n        playbook_parameters:  dict\n            Parameters to use to configure the k8s_application module in the\n            playbook\n    \"\"\"\n    # Make sure that the shared ansible process is not already started\n    assert (\n        self.ANSIBLE_PROCESS is None\n    ), \"Cannot create an AnsibleWatchManager after starting another AnsibleWatchManager\"\n\n    # Set up the function arguments based on override precedence\n    ansible_base_path = self._init_arg(\"ansible_base_path\", ansible_base_path, str)\n    ansible_entrypoint = self._init_arg(\n        \"ansible_entrypoint\", ansible_entrypoint, str\n    )\n    ansible_args = self._init_arg(\"ansible_args\", ansible_args, str)\n    manage_status = self._init_arg(\"manage_status\", manage_status, bool)\n    watch_dependent_resources = self._init_arg(\n        \"watch_dependent_resources\", watch_dependent_resources, bool\n    )\n    reconcile_period = self._init_arg(\"reconcile_period\", reconcile_period, str)\n    playbook_parameters = self._init_arg(\n        \"playbook_parameters\", playbook_parameters, dict\n    )\n\n    super().__init__(controller_type)\n    self._ansible_base_path = ansible_base_path\n    self._ansible_entrypoint = ansible_entrypoint\n    self._ansible_args = ansible_args\n\n    # Create the playbook\n    playbook_path = self._add_playbook(playbook_parameters)\n\n    # Create the entry in the watches.yaml\n    self._add_watch_entry(\n        playbook_path=playbook_path,\n        manage_status=manage_status,\n        watch_dependent_resources=watch_dependent_resources,\n        reconcile_period=reconcile_period,\n        add_finalizer=controller_type.has_finalizer,\n        disable_vcs=getattr(controller_type, \"disable_vcs\", None),\n    )\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.ansible_watch_manager.ansible_watch_manager.AnsibleWatchManager.stop","title":"<code>stop()</code>","text":"<p>Attempt to terminate the ansible process. This asserts that the process has been created in order to avoid race conditions with a None check.</p> Source code in <code>oper8/watch_manager/ansible_watch_manager/ansible_watch_manager.py</code> <pre><code>def stop(self):\n    \"\"\"Attempt to terminate the ansible process. This asserts that the\n    process has been created in order to avoid race conditions with a None\n    check.\n    \"\"\"\n    assert self.ANSIBLE_PROCESS is not None, \"Cannot stop before watching\"\n    log.info(\"Killing shared ansible process\")\n    self.ANSIBLE_PROCESS.terminate()\n    kill_start_time = time.time()\n    while (\n        self.ANSIBLE_PROCESS.poll() is None\n        and time.time() - kill_start_time\n        &lt; config.ansible_watch_manager.kill_max_wait\n    ):\n        time.sleep(0.001)\n    assert (\n        self.ANSIBLE_PROCESS.poll() is not None\n    ), \"The only way to shut down ansible is with a sledge hammer!\"\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.ansible_watch_manager.ansible_watch_manager.AnsibleWatchManager.wait","title":"<code>wait()</code>","text":"<p>Wait for the ansible process to terminate</p> Source code in <code>oper8/watch_manager/ansible_watch_manager/ansible_watch_manager.py</code> <pre><code>def wait(self):\n    \"\"\"Wait for the ansible process to terminate\"\"\"\n    if self.ANSIBLE_PROCESS is not None:\n        self.ANSIBLE_PROCESS.wait()\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.ansible_watch_manager.ansible_watch_manager.AnsibleWatchManager.watch","title":"<code>watch()</code>","text":"<p>Start the global ansible process if not already started</p> This is intentionally not thread safe! The watches should all be <p>managed from the primary entrypoint thread.</p> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool True if the asible process is running correctly</p> Source code in <code>oper8/watch_manager/ansible_watch_manager/ansible_watch_manager.py</code> <pre><code>def watch(self) -&gt; bool:\n    \"\"\"Start the global ansible process if not already started\n\n    NOTE: This is intentionally not thread safe! The watches should all be\n        managed from the primary entrypoint thread.\n\n    Returns:\n        success:  bool\n            True if the asible process is running correctly\n    \"\"\"\n    cls = self.__class__\n    if cls.ANSIBLE_PROCESS is None:\n        log.info(\"Starting ansible watch process\")\n        env = copy.deepcopy(os.environ)\n        env[\"ANSIBLE_LIBRARY\"] = self._ansible_library_path()\n        env[\"ANSIBLE_ROLES_PATH\"] = self._ansible_roles_path()\n        cls.ANSIBLE_PROCESS = (\n            subprocess.Popen(  # pylint: disable=consider-using-with\n                shlex.split(\n                    \" \".join((self._ansible_entrypoint, self._ansible_args)).strip()\n                ),\n                cwd=self._ansible_base_path,\n                env=env,\n            )\n        )\n\n    # If the process does not have a returncode on poll, it's up. This is a\n    # point-in-time statement. There's no way for this code to actually\n    # validate the state of the process since it may crash at any\n    # indeterminate time after starting.\n    return self.ANSIBLE_PROCESS.poll() is None\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.base","title":"<code>base</code>","text":"<p>This module holds the base class interface for the various implementations of WatchManager</p>"},{"location":"API%20References/#oper8.watch_manager.base.WatchManagerBase","title":"<code>WatchManagerBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A WatchManager is responsible for linking a kubernetes custom resource type with a Controller that will execute the reconciliation loop</p> Source code in <code>oper8/watch_manager/base.py</code> <pre><code>class WatchManagerBase(abc.ABC):\n    \"\"\"A WatchManager is responsible for linking a kubernetes custom resource\n    type with a Controller that will execute the reconciliation loop\n    \"\"\"\n\n    # Class-global mapping of all watches managed by this operator\n    _ALL_WATCHES = {}\n\n    ## Interface ###############################################################\n\n    def __init__(\n        self,\n        controller_type: Type[Controller],\n    ):\n        \"\"\"Construct with the controller type that will be watched\n\n        Args:\n            controller_type:  Type[Controller],\n                The Controller instance that will manage this group/version/kind\n        \"\"\"\n        self.controller_type = controller_type\n        self.group = controller_type.group\n        self.version = controller_type.version\n        self.kind = controller_type.kind\n\n        # Register this watch instance\n        watch_key = str(self)\n        assert (\n            watch_key not in self._ALL_WATCHES\n        ), \"Only a single controller may watch a given group/version/kind\"\n        self._ALL_WATCHES[watch_key] = self\n\n    @abc.abstractmethod\n    def watch(self) -&gt; bool:\n        \"\"\"The watch function is responsible for initializing the persistent\n        watch and returning whether or not the watch was started successfully.\n\n        Returns:\n            success:  bool\n                True if the watch was spawned correctly, False otherwise.\n        \"\"\"\n\n    @abc.abstractmethod\n    def wait(self):\n        \"\"\"The wait function is responsible for blocking until the managed watch\n        has been terminated.\n        \"\"\"\n\n    @abc.abstractmethod\n    def stop(self):\n        \"\"\"Terminate this watch if it is currently running\"\"\"\n\n    ## Utilities ###############################################################\n\n    @classmethod\n    def start_all(cls) -&gt; bool:\n        \"\"\"This utility starts all registered watches\n\n        Returns:\n            success:  bool\n                True if all watches started succssfully, False otherwise\n        \"\"\"\n        started_watches = []\n        success = True\n        # NOTE: sorting gives deterministic order so that launch failures can be\n        #   diagnosed (and tested) more easily. This is not strictly necessary,\n        #   but it also doesn't hurt and it is nice to have.\n        for _, watch in sorted(cls._ALL_WATCHES.items()):\n            if watch.watch():\n                log.debug(\"Successfully started %s\", watch)\n                started_watches.append(watch)\n            else:\n                log.warning(\"Failed to start %s\", watch)\n                success = False\n\n                # Shut down all successfully started watches\n                for started_watch in started_watches:\n                    started_watch.stop()\n\n                # Don't start any of the others\n                break\n\n        # Wait on all of them to terminate\n        for watch in cls._ALL_WATCHES.values():\n            watch.wait()\n\n        return success\n\n    @classmethod\n    def stop_all(cls):\n        \"\"\"This utility stops all watches\"\"\"\n        for watch in cls._ALL_WATCHES.values():\n            try:\n                watch.stop()\n                log.debug2(\"Waiting for %s to terminate\", watch)\n                watch.wait()\n            except Exception as exc:  # pylint: disable=broad-exception-caught\n                log.error(\"Failed to stop watch manager %s\", exc, exc_info=True)\n\n    ## Implementation Details ##################################################\n\n    def __str__(self):\n        \"\"\"String representation of this watch\"\"\"\n        return f\"Watch[{self.controller_type}]\"\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.base.WatchManagerBase.__init__","title":"<code>__init__(controller_type)</code>","text":"<p>Construct with the controller type that will be watched</p> <p>Parameters:</p> Name Type Description Default <code>controller_type</code> <code>Type[Controller]</code> <p>Type[Controller], The Controller instance that will manage this group/version/kind</p> required Source code in <code>oper8/watch_manager/base.py</code> <pre><code>def __init__(\n    self,\n    controller_type: Type[Controller],\n):\n    \"\"\"Construct with the controller type that will be watched\n\n    Args:\n        controller_type:  Type[Controller],\n            The Controller instance that will manage this group/version/kind\n    \"\"\"\n    self.controller_type = controller_type\n    self.group = controller_type.group\n    self.version = controller_type.version\n    self.kind = controller_type.kind\n\n    # Register this watch instance\n    watch_key = str(self)\n    assert (\n        watch_key not in self._ALL_WATCHES\n    ), \"Only a single controller may watch a given group/version/kind\"\n    self._ALL_WATCHES[watch_key] = self\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.base.WatchManagerBase.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this watch</p> Source code in <code>oper8/watch_manager/base.py</code> <pre><code>def __str__(self):\n    \"\"\"String representation of this watch\"\"\"\n    return f\"Watch[{self.controller_type}]\"\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.base.WatchManagerBase.start_all","title":"<code>start_all()</code>  <code>classmethod</code>","text":"<p>This utility starts all registered watches</p> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool True if all watches started succssfully, False otherwise</p> Source code in <code>oper8/watch_manager/base.py</code> <pre><code>@classmethod\ndef start_all(cls) -&gt; bool:\n    \"\"\"This utility starts all registered watches\n\n    Returns:\n        success:  bool\n            True if all watches started succssfully, False otherwise\n    \"\"\"\n    started_watches = []\n    success = True\n    # NOTE: sorting gives deterministic order so that launch failures can be\n    #   diagnosed (and tested) more easily. This is not strictly necessary,\n    #   but it also doesn't hurt and it is nice to have.\n    for _, watch in sorted(cls._ALL_WATCHES.items()):\n        if watch.watch():\n            log.debug(\"Successfully started %s\", watch)\n            started_watches.append(watch)\n        else:\n            log.warning(\"Failed to start %s\", watch)\n            success = False\n\n            # Shut down all successfully started watches\n            for started_watch in started_watches:\n                started_watch.stop()\n\n            # Don't start any of the others\n            break\n\n    # Wait on all of them to terminate\n    for watch in cls._ALL_WATCHES.values():\n        watch.wait()\n\n    return success\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.base.WatchManagerBase.stop","title":"<code>stop()</code>  <code>abstractmethod</code>","text":"<p>Terminate this watch if it is currently running</p> Source code in <code>oper8/watch_manager/base.py</code> <pre><code>@abc.abstractmethod\ndef stop(self):\n    \"\"\"Terminate this watch if it is currently running\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.base.WatchManagerBase.stop_all","title":"<code>stop_all()</code>  <code>classmethod</code>","text":"<p>This utility stops all watches</p> Source code in <code>oper8/watch_manager/base.py</code> <pre><code>@classmethod\ndef stop_all(cls):\n    \"\"\"This utility stops all watches\"\"\"\n    for watch in cls._ALL_WATCHES.values():\n        try:\n            watch.stop()\n            log.debug2(\"Waiting for %s to terminate\", watch)\n            watch.wait()\n        except Exception as exc:  # pylint: disable=broad-exception-caught\n            log.error(\"Failed to stop watch manager %s\", exc, exc_info=True)\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.base.WatchManagerBase.wait","title":"<code>wait()</code>  <code>abstractmethod</code>","text":"<p>The wait function is responsible for blocking until the managed watch has been terminated.</p> Source code in <code>oper8/watch_manager/base.py</code> <pre><code>@abc.abstractmethod\ndef wait(self):\n    \"\"\"The wait function is responsible for blocking until the managed watch\n    has been terminated.\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.base.WatchManagerBase.watch","title":"<code>watch()</code>  <code>abstractmethod</code>","text":"<p>The watch function is responsible for initializing the persistent watch and returning whether or not the watch was started successfully.</p> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool True if the watch was spawned correctly, False otherwise.</p> Source code in <code>oper8/watch_manager/base.py</code> <pre><code>@abc.abstractmethod\ndef watch(self) -&gt; bool:\n    \"\"\"The watch function is responsible for initializing the persistent\n    watch and returning whether or not the watch was started successfully.\n\n    Returns:\n        success:  bool\n            True if the watch was spawned correctly, False otherwise.\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.dry_run_watch_manager","title":"<code>dry_run_watch_manager</code>","text":"<p>Dry run implementation of the WatchManager abstraction</p>"},{"location":"API%20References/#oper8.watch_manager.dry_run_watch_manager.DryRunWatchManager","title":"<code>DryRunWatchManager</code>","text":"<p>               Bases: <code>WatchManagerBase</code></p> <p>The DryRunWatchManager implements the WatchManagerBase interface with using a single shared DryRunDeployManager to manage an in-memory representation of the cluster.</p> Source code in <code>oper8/watch_manager/dry_run_watch_manager.py</code> <pre><code>class DryRunWatchManager(WatchManagerBase):\n    \"\"\"\n    The DryRunWatchManager implements the WatchManagerBase interface with using\n    a single shared DryRunDeployManager to manage an in-memory representation of\n    the cluster.\n    \"\"\"\n\n    reconcile_manager = None\n\n    def __init__(\n        self,\n        controller_type: Type[Controller],\n        deploy_manager: Optional[DryRunDeployManager] = None,\n    ):\n        \"\"\"Construct with the type of controller to watch and optionally a\n        deploy_manager instance. A deploy_manager will be constructed if none is\n        given.\n\n        Args:\n            controller_type:  Type[Controller]\n                The class for the controller that will be watched\n            deploy_manager:  Optional[DryRunWatchManager]\n                If given, this deploy_manager will be used. This allows for\n                there to be pre-populated resources. Note that it _must_ be a\n                DryRunDeployManager (or child class) that supports registering\n                watches.\n        \"\"\"\n        super().__init__(controller_type)\n\n        # Set up the deploy manager\n        self._deploy_manager = deploy_manager or DryRunDeployManager()\n\n        # We lazily initialize the controller instance in watch and _resource in run_reconcile\n        self._controller = None\n        self._resource = {}\n\n        # We initialize the reconcile_manager instance on first watch creation\n        if not self.reconcile_manager:\n            self.reconcile_manager = ReconcileManager(\n                deploy_manager=self._deploy_manager, reimport_controller=False\n            )\n\n    def watch(self) -&gt; bool:\n        \"\"\"Register the watch with the deploy manager\"\"\"\n        if self._controller is not None:\n            log.warning(\"Cannot watch multiple times!\")\n            return False\n\n        log.debug(\"Registering %s with the DeployManager\", self.controller_type)\n\n        # Construct controller\n        self._controller = self.controller_type()\n\n        # Register watch and finalizers\n        api_version = f\"{self.group}/{self.version}\"\n        self._deploy_manager.register_watch(\n            api_version=api_version,\n            kind=self.kind,\n            callback=partial(self.run_reconcile, False),\n        )\n        if self.controller_type.has_finalizer:\n            log.debug(\"Registering finalizer\")\n            self._deploy_manager.register_finalizer(\n                api_version=api_version,\n                kind=self.kind,\n                callback=partial(self.run_reconcile, True),\n            )\n\n        return True\n\n    def wait(self):\n        \"\"\"There is nothing to do in wait\"\"\"\n\n    def stop(self):\n        \"\"\"There is nothing to do in stop\"\"\"\n\n    def run_reconcile(self, is_finalizer: bool, resource: dict):\n        \"\"\"Wrapper function to simplify parameter/partial mapping\"\"\"\n        if not self.reconcile_manager:\n            return\n\n        # Only run reconcile if it's a unique resource\n        resource_metadata = self._resource.get(\"metadata\", {})\n        if (\n            self._resource.get(\"kind\") == resource.get(\"kind\")\n            and self._resource.get(\"apiVersion\") == resource.get(\"apiVersion\")\n            and resource_metadata.get(\"name\")\n            == resource.get(\"metadata\", {}).get(\"name\")\n            and resource_metadata.get(\"namespace\")\n            == resource.get(\"metadata\", {}).get(\"namespace\")\n        ):\n            return\n\n        # Save the current resource and log handlers then restore it after the reconcile\n        # is completed\n        log_formatters = {}\n        for handler in logging.getLogger().handlers:\n            log_formatters[handler] = handler.formatter\n        current_resource = self._resource\n        self._resource = resource\n\n        self.reconcile_manager.reconcile(self._controller, resource, is_finalizer)\n        self._resource = current_resource\n        for handler, formatter in log_formatters.items():\n            handler.setFormatter(formatter)\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.dry_run_watch_manager.DryRunWatchManager.__init__","title":"<code>__init__(controller_type, deploy_manager=None)</code>","text":"<p>Construct with the type of controller to watch and optionally a deploy_manager instance. A deploy_manager will be constructed if none is given.</p> <p>Parameters:</p> Name Type Description Default <code>controller_type</code> <code>Type[Controller]</code> <p>Type[Controller] The class for the controller that will be watched</p> required <code>deploy_manager</code> <code>Optional[DryRunDeployManager]</code> <p>Optional[DryRunWatchManager] If given, this deploy_manager will be used. This allows for there to be pre-populated resources. Note that it must be a DryRunDeployManager (or child class) that supports registering watches.</p> <code>None</code> Source code in <code>oper8/watch_manager/dry_run_watch_manager.py</code> <pre><code>def __init__(\n    self,\n    controller_type: Type[Controller],\n    deploy_manager: Optional[DryRunDeployManager] = None,\n):\n    \"\"\"Construct with the type of controller to watch and optionally a\n    deploy_manager instance. A deploy_manager will be constructed if none is\n    given.\n\n    Args:\n        controller_type:  Type[Controller]\n            The class for the controller that will be watched\n        deploy_manager:  Optional[DryRunWatchManager]\n            If given, this deploy_manager will be used. This allows for\n            there to be pre-populated resources. Note that it _must_ be a\n            DryRunDeployManager (or child class) that supports registering\n            watches.\n    \"\"\"\n    super().__init__(controller_type)\n\n    # Set up the deploy manager\n    self._deploy_manager = deploy_manager or DryRunDeployManager()\n\n    # We lazily initialize the controller instance in watch and _resource in run_reconcile\n    self._controller = None\n    self._resource = {}\n\n    # We initialize the reconcile_manager instance on first watch creation\n    if not self.reconcile_manager:\n        self.reconcile_manager = ReconcileManager(\n            deploy_manager=self._deploy_manager, reimport_controller=False\n        )\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.dry_run_watch_manager.DryRunWatchManager.run_reconcile","title":"<code>run_reconcile(is_finalizer, resource)</code>","text":"<p>Wrapper function to simplify parameter/partial mapping</p> Source code in <code>oper8/watch_manager/dry_run_watch_manager.py</code> <pre><code>def run_reconcile(self, is_finalizer: bool, resource: dict):\n    \"\"\"Wrapper function to simplify parameter/partial mapping\"\"\"\n    if not self.reconcile_manager:\n        return\n\n    # Only run reconcile if it's a unique resource\n    resource_metadata = self._resource.get(\"metadata\", {})\n    if (\n        self._resource.get(\"kind\") == resource.get(\"kind\")\n        and self._resource.get(\"apiVersion\") == resource.get(\"apiVersion\")\n        and resource_metadata.get(\"name\")\n        == resource.get(\"metadata\", {}).get(\"name\")\n        and resource_metadata.get(\"namespace\")\n        == resource.get(\"metadata\", {}).get(\"namespace\")\n    ):\n        return\n\n    # Save the current resource and log handlers then restore it after the reconcile\n    # is completed\n    log_formatters = {}\n    for handler in logging.getLogger().handlers:\n        log_formatters[handler] = handler.formatter\n    current_resource = self._resource\n    self._resource = resource\n\n    self.reconcile_manager.reconcile(self._controller, resource, is_finalizer)\n    self._resource = current_resource\n    for handler, formatter in log_formatters.items():\n        handler.setFormatter(formatter)\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.dry_run_watch_manager.DryRunWatchManager.stop","title":"<code>stop()</code>","text":"<p>There is nothing to do in stop</p> Source code in <code>oper8/watch_manager/dry_run_watch_manager.py</code> <pre><code>def stop(self):\n    \"\"\"There is nothing to do in stop\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.dry_run_watch_manager.DryRunWatchManager.wait","title":"<code>wait()</code>","text":"<p>There is nothing to do in wait</p> Source code in <code>oper8/watch_manager/dry_run_watch_manager.py</code> <pre><code>def wait(self):\n    \"\"\"There is nothing to do in wait\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.dry_run_watch_manager.DryRunWatchManager.watch","title":"<code>watch()</code>","text":"<p>Register the watch with the deploy manager</p> Source code in <code>oper8/watch_manager/dry_run_watch_manager.py</code> <pre><code>def watch(self) -&gt; bool:\n    \"\"\"Register the watch with the deploy manager\"\"\"\n    if self._controller is not None:\n        log.warning(\"Cannot watch multiple times!\")\n        return False\n\n    log.debug(\"Registering %s with the DeployManager\", self.controller_type)\n\n    # Construct controller\n    self._controller = self.controller_type()\n\n    # Register watch and finalizers\n    api_version = f\"{self.group}/{self.version}\"\n    self._deploy_manager.register_watch(\n        api_version=api_version,\n        kind=self.kind,\n        callback=partial(self.run_reconcile, False),\n    )\n    if self.controller_type.has_finalizer:\n        log.debug(\"Registering finalizer\")\n        self._deploy_manager.register_finalizer(\n            api_version=api_version,\n            kind=self.kind,\n            callback=partial(self.run_reconcile, True),\n        )\n\n    return True\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager","title":"<code>python_watch_manager</code>","text":"<p>This module holds the pure-python implementation of the WatchManager</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters","title":"<code>filters</code>","text":"<p>init file for Filter submodule. Imports all filters, functions, and classes from filters module</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.common","title":"<code>common</code>","text":"<p>Common functions used for interacting with filters including default filter classes</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.common.get_configured_filter","title":"<code>get_configured_filter()</code>  <code>cached</code>","text":"<p>Get the default filter that should be applied to every resource</p> <p>Returns:</p> Name Type Description <code>default_filter</code> <code>Filter</code> <p>Filter The default filter specified in the Config</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/common.py</code> <pre><code>@lru_cache(maxsize=1)\ndef get_configured_filter() -&gt; Filter:\n    \"\"\"Get the default filter that should be applied to every resource\n\n    Returns:\n        default_filter: Filter\n            The default filter specified in the Config\"\"\"\n\n    filter_name = config.python_watch_manager.filter\n\n    # Check for filter in default list or attempt to\n    # manually import one\n    if filter_name in FILTER_CLASSES:\n        filter_obj = FILTER_CLASSES[filter_name]\n    elif inspect.isclass(filter_name) and issubclass(filter_name, Filter):\n        filter_obj = filter_name\n    elif isinstance(filter_name, str):\n        filter_obj = import_filter(filter_name)\n    # If no filter is provided then always enable\n    else:\n        filter_obj = EnableFilter\n\n    log.debug2(f\"Found filter: {filter_obj}\")\n    return filter_obj\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.common.get_filters_for_resource_id","title":"<code>get_filters_for_resource_id(controller_type, resource_id)</code>","text":"<p>Get the filters for a particular resource_id given a controller_type</p> <p>Parameters:</p> Name Type Description Default <code>controller_type</code> <code>CONTROLLER_CLASS_TYPE</code> <p>CONTROLLER_CLASS_TYPE The controller type whose filters we're inspecting</p> required <code>resource_id</code> <code>RESOURCE_ID_TYPE</code> <p>\"ResourceId\" The requested resource</p> required <p>Returns:</p> Name Type Description <code>filter_list</code> <code>List[Filter]</code> <p>List[Filter] The list of filters to be applied</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/common.py</code> <pre><code>def get_filters_for_resource_id(\n    controller_type: CONTROLLER_CLASS_TYPE, resource_id: RESOURCE_ID_TYPE\n) -&gt; List[Filter]:\n    \"\"\"Get the filters for a particular resource_id given a controller_type\n\n    Args:\n        controller_type: CONTROLLER_CLASS_TYPE\n            The controller type whose filters we're inspecting\n        resource_id: \"ResourceId\"\n            The requested resource\n\n    Returns:\n        filter_list: List[Filter]\n            The list of filters to be applied\n    \"\"\"\n    filters = getattr(controller_type, \"pwm_filters\", [])\n\n    if isinstance(filters, list):\n        return_filters = filters\n\n    elif isinstance(filters, dict):\n        return_filters = filters.get(resource_id.global_id, [])\n\n    else:\n        raise ConfigError(f\"Invalid type for PWM filters: {type(filters)}\")\n\n    log.debug3(f\"Found filters {return_filters} for resource: {resource_id}\")\n    return return_filters\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.common.import_filter","title":"<code>import_filter(filter_name)</code>","text":"<p>Import a filter from a string reference</p> <p>Parameters:</p> Name Type Description Default <code>filter_name</code> <code>str</code> <p>str Filter name in . form required <p>Returns:</p> Name Type Description <code>imported_filter</code> <code>Filter</code> <p>Filter The filter that was requested</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/common.py</code> <pre><code>def import_filter(filter_name: str) -&gt; Filter:\n    \"\"\"Import a filter from a string reference\n\n    Args:\n        filter_name: str\n            Filter name in &lt;module&gt;.&lt;filter&gt; form\n\n    Returns:\n        imported_filter: Filter\n            The filter that was requested\n    \"\"\"\n    module_path, class_name = filter_name.rsplit(\".\", 1)\n    try:\n        filter_module = importlib.import_module(module_path)\n        filter_obj = getattr(filter_module, class_name)\n    except (ImportError, AttributeError) as exc:\n        raise ConfigError(\n            f\"Invalid Filter: {filter_name}. Module or class not found\"\n        ) from exc\n\n    if (\n        inspect.isclass(filter_obj) and not issubclass(filter_obj, Filter)\n    ) and not isinstance(filter_obj, (Filter, list, tuple)):\n        raise ConfigError(f\"{filter_obj} is not a instance of {Filter}\")\n\n    return filter_obj\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.filters","title":"<code>filters</code>","text":"<p>Filters are used to limit the amount of events being reconciled by a watch manager This is based off of the kubernetes controller runtime's \"predicates\": https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.15.0/pkg/predicate#Funcs The default set of filters is derived from operator-sdk's ansible predicates https://github.com/operator-framework/operator-sdk/blob/50c6ac03746ff4edf582feb9a71d2a7ea6ae6c40/internal/ansible/controller/controller.go#L105</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.filters.AnnotationFilter","title":"<code>AnnotationFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Filter resources to reconcile on annotation changes</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>class AnnotationFilter(Filter):\n    \"\"\"Filter resources to reconcile on annotation changes\"\"\"\n\n    def __init__(self, resource: ManagedObject):\n        \"\"\"Initialize the annotation hash variable\"\"\"\n        self.annotations = None\n        super().__init__(resource)\n\n    def test(  # pylint: disable=inconsistent-return-statements\n        self,\n        resource: ManagedObject,\n        event: KubeEventType,\n    ) -&gt; Optional[bool]:\n        \"\"\"Test if a resource's annotation has changed\"\"\"\n        # Ignore Added and deleted events\n        if event in [KubeEventType.ADDED, KubeEventType.DELETED]:\n            return\n\n        return self.annotations != self.get_annotation_hash(resource)\n\n    def update(self, resource: ManagedObject):\n        \"\"\"Update the currently stored annotation\"\"\"\n        self.annotations = self.get_annotation_hash(resource)\n\n    def get_annotation_hash(self, resource: ManagedObject) -&gt; str:\n        \"\"\"Helper function to get the annotation hash\"\"\"\n        return obj_to_hash(resource.metadata.get(\"annotations\", {}))\n</code></pre> <code>__init__(resource)</code> <p>Initialize the annotation hash variable</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def __init__(self, resource: ManagedObject):\n    \"\"\"Initialize the annotation hash variable\"\"\"\n    self.annotations = None\n    super().__init__(resource)\n</code></pre> <code>get_annotation_hash(resource)</code> <p>Helper function to get the annotation hash</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def get_annotation_hash(self, resource: ManagedObject) -&gt; str:\n    \"\"\"Helper function to get the annotation hash\"\"\"\n    return obj_to_hash(resource.metadata.get(\"annotations\", {}))\n</code></pre> <code>test(resource, event)</code> <p>Test if a resource's annotation has changed</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def test(  # pylint: disable=inconsistent-return-statements\n    self,\n    resource: ManagedObject,\n    event: KubeEventType,\n) -&gt; Optional[bool]:\n    \"\"\"Test if a resource's annotation has changed\"\"\"\n    # Ignore Added and deleted events\n    if event in [KubeEventType.ADDED, KubeEventType.DELETED]:\n        return\n\n    return self.annotations != self.get_annotation_hash(resource)\n</code></pre> <code>update(resource)</code> <p>Update the currently stored annotation</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def update(self, resource: ManagedObject):\n    \"\"\"Update the currently stored annotation\"\"\"\n    self.annotations = self.get_annotation_hash(resource)\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.filters.CreationDeletionFilter","title":"<code>CreationDeletionFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Filter to ensure reconciliation on creation and deletion events</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>class CreationDeletionFilter(Filter):\n    \"\"\"Filter to ensure reconciliation on creation and deletion events\"\"\"\n\n    def test(  # pylint: disable=inconsistent-return-statements\n        self,\n        resource: ManagedObject,\n        event: KubeEventType,\n    ) -&gt; Optional[bool]:\n        \"\"\"Return true if event is ADDED or DELETED\"\"\"\n\n        # Ignore non Added/Deleted Events\n        if event not in [KubeEventType.ADDED, KubeEventType.DELETED]:\n            return\n\n        return True\n</code></pre> <code>test(resource, event)</code> <p>Return true if event is ADDED or DELETED</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def test(  # pylint: disable=inconsistent-return-statements\n    self,\n    resource: ManagedObject,\n    event: KubeEventType,\n) -&gt; Optional[bool]:\n    \"\"\"Return true if event is ADDED or DELETED\"\"\"\n\n    # Ignore non Added/Deleted Events\n    if event not in [KubeEventType.ADDED, KubeEventType.DELETED]:\n        return\n\n    return True\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.filters.DependentWatchFilter","title":"<code>DependentWatchFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Don't reconcile creation events as we can assume the owner created them</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>class DependentWatchFilter(Filter):\n    \"\"\"Don't reconcile creation events as we can assume the owner created\n    them\"\"\"\n\n    def test(self, resource: ManagedObject, event: KubeEventType) -&gt; Optional[bool]:\n        \"\"\"Return False if event is ADDED\"\"\"\n        return event != KubeEventType.ADDED\n</code></pre> <code>test(resource, event)</code> <p>Return False if event is ADDED</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def test(self, resource: ManagedObject, event: KubeEventType) -&gt; Optional[bool]:\n    \"\"\"Return False if event is ADDED\"\"\"\n    return event != KubeEventType.ADDED\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.filters.DisableFilter","title":"<code>DisableFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Filter to disable all reconciles</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>class DisableFilter(Filter):\n    \"\"\"Filter to disable all reconciles\"\"\"\n\n    def test(self, resource: ManagedObject, event: KubeEventType) -&gt; Optional[bool]:\n        \"\"\"Always return False\"\"\"\n        return False\n</code></pre> <code>test(resource, event)</code> <p>Always return False</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def test(self, resource: ManagedObject, event: KubeEventType) -&gt; Optional[bool]:\n    \"\"\"Always return False\"\"\"\n    return False\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.filters.EnableFilter","title":"<code>EnableFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Filter to run all reconciles</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>class EnableFilter(Filter):\n    \"\"\"Filter to run all reconciles\"\"\"\n\n    def test(self, resource: ManagedObject, event: KubeEventType) -&gt; Optional[bool]:\n        \"\"\"Always return True\"\"\"\n        return True\n</code></pre> <code>test(resource, event)</code> <p>Always return True</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def test(self, resource: ManagedObject, event: KubeEventType) -&gt; Optional[bool]:\n    \"\"\"Always return True\"\"\"\n    return True\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.filters.Filter","title":"<code>Filter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Generic Filter Interface for subclassing. Every subclass should implement a <code>test</code> function which returns true when a resource should be reconciled. Subclasses can optionally implement a <code>update</code> method if the filter requires storing some stateful information like ResourceVersion or Metadata.</p> <p>NOTE: A unique Filter instance is created for each resource</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>class Filter(ABC):\n    \"\"\"Generic Filter Interface for subclassing. Every subclass should implement a\n    `test` function which returns true when a resource should be reconciled. Subclasses\n    can optionally implement a `update` method if the filter requires storing some stateful\n    information like ResourceVersion or Metadata.\n\n    NOTE: A unique Filter instance is created for each resource\n    \"\"\"\n\n    def __init__(self, resource: ManagedObject):  # noqa: B027\n        \"\"\"Initializer can be used to detect configuration or create instance\n        variables. Even though a resource is provided it should not set state until\n        update is called\n\n        Args:\n            resource: ManagedObject\n                This resource can be used by subclass to gather generic information.\n\n        \"\"\"\n\n    ## Abstract Interface ######################################################\n    #\n    # These functions must be implemented by child classes\n    ##\n\n    @abstractmethod\n    def test(self, resource: ManagedObject, event: KubeEventType) -&gt; Optional[bool]:\n        \"\"\"Test whether the resource&amp;event passes the filter. Returns true if\n        the filter should be reconciled and return false if it should not be. A filter\n        can optionally return None to ignore an event\n\n        Args:\n            resource: ManagedObject\n                The current resource being checked\n            event: KubeEventType\n                The event type that triggered this filter\n\n        Returns:\n            result: Optional[bool]\n                The result of the test.\n\n        \"\"\"\n\n    ## Base Class Interface ####################################################\n    #\n    # These methods MAY be implemented by children, but contain default\n    # implementations that are appropriate for simple cases.\n    #\n    ##\n\n    def update(self, resource: ManagedObject):  # noqa: B027\n        \"\"\"Update the instances current state.\n\n        Args:\n            resource: ManagedObject\n               The current state of the resource\n        \"\"\"\n\n    def update_and_test(self, resource: ManagedObject, event: KubeEventType) -&gt; bool:\n        \"\"\"First test a resource/event against a filter then update the current state\n\n        Args:\n            resource: ManagedObject\n                The resource being filtered\n            event: KubeEventType\n                The event to be filtered\n\n        Returns:\n            test_result: bool\n                The test result\n        \"\"\"\n        result = self.test(resource, event)\n        if result is not None and not result:\n            log.debug3(\n                \"Failed filter: %s with return val %s\",\n                self,\n                result,\n                extra={\"resource\": resource},\n            )\n        self.update(resource)\n        return result\n</code></pre> <code>__init__(resource)</code> <p>Initializer can be used to detect configuration or create instance variables. Even though a resource is provided it should not set state until update is called</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>ManagedObject</code> <p>ManagedObject This resource can be used by subclass to gather generic information.</p> required Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def __init__(self, resource: ManagedObject):  # noqa: B027\n    \"\"\"Initializer can be used to detect configuration or create instance\n    variables. Even though a resource is provided it should not set state until\n    update is called\n\n    Args:\n        resource: ManagedObject\n            This resource can be used by subclass to gather generic information.\n\n    \"\"\"\n</code></pre> <code>test(resource, event)</code> <code>abstractmethod</code> <p>Test whether the resource&amp;event passes the filter. Returns true if the filter should be reconciled and return false if it should not be. A filter can optionally return None to ignore an event</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>ManagedObject</code> <p>ManagedObject The current resource being checked</p> required <code>event</code> <code>KubeEventType</code> <p>KubeEventType The event type that triggered this filter</p> required <p>Returns:</p> Name Type Description <code>result</code> <code>Optional[bool]</code> <p>Optional[bool] The result of the test.</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>@abstractmethod\ndef test(self, resource: ManagedObject, event: KubeEventType) -&gt; Optional[bool]:\n    \"\"\"Test whether the resource&amp;event passes the filter. Returns true if\n    the filter should be reconciled and return false if it should not be. A filter\n    can optionally return None to ignore an event\n\n    Args:\n        resource: ManagedObject\n            The current resource being checked\n        event: KubeEventType\n            The event type that triggered this filter\n\n    Returns:\n        result: Optional[bool]\n            The result of the test.\n\n    \"\"\"\n</code></pre> <code>update(resource)</code> <p>Update the instances current state.</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>ManagedObject</code> <p>ManagedObject The current state of the resource</p> required Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def update(self, resource: ManagedObject):  # noqa: B027\n    \"\"\"Update the instances current state.\n\n    Args:\n        resource: ManagedObject\n           The current state of the resource\n    \"\"\"\n</code></pre> <code>update_and_test(resource, event)</code> <p>First test a resource/event against a filter then update the current state</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>ManagedObject</code> <p>ManagedObject The resource being filtered</p> required <code>event</code> <code>KubeEventType</code> <p>KubeEventType The event to be filtered</p> required <p>Returns:</p> Name Type Description <code>test_result</code> <code>bool</code> <p>bool The test result</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def update_and_test(self, resource: ManagedObject, event: KubeEventType) -&gt; bool:\n    \"\"\"First test a resource/event against a filter then update the current state\n\n    Args:\n        resource: ManagedObject\n            The resource being filtered\n        event: KubeEventType\n            The event to be filtered\n\n    Returns:\n        test_result: bool\n            The test result\n    \"\"\"\n    result = self.test(resource, event)\n    if result is not None and not result:\n        log.debug3(\n            \"Failed filter: %s with return val %s\",\n            self,\n            result,\n            extra={\"resource\": resource},\n        )\n    self.update(resource)\n    return result\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.filters.GenerationFilter","title":"<code>GenerationFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Filter for reconciling on generation changes for resources that support it</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>class GenerationFilter(Filter):\n    \"\"\"Filter for reconciling on generation changes for resources that support it\"\"\"\n\n    def __init__(self, resource: ManagedObject):\n        \"\"\"Set generation instance variable\"\"\"\n        super().__init__(resource)\n        self.generation = None\n\n    def test(  # pylint: disable=inconsistent-return-statements\n        self,\n        resource: ManagedObject,\n        event: KubeEventType,\n    ) -&gt; Optional[bool]:\n        \"\"\"Return true if resource generation is different than before\"\"\"\n        # Only update&amp;test resources with a generation\n        if not self.generation:\n            return\n\n        # Only test on resource updates\n        if event in [KubeEventType.ADDED, KubeEventType.DELETED]:\n            return\n\n        # Test if new generation is different\n        return self.generation != resource.metadata.get(\"generation\")\n\n    def update(self, resource: ManagedObject):\n        \"\"\"Update the currently observed generation\"\"\"\n        self.generation = resource.metadata.get(\"generation\")\n</code></pre> <code>__init__(resource)</code> <p>Set generation instance variable</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def __init__(self, resource: ManagedObject):\n    \"\"\"Set generation instance variable\"\"\"\n    super().__init__(resource)\n    self.generation = None\n</code></pre> <code>test(resource, event)</code> <p>Return true if resource generation is different than before</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def test(  # pylint: disable=inconsistent-return-statements\n    self,\n    resource: ManagedObject,\n    event: KubeEventType,\n) -&gt; Optional[bool]:\n    \"\"\"Return true if resource generation is different than before\"\"\"\n    # Only update&amp;test resources with a generation\n    if not self.generation:\n        return\n\n    # Only test on resource updates\n    if event in [KubeEventType.ADDED, KubeEventType.DELETED]:\n        return\n\n    # Test if new generation is different\n    return self.generation != resource.metadata.get(\"generation\")\n</code></pre> <code>update(resource)</code> <p>Update the currently observed generation</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def update(self, resource: ManagedObject):\n    \"\"\"Update the currently observed generation\"\"\"\n    self.generation = resource.metadata.get(\"generation\")\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.filters.LabelFilter","title":"<code>LabelFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Filter for resources that match a set of labels</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>class LabelFilter(Filter):\n    \"\"\"Filter for resources that match a set of labels\"\"\"\n\n    @abstractclassproperty\n    def labels(self) -&gt; dict:\n        \"\"\"Subclasses must implement a labels class attribute\"\"\"\n\n    def test(self, resource: ManagedObject, event: KubeEventType) -&gt; Optional[bool]:\n        \"\"\"Return true is a resource matches the requested labels\"\"\"\n        resource_labels = resource.get(\"metadata\", {}).get(\"labels\")\n        # Check to make sure every requested label matches\n        return all(\n            resource_labels.get(label) == value for label, value in self.labels.items()\n        )\n</code></pre> <code>labels()</code> <p>Subclasses must implement a labels class attribute</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>@abstractclassproperty\ndef labels(self) -&gt; dict:\n    \"\"\"Subclasses must implement a labels class attribute\"\"\"\n</code></pre> <code>test(resource, event)</code> <p>Return true is a resource matches the requested labels</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def test(self, resource: ManagedObject, event: KubeEventType) -&gt; Optional[bool]:\n    \"\"\"Return true is a resource matches the requested labels\"\"\"\n    resource_labels = resource.get(\"metadata\", {}).get(\"labels\")\n    # Check to make sure every requested label matches\n    return all(\n        resource_labels.get(label) == value for label, value in self.labels.items()\n    )\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.filters.NoGenerationFilter","title":"<code>NoGenerationFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Filter for reconciling changes to spec on resources that don't support the generation field like pods. It does this by hashing the objects excluding status and metadata</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>class NoGenerationFilter(Filter):\n    \"\"\"Filter for reconciling changes to spec on resources that don't support\n    the generation field like pods. It does this by hashing the objects excluding\n    status and metadata\"\"\"\n\n    def __init__(self, resource: ManagedObject):\n        \"\"\"Check if resource supports generation and initialize the hash dict\"\"\"\n        self.supports_generation = resource.metadata.get(\"generation\") is not None\n        self.resource_hashes = {}\n        super().__init__(resource)\n\n    def test(  # pylint: disable=inconsistent-return-statements\n        self,\n        resource: ManagedObject,\n        event: KubeEventType,\n    ) -&gt; Optional[bool]:\n        \"\"\"Return True if a resources current hash differs from the current\"\"\"\n        # Don't test resources that support generation or if we don't have hashes yet\n        if self.supports_generation or not self.resource_hashes:\n            return\n\n        # Only test on resource updates\n        if event in [KubeEventType.ADDED, KubeEventType.DELETED]:\n            return\n\n        # Check each stored resource hash to see if its\n        # changed\n        for key, obj_has in self.resource_hashes.items():\n            if obj_has != obj_to_hash(resource.get(key)):\n                log.debug2(\"Detected change in %s\", key)\n                return True\n\n        return False\n\n    def update(self, resource: ManagedObject):\n        \"\"\"Update the observed spec hashes\"\"\"\n        if self.supports_generation:\n            return\n\n        # Get the default hashes for all object keys except metadata\n        # and status\n        for key, obj in resource.definition.items():\n            if key in [\"metadata\", \"status\", \"kind\", \"apiVersion\"]:\n                continue\n\n            self.resource_hashes[key] = obj_to_hash(obj)\n</code></pre> <code>__init__(resource)</code> <p>Check if resource supports generation and initialize the hash dict</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def __init__(self, resource: ManagedObject):\n    \"\"\"Check if resource supports generation and initialize the hash dict\"\"\"\n    self.supports_generation = resource.metadata.get(\"generation\") is not None\n    self.resource_hashes = {}\n    super().__init__(resource)\n</code></pre> <code>test(resource, event)</code> <p>Return True if a resources current hash differs from the current</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def test(  # pylint: disable=inconsistent-return-statements\n    self,\n    resource: ManagedObject,\n    event: KubeEventType,\n) -&gt; Optional[bool]:\n    \"\"\"Return True if a resources current hash differs from the current\"\"\"\n    # Don't test resources that support generation or if we don't have hashes yet\n    if self.supports_generation or not self.resource_hashes:\n        return\n\n    # Only test on resource updates\n    if event in [KubeEventType.ADDED, KubeEventType.DELETED]:\n        return\n\n    # Check each stored resource hash to see if its\n    # changed\n    for key, obj_has in self.resource_hashes.items():\n        if obj_has != obj_to_hash(resource.get(key)):\n            log.debug2(\"Detected change in %s\", key)\n            return True\n\n    return False\n</code></pre> <code>update(resource)</code> <p>Update the observed spec hashes</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def update(self, resource: ManagedObject):\n    \"\"\"Update the observed spec hashes\"\"\"\n    if self.supports_generation:\n        return\n\n    # Get the default hashes for all object keys except metadata\n    # and status\n    for key, obj in resource.definition.items():\n        if key in [\"metadata\", \"status\", \"kind\", \"apiVersion\"]:\n            continue\n\n        self.resource_hashes[key] = obj_to_hash(obj)\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.filters.PauseFilter","title":"<code>PauseFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>This filter skips resources that have the oper8 pause annotation</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>class PauseFilter(Filter):\n    \"\"\"This filter skips resources that have the oper8 pause annotation\"\"\"\n\n    def test(self, resource: ManagedObject, event: KubeEventType) -&gt; Optional[bool]:\n        \"\"\"Test if a resource has the pause annotation\"\"\"\n        return not ReconcileManager._is_paused(  # pylint: disable=protected-access\n            resource\n        )\n</code></pre> <code>test(resource, event)</code> <p>Test if a resource has the pause annotation</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def test(self, resource: ManagedObject, event: KubeEventType) -&gt; Optional[bool]:\n    \"\"\"Test if a resource has the pause annotation\"\"\"\n    return not ReconcileManager._is_paused(  # pylint: disable=protected-access\n        resource\n    )\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.filters.ResourceVersionFilter","title":"<code>ResourceVersionFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Filter for duplicate resource versions which happens when restarting a watch connection</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>class ResourceVersionFilter(Filter):\n    \"\"\"Filter for duplicate resource versions which happens when restarting a\n    watch connection\"\"\"\n\n    def __init__(self, resource: ManagedObject):\n        \"\"\"Initialize the resource version list\"\"\"\n        # Use a dequeue instead of a list/set to set a bound on the number\n        # of tracked versions\n        self.resource_versions = deque([], maxlen=RESOURCE_VERSION_KEEP_COUNT)\n        super().__init__(resource)\n\n    def test(  # pylint: disable=inconsistent-return-statements\n        self,\n        resource: ManagedObject,\n        event: KubeEventType,\n    ) -&gt; Optional[bool]:\n        \"\"\"Test if the resource's resourceVersion has been seen before\"\"\"\n\n        # Don't skip add events as the kubernetes watch can duplicate events\n        if event == KubeEventType.DELETED:\n            return\n\n        return resource.resource_version not in self.resource_versions\n\n    def update(self, resource: ManagedObject):\n        \"\"\"Add the resources ResourceVersion to the list\"\"\"\n        self.resource_versions.append(resource.resource_version)\n</code></pre> <code>__init__(resource)</code> <p>Initialize the resource version list</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def __init__(self, resource: ManagedObject):\n    \"\"\"Initialize the resource version list\"\"\"\n    # Use a dequeue instead of a list/set to set a bound on the number\n    # of tracked versions\n    self.resource_versions = deque([], maxlen=RESOURCE_VERSION_KEEP_COUNT)\n    super().__init__(resource)\n</code></pre> <code>test(resource, event)</code> <p>Test if the resource's resourceVersion has been seen before</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def test(  # pylint: disable=inconsistent-return-statements\n    self,\n    resource: ManagedObject,\n    event: KubeEventType,\n) -&gt; Optional[bool]:\n    \"\"\"Test if the resource's resourceVersion has been seen before\"\"\"\n\n    # Don't skip add events as the kubernetes watch can duplicate events\n    if event == KubeEventType.DELETED:\n        return\n\n    return resource.resource_version not in self.resource_versions\n</code></pre> <code>update(resource)</code> <p>Add the resources ResourceVersion to the list</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def update(self, resource: ManagedObject):\n    \"\"\"Add the resources ResourceVersion to the list\"\"\"\n    self.resource_versions.append(resource.resource_version)\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.filters.SubsystemStatusFilter","title":"<code>SubsystemStatusFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Reconcile oper8 controllers when their oper8 status changes</p> This has passed basic validation but has not been rigorously tested <p>in the field</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>class SubsystemStatusFilter(Filter):\n    \"\"\"Reconcile oper8 controllers when their oper8 status changes\n\n    EXPERIMENTAL: This has passed basic validation but has not been rigorously tested\n     in the field\n    \"\"\"\n\n    def __init__(self, resource: ManagedObject):\n        \"\"\"Initialize the currently observed ready condition\"\"\"\n        self.ready_condition = None\n        super().__init__(resource)\n\n    def test(  # pylint: disable=inconsistent-return-statements\n        self,\n        resource: ManagedObject,\n        event: KubeEventType,\n    ) -&gt; Optional[bool]:\n        \"\"\"Test if a resources subsystem condition has changed\"\"\"\n        if event in [KubeEventType.ADDED, KubeEventType.DELETED]:\n            return\n\n        return self.ready_condition != get_condition(\n            READY_CONDITION, resource.get(\"status\", {})\n        ).get(\"reason\")\n\n    def update(self, resource: ManagedObject):\n        \"\"\"Update the currently observed ready condition\"\"\"\n        self.ready_condition = get_condition(\n            READY_CONDITION, resource.get(\"status\", {})\n        ).get(\"reason\")\n</code></pre> <code>__init__(resource)</code> <p>Initialize the currently observed ready condition</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def __init__(self, resource: ManagedObject):\n    \"\"\"Initialize the currently observed ready condition\"\"\"\n    self.ready_condition = None\n    super().__init__(resource)\n</code></pre> <code>test(resource, event)</code> <p>Test if a resources subsystem condition has changed</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def test(  # pylint: disable=inconsistent-return-statements\n    self,\n    resource: ManagedObject,\n    event: KubeEventType,\n) -&gt; Optional[bool]:\n    \"\"\"Test if a resources subsystem condition has changed\"\"\"\n    if event in [KubeEventType.ADDED, KubeEventType.DELETED]:\n        return\n\n    return self.ready_condition != get_condition(\n        READY_CONDITION, resource.get(\"status\", {})\n    ).get(\"reason\")\n</code></pre> <code>update(resource)</code> <p>Update the currently observed ready condition</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def update(self, resource: ManagedObject):\n    \"\"\"Update the currently observed ready condition\"\"\"\n    self.ready_condition = get_condition(\n        READY_CONDITION, resource.get(\"status\", {})\n    ).get(\"reason\")\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.filters.UserAnnotationFilter","title":"<code>UserAnnotationFilter</code>","text":"<p>               Bases: <code>AnnotationFilter</code></p> <p>Filter resources to reconcile on user annotation changes. This excludes kubernetes and openshift annotations</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>class UserAnnotationFilter(AnnotationFilter):\n    \"\"\"Filter resources to reconcile on user annotation changes. This excludes\n    kubernetes and openshift annotations\n    \"\"\"\n\n    def get_annotation_hash(self, resource: ManagedObject) -&gt; str:\n        \"\"\"Overriden function to exclude common platform annotations from\n        the annotation hash\"\"\"\n        output_annotations = {}\n        for key, value in resource.metadata.get(\"annotations\", {}).items():\n            if self.contains_platform_key(key):\n                continue\n\n            output_annotations[key] = value\n\n        return obj_to_hash(output_annotations)\n\n    def contains_platform_key(self, key: str) -&gt; bool:\n        \"\"\"Helper to check if the key contains one of the\n        platform annotations\"\"\"\n        return any(\n            reserved_key in key for reserved_key in RESERVED_PLATFORM_ANNOTATIONS\n        )\n</code></pre> <code>contains_platform_key(key)</code> <p>Helper to check if the key contains one of the platform annotations</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def contains_platform_key(self, key: str) -&gt; bool:\n    \"\"\"Helper to check if the key contains one of the\n    platform annotations\"\"\"\n    return any(\n        reserved_key in key for reserved_key in RESERVED_PLATFORM_ANNOTATIONS\n    )\n</code></pre> <code>get_annotation_hash(resource)</code> <p>Overriden function to exclude common platform annotations from the annotation hash</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/filters.py</code> <pre><code>def get_annotation_hash(self, resource: ManagedObject) -&gt; str:\n    \"\"\"Overriden function to exclude common platform annotations from\n    the annotation hash\"\"\"\n    output_annotations = {}\n    for key, value in resource.metadata.get(\"annotations\", {}).items():\n        if self.contains_platform_key(key):\n            continue\n\n        output_annotations[key] = value\n\n    return obj_to_hash(output_annotations)\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.manager","title":"<code>manager</code>","text":"<p>Module contains helpers for processing a group of filters</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.manager.FilterManager","title":"<code>FilterManager</code>","text":"<p>               Bases: <code>Filter</code></p> <p>The FilterManager class helps process conditional filters and groups of filters. Filters that in a list are \"anded\" together while Filters in a tuple are \"ored\". This class also contains helpers to recursively convert between ClassInfo and Filters.</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/manager.py</code> <pre><code>class FilterManager(Filter):\n    \"\"\"The FilterManager class helps process conditional filters and groups of filters.\n    Filters that in a list are \"anded\" together while Filters in a tuple are \"ored\".\n    This class also contains helpers to recursively convert between ClassInfo and Filters.\n    \"\"\"\n\n    def __init__(\n        self,\n        filters: Union[List[Type[Filter]], Tuple[Type[Filter]]],\n        resource: ManagedObject,\n    ):\n        \"\"\"Initialize all filters in the provided group\n\n        Args:\n            filters: Union[List[Type[Filter]], Tuple[Type[Filter]]]\n                The filters to manage\n            resource: ManagedObject\n                The initial resource\n        \"\"\"\n        self.filters = self.__recursive_filter_init(filters, resource)\n\n    ### Public Interface\n\n    def update_and_test(\n        self, resource: ManagedObject, event: KubeEventType\n    ) -&gt; Optional[bool]:\n        \"\"\"Recursively update and test each filter\"\"\"\n        return self.__recursive_update_and_test(self.filters, resource, event)\n\n    def test(self, resource: ManagedObject, event: KubeEventType) -&gt; Optional[bool]:\n        \"\"\"Recursively test each filter\"\"\"\n        # test with test_only set to True so nothing is updated\n        return self.__recursive_update_and_test(\n            self.filters, resource, event, test_only=True\n        )\n\n    def update(self, resource: ManagedObject):\n        \"\"\"Update each filter recursively\"\"\"\n        # Update with update_only set to True so no tests are ran\n        self.__recursive_update_and_test(  # pylint: disable=redundant-keyword-arg\n            self, self.filters, resource, None, update_only=True\n        )\n\n    @classmethod\n    def to_info(cls, filters: Type[Filter]) -&gt; Type[ClassInfo]:\n        \"\"\"Helper function to convert from filters to ClassInfos. This is used for pickling and IPC\n\n        Args:\n            filters: Type[Filter]\n                The filters to convert\n\n        Returns:\n            class_info: Type[ClassInfo]\n                The class info objects describing the filter\n        \"\"\"\n        return cls.__recursive_filter_info(filters)\n\n    @classmethod\n    def from_info(cls, info: Type[ClassInfo]) -&gt; Type[Filter]:\n        \"\"\"Helper function to convert from ClassInfos to a filter\n\n\n        Args:\n            class_info: Type[ClassInfo]\n                The classinfos to convert back into filters\n\n        Returns:\n            filters: Type[Filter]\n                The converted filter objects\n        \"\"\"\n        return cls.__recursive_filter_info(info)\n\n    ### Private Helper Functions\n    @classmethod\n    def __recursive_filter_info(\n        cls, descriptor: Union[Type[Filter], Type[ClassInfo]]\n    ) -&gt; Union[Type[Filter], Type[ClassInfo]]:\n        \"\"\"Recursive helper to convert from filters to class infos and back\n\n        Args:\n            descriptor: Union[Type[Filter],Type[ClassInfo]]\n                Either the filter or class_info to convert\n\n        Returns:\n            type: Union[Type[Filter],Type[ClassInfo]]\n                The converted types\n        \"\"\"\n\n        def convert_filter_type(descriptor):\n            \"\"\"Generic function to convert between types\"\"\"\n\n            # If we get a filter than we're converting to ClassInfo else\n            # we're converting back to Filters\n            if inspect.isclass(descriptor) and issubclass(descriptor, Filter):\n                return ClassInfo.from_type(descriptor)\n            if isinstance(descriptor, ClassInfo):\n                return descriptor.to_class()\n            # The instance must be a list or a tuple to be processed\n            raise ValueError(\n                f\"Unknown type: {type(descriptor)} {descriptor} passed to convert_filter_type\"\n            )\n\n        return cls.__recursive_map(descriptor, convert_filter_type)\n\n    def __recursive_filter_init(\n        self,\n        filters: Union[List[Type[Filter]], Tuple[Type[Filter]], Type[Filter]],\n        resource: ManagedObject,\n    ) -&gt; Union[List[Filter], Tuple[Filter], Filter]:\n        \"\"\"Helper function to recursively init each filter\n\n        Args:\n            filters: Union[List[Type[Filter]], Tuple[Type[Filter]], Type[Filter]]\n                The filters to be initialized\n            resource: ManagedObject\n                The resource to pass to the filters\n        Returns:\n            filters: Union[List[Filter], Tuple[Filter], Filter]\n                The initalized filters\n        \"\"\"\n\n        def init_filter(filter_type: Type[Filter]) -&gt; Filter:\n            if not (inspect.isclass(filter_type) and issubclass(filter_type, Filter)):\n                raise ValueError(\n                    f\"Unknown type: {type(filter_type)} passed to init_filter\"\n                )\n\n            return filter_type(resource)\n\n        return self.__recursive_map(filters, init_filter)\n\n    def __recursive_update_and_test(  # pylint: disable=too-many-arguments, inconsistent-return-statements\n        self,\n        filters: Union[list, tuple, Filter],\n        resource: ManagedObject,\n        event: KubeEventType,\n        update_only: bool = False,\n        test_only: bool = False,\n    ) -&gt; Optional[bool]:\n        \"\"\"Helper function to recursively update, test, or both.\n\n        Args:\n            filters: Union[list, tuple, Filter]\n                The current filters being tested. This is updated when recurring\n            resource: ManagedObject\n                The current resource being updated/tested\n            event: KubeEventType,\n                The current event type being updated/tested\n            update_only: bool = False\n                Whether to only update the filters\n            test_only: bool = False,\n                Whether to only test the filters\n\n        Returns:\n            result: Optional[bool]\n                The result of the tests if it was ran\n        \"\"\"\n        if update_only and test_only:\n            raise ValueError(\"update_only and test_only can not both be True\")\n\n        # Check Initial object types and exit condition\n        if isinstance(filters, Filter):\n            # If instance is a filter than call either update or update_and_test\n            # depending on the failed status\n            if update_only:\n                filters.update(resource)\n                return\n            if test_only:\n                return filters.test(resource, event)\n\n            return filters.update_and_test(resource, event)\n\n        # If filter list is empty then immediately return success\n        if not filters:\n            return True\n\n        return_value = None\n        operation = operator.and_ if isinstance(filters, list) else operator.or_\n\n        for filter_combo in filters:\n            # Recursively processes the filter combo\n            result = self.__recursive_update_and_test(\n                filter_combo, resource, event, update_only, test_only\n            )\n\n            # If return_value has already been set then combine it with the most recent result\n            # via the operation\n            if result is not None:\n                if return_value is not None:\n                    return_value = operation(return_value, result)\n                else:\n                    return_value = result\n\n            # There are two scenarios when filters only need to get updated not tested. The first\n            # is when an \"and\" condition fails or when an \"or\" succeeds. In both instances we\n            # know the end result so testing can be skipped for performance\n            if (\n                (not update_only and not test_only)\n                and result\n                and (\n                    (operation == operator.and_ and not result)\n                    or (operation == operator.or_ and result)\n                )\n            ):\n                update_only = True\n\n        # If no filter cared about the event then don't\n        # reconcile\n        if return_value is None:\n            return False\n\n        return return_value\n\n    @classmethod\n    def __recursive_map(\n        cls, filters: Union[List[Any], Tuple[Any]], operation: Callable[[Filter], Any]\n    ):\n        \"\"\"Helper function to map an operation onto every object in a filter chain\n\n        Args:\n            filters: Union[List[Any], Tuple[Any]]\n                The filters to map onto\n            op: Callable[[Filter],None]\n                The function to map onto each filter\n        \"\"\"\n\n        # Directly check tuple to ignore NamedTuples and subclasses\n        if not (isinstance(filters, list) or type(filters) is tuple):\n            return operation(filters)\n\n        filter_list = []\n        for filter_obj in filters:\n            filter_list.append(cls.__recursive_map(filter_obj, operation))\n\n        # Ensure the returned iterable is the same type as the original\n        return type(filters)(filter_list)\n</code></pre> <code>__init__(filters, resource)</code> <p>Initialize all filters in the provided group</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Union[List[Type[Filter]], Tuple[Type[Filter]]]</code> <p>Union[List[Type[Filter]], Tuple[Type[Filter]]] The filters to manage</p> required <code>resource</code> <code>ManagedObject</code> <p>ManagedObject The initial resource</p> required Source code in <code>oper8/watch_manager/python_watch_manager/filters/manager.py</code> <pre><code>def __init__(\n    self,\n    filters: Union[List[Type[Filter]], Tuple[Type[Filter]]],\n    resource: ManagedObject,\n):\n    \"\"\"Initialize all filters in the provided group\n\n    Args:\n        filters: Union[List[Type[Filter]], Tuple[Type[Filter]]]\n            The filters to manage\n        resource: ManagedObject\n            The initial resource\n    \"\"\"\n    self.filters = self.__recursive_filter_init(filters, resource)\n</code></pre> <code>__recursive_filter_info(descriptor)</code> <code>classmethod</code> <p>Recursive helper to convert from filters to class infos and back</p> <p>Parameters:</p> Name Type Description Default <code>descriptor</code> <code>Union[Type[Filter], Type[ClassInfo]]</code> <p>Union[Type[Filter],Type[ClassInfo]] Either the filter or class_info to convert</p> required <p>Returns:</p> Name Type Description <code>type</code> <code>Union[Type[Filter], Type[ClassInfo]]</code> <p>Union[Type[Filter],Type[ClassInfo]] The converted types</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/manager.py</code> <pre><code>@classmethod\ndef __recursive_filter_info(\n    cls, descriptor: Union[Type[Filter], Type[ClassInfo]]\n) -&gt; Union[Type[Filter], Type[ClassInfo]]:\n    \"\"\"Recursive helper to convert from filters to class infos and back\n\n    Args:\n        descriptor: Union[Type[Filter],Type[ClassInfo]]\n            Either the filter or class_info to convert\n\n    Returns:\n        type: Union[Type[Filter],Type[ClassInfo]]\n            The converted types\n    \"\"\"\n\n    def convert_filter_type(descriptor):\n        \"\"\"Generic function to convert between types\"\"\"\n\n        # If we get a filter than we're converting to ClassInfo else\n        # we're converting back to Filters\n        if inspect.isclass(descriptor) and issubclass(descriptor, Filter):\n            return ClassInfo.from_type(descriptor)\n        if isinstance(descriptor, ClassInfo):\n            return descriptor.to_class()\n        # The instance must be a list or a tuple to be processed\n        raise ValueError(\n            f\"Unknown type: {type(descriptor)} {descriptor} passed to convert_filter_type\"\n        )\n\n    return cls.__recursive_map(descriptor, convert_filter_type)\n</code></pre> <code>__recursive_filter_init(filters, resource)</code> <p>Helper function to recursively init each filter</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Union[List[Type[Filter]], Tuple[Type[Filter]], Type[Filter]]</code> <p>Union[List[Type[Filter]], Tuple[Type[Filter]], Type[Filter]] The filters to be initialized</p> required <code>resource</code> <code>ManagedObject</code> <p>ManagedObject The resource to pass to the filters</p> required <p>Returns:     filters: Union[List[Filter], Tuple[Filter], Filter]         The initalized filters</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/manager.py</code> <pre><code>def __recursive_filter_init(\n    self,\n    filters: Union[List[Type[Filter]], Tuple[Type[Filter]], Type[Filter]],\n    resource: ManagedObject,\n) -&gt; Union[List[Filter], Tuple[Filter], Filter]:\n    \"\"\"Helper function to recursively init each filter\n\n    Args:\n        filters: Union[List[Type[Filter]], Tuple[Type[Filter]], Type[Filter]]\n            The filters to be initialized\n        resource: ManagedObject\n            The resource to pass to the filters\n    Returns:\n        filters: Union[List[Filter], Tuple[Filter], Filter]\n            The initalized filters\n    \"\"\"\n\n    def init_filter(filter_type: Type[Filter]) -&gt; Filter:\n        if not (inspect.isclass(filter_type) and issubclass(filter_type, Filter)):\n            raise ValueError(\n                f\"Unknown type: {type(filter_type)} passed to init_filter\"\n            )\n\n        return filter_type(resource)\n\n    return self.__recursive_map(filters, init_filter)\n</code></pre> <code>__recursive_map(filters, operation)</code> <code>classmethod</code> <p>Helper function to map an operation onto every object in a filter chain</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Union[List[Any], Tuple[Any]]</code> <p>Union[List[Any], Tuple[Any]] The filters to map onto</p> required <code>op</code> <p>Callable[[Filter],None] The function to map onto each filter</p> required Source code in <code>oper8/watch_manager/python_watch_manager/filters/manager.py</code> <pre><code>@classmethod\ndef __recursive_map(\n    cls, filters: Union[List[Any], Tuple[Any]], operation: Callable[[Filter], Any]\n):\n    \"\"\"Helper function to map an operation onto every object in a filter chain\n\n    Args:\n        filters: Union[List[Any], Tuple[Any]]\n            The filters to map onto\n        op: Callable[[Filter],None]\n            The function to map onto each filter\n    \"\"\"\n\n    # Directly check tuple to ignore NamedTuples and subclasses\n    if not (isinstance(filters, list) or type(filters) is tuple):\n        return operation(filters)\n\n    filter_list = []\n    for filter_obj in filters:\n        filter_list.append(cls.__recursive_map(filter_obj, operation))\n\n    # Ensure the returned iterable is the same type as the original\n    return type(filters)(filter_list)\n</code></pre> <code>__recursive_update_and_test(filters, resource, event, update_only=False, test_only=False)</code> <p>Helper function to recursively update, test, or both.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Union[list, tuple, Filter]</code> <p>Union[list, tuple, Filter] The current filters being tested. This is updated when recurring</p> required <code>resource</code> <code>ManagedObject</code> <p>ManagedObject The current resource being updated/tested</p> required <code>event</code> <code>KubeEventType</code> <p>KubeEventType, The current event type being updated/tested</p> required <code>update_only</code> <code>bool</code> <p>bool = False Whether to only update the filters</p> <code>False</code> <code>test_only</code> <code>bool</code> <p>bool = False, Whether to only test the filters</p> <code>False</code> <p>Returns:</p> Name Type Description <code>result</code> <code>Optional[bool]</code> <p>Optional[bool] The result of the tests if it was ran</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/manager.py</code> <pre><code>def __recursive_update_and_test(  # pylint: disable=too-many-arguments, inconsistent-return-statements\n    self,\n    filters: Union[list, tuple, Filter],\n    resource: ManagedObject,\n    event: KubeEventType,\n    update_only: bool = False,\n    test_only: bool = False,\n) -&gt; Optional[bool]:\n    \"\"\"Helper function to recursively update, test, or both.\n\n    Args:\n        filters: Union[list, tuple, Filter]\n            The current filters being tested. This is updated when recurring\n        resource: ManagedObject\n            The current resource being updated/tested\n        event: KubeEventType,\n            The current event type being updated/tested\n        update_only: bool = False\n            Whether to only update the filters\n        test_only: bool = False,\n            Whether to only test the filters\n\n    Returns:\n        result: Optional[bool]\n            The result of the tests if it was ran\n    \"\"\"\n    if update_only and test_only:\n        raise ValueError(\"update_only and test_only can not both be True\")\n\n    # Check Initial object types and exit condition\n    if isinstance(filters, Filter):\n        # If instance is a filter than call either update or update_and_test\n        # depending on the failed status\n        if update_only:\n            filters.update(resource)\n            return\n        if test_only:\n            return filters.test(resource, event)\n\n        return filters.update_and_test(resource, event)\n\n    # If filter list is empty then immediately return success\n    if not filters:\n        return True\n\n    return_value = None\n    operation = operator.and_ if isinstance(filters, list) else operator.or_\n\n    for filter_combo in filters:\n        # Recursively processes the filter combo\n        result = self.__recursive_update_and_test(\n            filter_combo, resource, event, update_only, test_only\n        )\n\n        # If return_value has already been set then combine it with the most recent result\n        # via the operation\n        if result is not None:\n            if return_value is not None:\n                return_value = operation(return_value, result)\n            else:\n                return_value = result\n\n        # There are two scenarios when filters only need to get updated not tested. The first\n        # is when an \"and\" condition fails or when an \"or\" succeeds. In both instances we\n        # know the end result so testing can be skipped for performance\n        if (\n            (not update_only and not test_only)\n            and result\n            and (\n                (operation == operator.and_ and not result)\n                or (operation == operator.or_ and result)\n            )\n        ):\n            update_only = True\n\n    # If no filter cared about the event then don't\n    # reconcile\n    if return_value is None:\n        return False\n\n    return return_value\n</code></pre> <code>from_info(info)</code> <code>classmethod</code> <p>Helper function to convert from ClassInfos to a filter</p> <p>Parameters:</p> Name Type Description Default <code>class_info</code> <p>Type[ClassInfo] The classinfos to convert back into filters</p> required <p>Returns:</p> Name Type Description <code>filters</code> <code>Type[Filter]</code> <p>Type[Filter] The converted filter objects</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/manager.py</code> <pre><code>@classmethod\ndef from_info(cls, info: Type[ClassInfo]) -&gt; Type[Filter]:\n    \"\"\"Helper function to convert from ClassInfos to a filter\n\n\n    Args:\n        class_info: Type[ClassInfo]\n            The classinfos to convert back into filters\n\n    Returns:\n        filters: Type[Filter]\n            The converted filter objects\n    \"\"\"\n    return cls.__recursive_filter_info(info)\n</code></pre> <code>test(resource, event)</code> <p>Recursively test each filter</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/manager.py</code> <pre><code>def test(self, resource: ManagedObject, event: KubeEventType) -&gt; Optional[bool]:\n    \"\"\"Recursively test each filter\"\"\"\n    # test with test_only set to True so nothing is updated\n    return self.__recursive_update_and_test(\n        self.filters, resource, event, test_only=True\n    )\n</code></pre> <code>to_info(filters)</code> <code>classmethod</code> <p>Helper function to convert from filters to ClassInfos. This is used for pickling and IPC</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Type[Filter]</code> <p>Type[Filter] The filters to convert</p> required <p>Returns:</p> Name Type Description <code>class_info</code> <code>Type[ClassInfo]</code> <p>Type[ClassInfo] The class info objects describing the filter</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/manager.py</code> <pre><code>@classmethod\ndef to_info(cls, filters: Type[Filter]) -&gt; Type[ClassInfo]:\n    \"\"\"Helper function to convert from filters to ClassInfos. This is used for pickling and IPC\n\n    Args:\n        filters: Type[Filter]\n            The filters to convert\n\n    Returns:\n        class_info: Type[ClassInfo]\n            The class info objects describing the filter\n    \"\"\"\n    return cls.__recursive_filter_info(filters)\n</code></pre> <code>update(resource)</code> <p>Update each filter recursively</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/manager.py</code> <pre><code>def update(self, resource: ManagedObject):\n    \"\"\"Update each filter recursively\"\"\"\n    # Update with update_only set to True so no tests are ran\n    self.__recursive_update_and_test(  # pylint: disable=redundant-keyword-arg\n        self, self.filters, resource, None, update_only=True\n    )\n</code></pre> <code>update_and_test(resource, event)</code> <p>Recursively update and test each filter</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/manager.py</code> <pre><code>def update_and_test(\n    self, resource: ManagedObject, event: KubeEventType\n) -&gt; Optional[bool]:\n    \"\"\"Recursively update and test each filter\"\"\"\n    return self.__recursive_update_and_test(self.filters, resource, event)\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.manager.AndFilter","title":"<code>AndFilter(*args)</code>","text":"<p>An \"And\" Filter is just a list of filters</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/manager.py</code> <pre><code>def AndFilter(*args):  # pylint: disable=invalid-name\n    \"\"\"An \"And\" Filter is just a list of filters\"\"\"\n    return list(args)\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.filters.manager.OrFilter","title":"<code>OrFilter(*args)</code>","text":"<p>An \"Or\" Filter is just a tuple of filters</p> Source code in <code>oper8/watch_manager/python_watch_manager/filters/manager.py</code> <pre><code>def OrFilter(*args):  # pylint: disable=invalid-name\n    \"\"\"An \"Or\" Filter is just a tuple of filters\"\"\"\n    return tuple(args)\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.leader_election","title":"<code>leader_election</code>","text":"<p>init file for leadership election classes. Imports all leadership managers and defines a generic helper</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.leader_election.get_leader_election_class","title":"<code>get_leader_election_class()</code>","text":"<p>Get the current configured leadership election</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/__init__.py</code> <pre><code>def get_leader_election_class() -&gt; Type[LeadershipManagerBase]:\n    \"\"\"Get the current configured leadership election\"\"\"\n    if config.python_watch_manager.lock.type == \"leader-for-life\":\n        return LeaderForLifeManager\n    if config.python_watch_manager.lock.type == \"leader-with-lease\":\n        return LeaderWithLeaseManager\n    if config.python_watch_manager.lock.type == \"annotation\":\n        return AnnotationLeadershipManager\n    if config.python_watch_manager.lock.type == \"dryrun\":\n        return DryRunLeadershipManager\n    return DryRunLeadershipManager\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.leader_election.annotation","title":"<code>annotation</code>","text":"<p>Annotation Based Leadership Manager</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.leader_election.annotation.AnnotationLeadershipManager","title":"<code>AnnotationLeadershipManager</code>","text":"<p>               Bases: <code>LeadershipManagerBase</code></p> <p>Annotation based leadership manager that uses two annotations to track leadership on a per-resource basis. This allows for horizontally scalable operations.</p> This has passed basic validation but has not been rigorously tested <p>in the field</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/annotation.py</code> <pre><code>class AnnotationLeadershipManager(LeadershipManagerBase):\n    \"\"\"\n    Annotation based leadership manager that uses two annotations\n    to track leadership on a per-resource basis. This allows for\n    horizontally scalable operations.\n\n    EXPERIMENTAL: This has passed basic validation but has not been rigorously tested\n     in the field\n    \"\"\"\n\n    def __init__(self, deploy_manager: DeployManagerBase = None):\n        \"\"\"Initialize Leadership and gather current name\n\n        Args:\n            deploy_manager: DeployManagerBase = None\n                DeployManager for this Manager\n        \"\"\"\n\n        super().__init__(deploy_manager)\n        self.duration_delta = parse_time_delta(\n            config.python_watch_manager.lock.duration\n        )\n\n        # Gather lock_name, namespace and pod manifest\n        self.pod_name = get_pod_name()\n        assert_config(self.pod_name, \"Unable to detect pod name\")\n\n    ## Lock Interface ####################################################\n    def acquire(self, force: bool = False) -&gt; bool:\n        \"\"\"\n        Return true as leadership is managed at resource level\n        \"\"\"\n        return True\n\n    def acquire_resource(self, resource: ManagedObject):\n        \"\"\"Check a resource for leadership annotation and add one if it's expired\n        or does not exit\"\"\"\n        success, current_resource = self.deploy_manager.get_object_current_state(\n            resource.kind, resource.name, resource.namespace, resource.api_version\n        )\n        if not success or not current_resource:\n            log.warning(\n                \"Unable to fetch owner resource %s/%s/%s/%s\",\n                resource.kind,\n                resource.api_version,\n                resource.namespace,\n                resource.name,\n            )\n            return False\n\n        if \"annotations\" not in current_resource.get(\"metadata\"):\n            current_resource[\"metadata\"][\"annotations\"] = {}\n\n        # Check the current annotation\n        annotations = current_resource[\"metadata\"][\"annotations\"]\n        current_time = datetime.now()\n\n        # If no leader than take ownership\n        if not annotations.get(LEASE_NAME_ANNOTATION_NAME):\n            annotations[LEASE_NAME_ANNOTATION_NAME] = self.pod_name\n            annotations[LEASE_TIME_ANNOTATION_NAME] = current_time.isoformat()\n\n        # If already the current leader then update lease time\n        elif self.pod_name == annotations.get(LEASE_NAME_ANNOTATION_NAME):\n            annotations[LEASE_TIME_ANNOTATION_NAME] = current_time.isoformat()\n\n        # If the current leader's lease has timed out than take ownership\n        elif not self._check_lease_time(\n            annotations.get(LEASE_TIME_ANNOTATION_NAME), current_time\n        ):\n            annotations[LEASE_NAME_ANNOTATION_NAME] = self.pod_name\n            annotations[LEASE_TIME_ANNOTATION_NAME] = current_time.isoformat()\n\n        # Otherwise unable to acquire lock\n        else:\n            return False\n\n        success, _ = self.deploy_manager.deploy([current_resource])\n        if not success:\n            log.warning(\n                \"Unable to update resource annotation%s/%s/%s/%s\",\n                resource.kind,\n                resource.api_version,\n                resource.namespace,\n                resource.name,\n            )\n            return False\n\n        return True\n\n    def release(self):\n        \"\"\"\n        Release lock on global resource\n        \"\"\"\n        return True\n\n    def release_resource(self, resource: ManagedObject):\n        \"\"\"\n        Release lock on specific resource by removing the annotation\n        \"\"\"\n        current_resource = copy(resource.definition)\n\n        # Only clear annotation if we're the current leader\n        if self.pod_name == current_resource[\"metadata\"].get(\"annotations\", {}).get(\n            LEASE_NAME_ANNOTATION_NAME\n        ):\n            current_resource[\"metadata\"][\"annotations\"][\n                LEASE_NAME_ANNOTATION_NAME\n            ] = None\n            current_resource[\"metadata\"][\"annotations\"][\n                LEASE_TIME_ANNOTATION_NAME\n            ] = None\n            self.deploy_manager.deploy([current_resource])\n\n        return True\n\n    def is_leader(self, resource: Optional[ManagedObject] = None):\n        \"\"\"\n        Determines if current instance is leader\n        \"\"\"\n        if resource:\n            annotations = resource.metadata.get(\"annotations\", {})\n            return self.pod_name == annotations.get(\n                LEASE_NAME_ANNOTATION_NAME\n            ) and self._check_lease_time(annotations.get(LEASE_TIME_ANNOTATION_NAME))\n\n        return True\n\n    def _check_lease_time(\n        self, lease_time: str, current_time: Optional[datetime] = None\n    ) -&gt; bool:\n        \"\"\"Helper function to check if lease time is still valid\n\n        Args:\n            lease_time: str\n                A datetime in isoformat\n            current_time: Optional[datetime]\n                The time to compare the lease_time to. Use datetime.now() if None\n\n        Returns:\n            valid_lease: bool\n                If the lease should still be an owner\n        \"\"\"\n        # Don't default to datetime.now() in function args as that's only evaluated once\n        current_time = current_time or datetime.now()\n        return current_time &lt; datetime.fromisoformat(lease_time) + self.duration_delta\n</code></pre> <code>__init__(deploy_manager=None)</code> <p>Initialize Leadership and gather current name</p> <p>Parameters:</p> Name Type Description Default <code>deploy_manager</code> <code>DeployManagerBase</code> <p>DeployManagerBase = None DeployManager for this Manager</p> <code>None</code> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/annotation.py</code> <pre><code>def __init__(self, deploy_manager: DeployManagerBase = None):\n    \"\"\"Initialize Leadership and gather current name\n\n    Args:\n        deploy_manager: DeployManagerBase = None\n            DeployManager for this Manager\n    \"\"\"\n\n    super().__init__(deploy_manager)\n    self.duration_delta = parse_time_delta(\n        config.python_watch_manager.lock.duration\n    )\n\n    # Gather lock_name, namespace and pod manifest\n    self.pod_name = get_pod_name()\n    assert_config(self.pod_name, \"Unable to detect pod name\")\n</code></pre> <code>acquire(force=False)</code> <p>Return true as leadership is managed at resource level</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/annotation.py</code> <pre><code>def acquire(self, force: bool = False) -&gt; bool:\n    \"\"\"\n    Return true as leadership is managed at resource level\n    \"\"\"\n    return True\n</code></pre> <code>acquire_resource(resource)</code> <p>Check a resource for leadership annotation and add one if it's expired or does not exit</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/annotation.py</code> <pre><code>def acquire_resource(self, resource: ManagedObject):\n    \"\"\"Check a resource for leadership annotation and add one if it's expired\n    or does not exit\"\"\"\n    success, current_resource = self.deploy_manager.get_object_current_state(\n        resource.kind, resource.name, resource.namespace, resource.api_version\n    )\n    if not success or not current_resource:\n        log.warning(\n            \"Unable to fetch owner resource %s/%s/%s/%s\",\n            resource.kind,\n            resource.api_version,\n            resource.namespace,\n            resource.name,\n        )\n        return False\n\n    if \"annotations\" not in current_resource.get(\"metadata\"):\n        current_resource[\"metadata\"][\"annotations\"] = {}\n\n    # Check the current annotation\n    annotations = current_resource[\"metadata\"][\"annotations\"]\n    current_time = datetime.now()\n\n    # If no leader than take ownership\n    if not annotations.get(LEASE_NAME_ANNOTATION_NAME):\n        annotations[LEASE_NAME_ANNOTATION_NAME] = self.pod_name\n        annotations[LEASE_TIME_ANNOTATION_NAME] = current_time.isoformat()\n\n    # If already the current leader then update lease time\n    elif self.pod_name == annotations.get(LEASE_NAME_ANNOTATION_NAME):\n        annotations[LEASE_TIME_ANNOTATION_NAME] = current_time.isoformat()\n\n    # If the current leader's lease has timed out than take ownership\n    elif not self._check_lease_time(\n        annotations.get(LEASE_TIME_ANNOTATION_NAME), current_time\n    ):\n        annotations[LEASE_NAME_ANNOTATION_NAME] = self.pod_name\n        annotations[LEASE_TIME_ANNOTATION_NAME] = current_time.isoformat()\n\n    # Otherwise unable to acquire lock\n    else:\n        return False\n\n    success, _ = self.deploy_manager.deploy([current_resource])\n    if not success:\n        log.warning(\n            \"Unable to update resource annotation%s/%s/%s/%s\",\n            resource.kind,\n            resource.api_version,\n            resource.namespace,\n            resource.name,\n        )\n        return False\n\n    return True\n</code></pre> <code>is_leader(resource=None)</code> <p>Determines if current instance is leader</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/annotation.py</code> <pre><code>def is_leader(self, resource: Optional[ManagedObject] = None):\n    \"\"\"\n    Determines if current instance is leader\n    \"\"\"\n    if resource:\n        annotations = resource.metadata.get(\"annotations\", {})\n        return self.pod_name == annotations.get(\n            LEASE_NAME_ANNOTATION_NAME\n        ) and self._check_lease_time(annotations.get(LEASE_TIME_ANNOTATION_NAME))\n\n    return True\n</code></pre> <code>release()</code> <p>Release lock on global resource</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/annotation.py</code> <pre><code>def release(self):\n    \"\"\"\n    Release lock on global resource\n    \"\"\"\n    return True\n</code></pre> <code>release_resource(resource)</code> <p>Release lock on specific resource by removing the annotation</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/annotation.py</code> <pre><code>def release_resource(self, resource: ManagedObject):\n    \"\"\"\n    Release lock on specific resource by removing the annotation\n    \"\"\"\n    current_resource = copy(resource.definition)\n\n    # Only clear annotation if we're the current leader\n    if self.pod_name == current_resource[\"metadata\"].get(\"annotations\", {}).get(\n        LEASE_NAME_ANNOTATION_NAME\n    ):\n        current_resource[\"metadata\"][\"annotations\"][\n            LEASE_NAME_ANNOTATION_NAME\n        ] = None\n        current_resource[\"metadata\"][\"annotations\"][\n            LEASE_TIME_ANNOTATION_NAME\n        ] = None\n        self.deploy_manager.deploy([current_resource])\n\n    return True\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.leader_election.base","title":"<code>base</code>","text":"<p>Base classes for leader election implementations</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.leader_election.base.LeadershipManagerBase","title":"<code>LeadershipManagerBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for leader election. Leadership election in the PWM is split into two types: global and resource locks. Global locks are required to run any reconciliation while resource locks are required to reconcile a specific resources. Most child classes implement one of these locks.</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>class LeadershipManagerBase(abc.ABC):\n    \"\"\"\n    Base class for leader election. Leadership election in the PWM\n    is split into two types: global and resource locks. Global locks\n    are required to run any reconciliation while resource locks are\n    required to reconcile a specific resources. Most child classes\n    implement one of these locks.\n    \"\"\"\n\n    def __init__(self, deploy_manager: DeployManagerBase = None):\n        \"\"\"\n        Initialize Class\n\n        Args:\n            deploy_manager:  DeployManagerBase\n                DeployManager used in lock acquisition\n        \"\"\"\n        self.deploy_manager = deploy_manager\n\n    ## Lock Interface ####################################################\n    @abc.abstractmethod\n    def acquire(self, force: bool = False) -&gt; bool:\n        \"\"\"\n        Acquire or renew global lock\n\n        Args:\n            force:  bool\n                Whether to force acquire the lock irregardless of status. Used\n                on shutdown\n\n        Returns:\n            success:  bool\n                True on successful acquisition\n        \"\"\"\n\n    @abc.abstractmethod\n    def acquire_resource(self, resource: ManagedObject) -&gt; bool:\n        \"\"\"\n        Acquire or renew lock on specific resource\n\n        Args:\n            resource:  ManagedObject\n                Resource to acquire lock for\n        Returns:\n            success:  bool\n                True on successful acquisition\n        \"\"\"\n\n    @abc.abstractmethod\n    def release(self):\n        \"\"\"\n        Release global lock\n        \"\"\"\n\n    @abc.abstractmethod\n    def release_resource(self, resource: ManagedObject):\n        \"\"\"\n        Release lock on specific resource\n\n        Args:\n            resource:  ManagedObject\n                Resource to release lock for\n        \"\"\"\n\n    @abc.abstractmethod\n    def is_leader(self, resource: Optional[ManagedObject] = None):\n        \"\"\"\n        Determines if current instance is leader\n\n        Args:\n            resource:  Optional[ManagedObject]\n                If provided the resource to determine if current instance\n                is leader for. If no resource if provided then the global\n                lock is checked\n        Returns:\n            leader:  bool\n                True if instance is leader\n        \"\"\"\n</code></pre> <code>__init__(deploy_manager=None)</code> <p>Initialize Class</p> <p>Parameters:</p> Name Type Description Default <code>deploy_manager</code> <code>DeployManagerBase</code> <p>DeployManagerBase DeployManager used in lock acquisition</p> <code>None</code> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>def __init__(self, deploy_manager: DeployManagerBase = None):\n    \"\"\"\n    Initialize Class\n\n    Args:\n        deploy_manager:  DeployManagerBase\n            DeployManager used in lock acquisition\n    \"\"\"\n    self.deploy_manager = deploy_manager\n</code></pre> <code>acquire(force=False)</code> <code>abstractmethod</code> <p>Acquire or renew global lock</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <code>bool</code> <p>bool Whether to force acquire the lock irregardless of status. Used on shutdown</p> <code>False</code> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool True on successful acquisition</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>@abc.abstractmethod\ndef acquire(self, force: bool = False) -&gt; bool:\n    \"\"\"\n    Acquire or renew global lock\n\n    Args:\n        force:  bool\n            Whether to force acquire the lock irregardless of status. Used\n            on shutdown\n\n    Returns:\n        success:  bool\n            True on successful acquisition\n    \"\"\"\n</code></pre> <code>acquire_resource(resource)</code> <code>abstractmethod</code> <p>Acquire or renew lock on specific resource</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>ManagedObject</code> <p>ManagedObject Resource to acquire lock for</p> required <p>Returns:     success:  bool         True on successful acquisition</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>@abc.abstractmethod\ndef acquire_resource(self, resource: ManagedObject) -&gt; bool:\n    \"\"\"\n    Acquire or renew lock on specific resource\n\n    Args:\n        resource:  ManagedObject\n            Resource to acquire lock for\n    Returns:\n        success:  bool\n            True on successful acquisition\n    \"\"\"\n</code></pre> <code>is_leader(resource=None)</code> <code>abstractmethod</code> <p>Determines if current instance is leader</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>Optional[ManagedObject]</code> <p>Optional[ManagedObject] If provided the resource to determine if current instance is leader for. If no resource if provided then the global lock is checked</p> <code>None</code> <p>Returns:     leader:  bool         True if instance is leader</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>@abc.abstractmethod\ndef is_leader(self, resource: Optional[ManagedObject] = None):\n    \"\"\"\n    Determines if current instance is leader\n\n    Args:\n        resource:  Optional[ManagedObject]\n            If provided the resource to determine if current instance\n            is leader for. If no resource if provided then the global\n            lock is checked\n    Returns:\n        leader:  bool\n            True if instance is leader\n    \"\"\"\n</code></pre> <code>release()</code> <code>abstractmethod</code> <p>Release global lock</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>@abc.abstractmethod\ndef release(self):\n    \"\"\"\n    Release global lock\n    \"\"\"\n</code></pre> <code>release_resource(resource)</code> <code>abstractmethod</code> <p>Release lock on specific resource</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>ManagedObject</code> <p>ManagedObject Resource to release lock for</p> required Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>@abc.abstractmethod\ndef release_resource(self, resource: ManagedObject):\n    \"\"\"\n    Release lock on specific resource\n\n    Args:\n        resource:  ManagedObject\n            Resource to release lock for\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.leader_election.base.ThreadedLeaderManagerBase","title":"<code>ThreadedLeaderManagerBase</code>","text":"<p>               Bases: <code>LeadershipManagerBase</code></p> <p>Base class for threaded leadership election. This base class aids in the creation of leadership election classes that require constantly checking or updating a resource. Child classes only need to implement renew_or_acquire, and it will automatically be looped while lock acquisition is needed</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>class ThreadedLeaderManagerBase(LeadershipManagerBase, metaclass=ABCSingletonMeta):\n    \"\"\"\n    Base class for threaded leadership election. This base class aids in the\n    creation of leadership election classes that require constantly checking\n    or updating a resource. Child classes only need to implement renew_or_acquire,\n    and it will automatically be looped while lock acquisition is needed\n    \"\"\"\n\n    def __init__(self, deploy_manager: DeployManagerBase):\n        \"\"\"\n        Initialize class with events to track leadership and shutdown and\n        a lock to ensure renew_or_acquire is only ran once.\n\n        Args:\n            deploy_manager: DeployManagerBase\n                DeployManager for leader election\n        \"\"\"\n        super().__init__(deploy_manager)\n\n        # Events to track status\n        self.leader = threading.Event()\n        self.shutdown = threading.Event()\n\n        # Lock to ensure multiple acquires aren't running at the same time\n        self.run_lock = threading.Lock()\n\n        # Object to track Leadership thread\n        self.leadership_thread = threading.Thread(\n            name=\"leadership_thread\", target=self.run, daemon=True\n        )\n\n        # Calculate threaded poll time:\n        poll_time_delta = parse_time_delta(config.python_watch_manager.lock.poll_time)\n        if not poll_time_delta:\n            log.error(\n                \"Invalid 'python_watch_manager.lock.poll_time' value: '%s'\",\n                config.python_watch_manager.lock.poll_time,\n            )\n            raise ConfigError(\n                \"Invalid 'python_watch_manager.lock.poll_time' value: \"\n                f\"'{config.python_watch_manager.lock.poll_time}'\"\n            )\n        self.poll_time = poll_time_delta.seconds\n\n    ## Public Interface ####################################################\n\n    def renew_or_acquire(self):\n        \"\"\"\n        Renew or acquire leadership lock\n        \"\"\"\n        raise NotImplementedError\n\n    def acquire_lock(self):\n        \"\"\"\n        Helper function for child classes to acquire leadership lock\n        \"\"\"\n        if not self.leader.is_set():\n            log.debug2(\"Acquiring leadership lock\")\n        # Always set the lock during acquire_lock to avoid concurrency issues\n        self.leader.set()\n\n    def release_lock(self):\n        \"\"\"\n        Helper function for child classes to release lock\n        \"\"\"\n        if self.leader.is_set():\n            log.debug2(\"Releasing leadership lock\")\n        self.leader.clear()\n\n    ## Lock Interface ####################################################\n    def acquire(self, force: bool = False):\n        \"\"\"\n        Start/Restart leadership thread or run renew_or_acquire\n\n        Args:\n            force:  bool=False\n                Whether to force acquire the lock\n\n        Returns:\n            success:  bool\n                True on successful acquisition\n        \"\"\"\n        if force:\n            self.leader.set()\n            return True\n\n        # ident is set when thread has started\n        if not self.leadership_thread.is_alive():\n            # Recreate leadership thread if its already exited\n            if self.leadership_thread.ident:\n                self.leadership_thread = threading.Thread(\n                    name=\"leadership_thread\", target=self.run, daemon=True\n                )\n            log.info(\n                \"Starting %s: %s\", self.__class__.__name__, self.leadership_thread.name\n            )\n            self.leadership_thread.start()\n        else:\n            self.run_renew_or_acquire()\n\n        return self.leader.wait()\n\n    def acquire_resource(self, resource: ManagedObject) -&gt; bool:\n        \"\"\"\n        Lock in background so acquire_resource just waits for value\n\n        Args:\n            resource:  ManagedObject\n                Resource that is being locked\n\n        Returns:\n            success:  bool\n                True on successful acquisition else False\n        \"\"\"\n        return self.leader.wait()\n\n    def release(self):\n        \"\"\"\n        Release lock and shutdown leader election thread. This thread\n        first shuts down the background thread before clearing the lock\n        \"\"\"\n        self.shutdown.set()\n        self.leadership_thread.join()\n        self.leader.clear()\n\n    def release_resource(self, resource: ManagedObject):\n        \"\"\"\n        Release resource is not implemented in Threaded classes\n        \"\"\"\n\n    def is_leader(self, resource: Optional[ManagedObject] = None) -&gt; bool:\n        \"\"\"\n        Return if leader event has been acquired\n\n        Returns:\n            leader: bool\n                If instance is current leader\n        \"\"\"\n        return self.leader.is_set()\n\n    ## Implementation Details ####################################################\n\n    def run(self):\n        \"\"\"\n        Loop to continuously run renew or acquire every so often\n        \"\"\"\n        while True:\n            if self.shutdown.is_set():\n                log.debug(\"Shutting down %s Thread\", self.__class__.__name__)\n                return\n\n            self.run_renew_or_acquire()\n            self.shutdown.wait(self.poll_time)\n\n    def run_renew_or_acquire(self):\n        \"\"\"\n        Run renew_or_acquire safely and with threaded lock\n        \"\"\"\n        log.debug2(\"Running renew or acquire for %s lock\", self.__class__.__name__)\n        with self.run_lock:\n            try:\n                self.renew_or_acquire()\n            except Exception as err:\n                log.warning(\n                    \"Error detected while acquiring leadership lock\", exc_info=True\n                )\n                raise RuntimeError(\"Error detected when acquiring lock\") from err\n</code></pre> <code>__init__(deploy_manager)</code> <p>Initialize class with events to track leadership and shutdown and a lock to ensure renew_or_acquire is only ran once.</p> <p>Parameters:</p> Name Type Description Default <code>deploy_manager</code> <code>DeployManagerBase</code> <p>DeployManagerBase DeployManager for leader election</p> required Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>def __init__(self, deploy_manager: DeployManagerBase):\n    \"\"\"\n    Initialize class with events to track leadership and shutdown and\n    a lock to ensure renew_or_acquire is only ran once.\n\n    Args:\n        deploy_manager: DeployManagerBase\n            DeployManager for leader election\n    \"\"\"\n    super().__init__(deploy_manager)\n\n    # Events to track status\n    self.leader = threading.Event()\n    self.shutdown = threading.Event()\n\n    # Lock to ensure multiple acquires aren't running at the same time\n    self.run_lock = threading.Lock()\n\n    # Object to track Leadership thread\n    self.leadership_thread = threading.Thread(\n        name=\"leadership_thread\", target=self.run, daemon=True\n    )\n\n    # Calculate threaded poll time:\n    poll_time_delta = parse_time_delta(config.python_watch_manager.lock.poll_time)\n    if not poll_time_delta:\n        log.error(\n            \"Invalid 'python_watch_manager.lock.poll_time' value: '%s'\",\n            config.python_watch_manager.lock.poll_time,\n        )\n        raise ConfigError(\n            \"Invalid 'python_watch_manager.lock.poll_time' value: \"\n            f\"'{config.python_watch_manager.lock.poll_time}'\"\n        )\n    self.poll_time = poll_time_delta.seconds\n</code></pre> <code>acquire(force=False)</code> <p>Start/Restart leadership thread or run renew_or_acquire</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <code>bool</code> <p>bool=False Whether to force acquire the lock</p> <code>False</code> <p>Returns:</p> Name Type Description <code>success</code> <p>bool True on successful acquisition</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>def acquire(self, force: bool = False):\n    \"\"\"\n    Start/Restart leadership thread or run renew_or_acquire\n\n    Args:\n        force:  bool=False\n            Whether to force acquire the lock\n\n    Returns:\n        success:  bool\n            True on successful acquisition\n    \"\"\"\n    if force:\n        self.leader.set()\n        return True\n\n    # ident is set when thread has started\n    if not self.leadership_thread.is_alive():\n        # Recreate leadership thread if its already exited\n        if self.leadership_thread.ident:\n            self.leadership_thread = threading.Thread(\n                name=\"leadership_thread\", target=self.run, daemon=True\n            )\n        log.info(\n            \"Starting %s: %s\", self.__class__.__name__, self.leadership_thread.name\n        )\n        self.leadership_thread.start()\n    else:\n        self.run_renew_or_acquire()\n\n    return self.leader.wait()\n</code></pre> <code>acquire_lock()</code> <p>Helper function for child classes to acquire leadership lock</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>def acquire_lock(self):\n    \"\"\"\n    Helper function for child classes to acquire leadership lock\n    \"\"\"\n    if not self.leader.is_set():\n        log.debug2(\"Acquiring leadership lock\")\n    # Always set the lock during acquire_lock to avoid concurrency issues\n    self.leader.set()\n</code></pre> <code>acquire_resource(resource)</code> <p>Lock in background so acquire_resource just waits for value</p> <p>Parameters:</p> Name Type Description Default <code>resource</code> <code>ManagedObject</code> <p>ManagedObject Resource that is being locked</p> required <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool True on successful acquisition else False</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>def acquire_resource(self, resource: ManagedObject) -&gt; bool:\n    \"\"\"\n    Lock in background so acquire_resource just waits for value\n\n    Args:\n        resource:  ManagedObject\n            Resource that is being locked\n\n    Returns:\n        success:  bool\n            True on successful acquisition else False\n    \"\"\"\n    return self.leader.wait()\n</code></pre> <code>is_leader(resource=None)</code> <p>Return if leader event has been acquired</p> <p>Returns:</p> Name Type Description <code>leader</code> <code>bool</code> <p>bool If instance is current leader</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>def is_leader(self, resource: Optional[ManagedObject] = None) -&gt; bool:\n    \"\"\"\n    Return if leader event has been acquired\n\n    Returns:\n        leader: bool\n            If instance is current leader\n    \"\"\"\n    return self.leader.is_set()\n</code></pre> <code>release()</code> <p>Release lock and shutdown leader election thread. This thread first shuts down the background thread before clearing the lock</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>def release(self):\n    \"\"\"\n    Release lock and shutdown leader election thread. This thread\n    first shuts down the background thread before clearing the lock\n    \"\"\"\n    self.shutdown.set()\n    self.leadership_thread.join()\n    self.leader.clear()\n</code></pre> <code>release_lock()</code> <p>Helper function for child classes to release lock</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>def release_lock(self):\n    \"\"\"\n    Helper function for child classes to release lock\n    \"\"\"\n    if self.leader.is_set():\n        log.debug2(\"Releasing leadership lock\")\n    self.leader.clear()\n</code></pre> <code>release_resource(resource)</code> <p>Release resource is not implemented in Threaded classes</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>def release_resource(self, resource: ManagedObject):\n    \"\"\"\n    Release resource is not implemented in Threaded classes\n    \"\"\"\n</code></pre> <code>renew_or_acquire()</code> <p>Renew or acquire leadership lock</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>def renew_or_acquire(self):\n    \"\"\"\n    Renew or acquire leadership lock\n    \"\"\"\n    raise NotImplementedError\n</code></pre> <code>run()</code> <p>Loop to continuously run renew or acquire every so often</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>def run(self):\n    \"\"\"\n    Loop to continuously run renew or acquire every so often\n    \"\"\"\n    while True:\n        if self.shutdown.is_set():\n            log.debug(\"Shutting down %s Thread\", self.__class__.__name__)\n            return\n\n        self.run_renew_or_acquire()\n        self.shutdown.wait(self.poll_time)\n</code></pre> <code>run_renew_or_acquire()</code> <p>Run renew_or_acquire safely and with threaded lock</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/base.py</code> <pre><code>def run_renew_or_acquire(self):\n    \"\"\"\n    Run renew_or_acquire safely and with threaded lock\n    \"\"\"\n    log.debug2(\"Running renew or acquire for %s lock\", self.__class__.__name__)\n    with self.run_lock:\n        try:\n            self.renew_or_acquire()\n        except Exception as err:\n            log.warning(\n                \"Error detected while acquiring leadership lock\", exc_info=True\n            )\n            raise RuntimeError(\"Error detected when acquiring lock\") from err\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.leader_election.dry_run","title":"<code>dry_run</code>","text":"<p>Implementation of the DryRun LeaderElection</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.leader_election.dry_run.DryRunLeadershipManager","title":"<code>DryRunLeadershipManager</code>","text":"<p>               Bases: <code>LeadershipManagerBase</code></p> <p>DryRunLeaderElection class implements an empty leadership election manager which always acts as a leader. This is useful for dryrun or running without leadership election</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/dry_run.py</code> <pre><code>class DryRunLeadershipManager(LeadershipManagerBase):\n    \"\"\"DryRunLeaderElection class implements an empty leadership\n    election manager which always acts as a leader. This is useful\n    for dryrun or running without leadership election\"\"\"\n\n    def acquire(self, force: bool = False):\n        \"\"\"\n        Return true as dryrun is always leader\n        \"\"\"\n        return True\n\n    def acquire_resource(self, resource: ManagedObject):\n        \"\"\"\n        Return true as dryrun is always leader\n        \"\"\"\n        return True\n\n    def release(self):\n        \"\"\"\n        NoOp in DryRun as lock is not real\n        \"\"\"\n\n    def release_resource(self, resource: ManagedObject):\n        \"\"\"\n        NoOp in DryRun as lock is not real\n        \"\"\"\n\n    def is_leader(self, resource: Optional[ManagedObject] = None):\n        \"\"\"\n        DryRunLeadershipManager is always leader\n        \"\"\"\n        return True\n</code></pre> <code>acquire(force=False)</code> <p>Return true as dryrun is always leader</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/dry_run.py</code> <pre><code>def acquire(self, force: bool = False):\n    \"\"\"\n    Return true as dryrun is always leader\n    \"\"\"\n    return True\n</code></pre> <code>acquire_resource(resource)</code> <p>Return true as dryrun is always leader</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/dry_run.py</code> <pre><code>def acquire_resource(self, resource: ManagedObject):\n    \"\"\"\n    Return true as dryrun is always leader\n    \"\"\"\n    return True\n</code></pre> <code>is_leader(resource=None)</code> <p>DryRunLeadershipManager is always leader</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/dry_run.py</code> <pre><code>def is_leader(self, resource: Optional[ManagedObject] = None):\n    \"\"\"\n    DryRunLeadershipManager is always leader\n    \"\"\"\n    return True\n</code></pre> <code>release()</code> <p>NoOp in DryRun as lock is not real</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/dry_run.py</code> <pre><code>def release(self):\n    \"\"\"\n    NoOp in DryRun as lock is not real\n    \"\"\"\n</code></pre> <code>release_resource(resource)</code> <p>NoOp in DryRun as lock is not real</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/dry_run.py</code> <pre><code>def release_resource(self, resource: ManagedObject):\n    \"\"\"\n    NoOp in DryRun as lock is not real\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.leader_election.lease","title":"<code>lease</code>","text":"<p>Implementation of the Leader-with-Lease LeaderElection</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.leader_election.lease.LeaderWithLeaseManager","title":"<code>LeaderWithLeaseManager</code>","text":"<p>               Bases: <code>ThreadedLeaderManagerBase</code></p> <p>LeaderWithLeaseManager Class implements the \"leader-with-lease\" operator-sdk lock type. This lock creates a lease object with the operator pod as owner and constantly re-acquires the lock.</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/lease.py</code> <pre><code>class LeaderWithLeaseManager(ThreadedLeaderManagerBase):\n    \"\"\"\n    LeaderWithLeaseManager Class implements the \"leader-with-lease\" operator-sdk\n    lock type. This lock creates a lease object with the operator pod as owner and\n    constantly re-acquires the lock.\n    \"\"\"\n\n    def __init__(self, deploy_manager):\n        \"\"\"\n        Initialize class with lock_name, current namespace, and pod information\n        \"\"\"\n        super().__init__(deploy_manager)\n\n        # Gather lock_name, namespace and pod manifest\n        self.lock_name = (\n            config.operator_name\n            if config.operator_name\n            else config.python_watch_manager.lock.name\n        )\n        self.namespace = get_operator_namespace()\n        self.lock_identity = get_pod_name()\n        assert_config(self.lock_name, \"Unable to detect lock name\")\n        assert_config(self.namespace, \"Unable to detect operator namespace\")\n        assert_config(self.lock_identity, \"Unable to detect lock identity\")\n\n    def renew_or_acquire(self):\n        \"\"\"\n        Renew or acquire lock by checking the current lease status\n        \"\"\"\n\n        # Template out the expected lease. This is edited based on the current\n        # lease status\n        current_time = datetime.now(timezone.utc)\n        lease_resource_version = None\n        expected_lease_data = {\n            \"holderIdentity\": self.lock_identity,\n            \"acquireTime\": current_time.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\n            \"leaseDurationSeconds\": round(\n                parse_time_delta(\n                    config.python_watch_manager.lock.duration\n                ).total_seconds()\n            ),\n            \"leaseTransitions\": 1,\n            \"renewTime\": current_time.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\n        }\n\n        # Get current lease\n        success, lease_obj = self.deploy_manager.get_object_current_state(\n            kind=\"Lease\",\n            name=self.lock_name,\n            namespace=self.namespace,\n            api_version=\"coordination.k8s.io/v1\",\n        )\n        if not success:\n            log.warning(\"Unable to fetch lease %s/%s\", self.namespace, self.lock_name)\n\n        # If lease exists then verify current holder is valid or update the expected\n        # lease with the proper values\n        if lease_obj and lease_obj.get(\"spec\"):\n            log.debug2(\n                \"Lease object %s already exists, checking holder\", self.lock_name\n            )\n\n            lease_resource_version = lease_obj.get(\"metadata\", {}).get(\n                \"resourceVersion\"\n            )\n            lease_spec = lease_obj.get(\"spec\")\n            lock_holder = lease_spec.get(\"holderIdentity\")\n\n            if lock_holder != self.lock_identity:\n                renew_time = parse(lease_spec.get(\"renewTime\"))\n                lease_duration = timedelta(\n                    seconds=lease_spec.get(\"leaseDurationSeconds\")\n                )\n\n                # If the renew+lease is after the current time than the other\n                # lease holder is still valid\n                if (renew_time + lease_duration) &gt; current_time:\n                    self.release_lock()\n                    return\n\n                log.info(\"Taking leadership from %s\", lock_holder)\n                # Increment leaseTransitions as we're taking ownership\n                expected_lease_data[\"leaseTransitions\"] = (\n                    lease_spec.get(\"leaseTransitions\", 1) + 1\n                )\n\n            # If we're the current holder than keep the current acquire time\n            else:\n                log.debug2(\n                    \"Lease object already owned. Reusing acquireTime and transitions\"\n                )\n                expected_lease_data[\"acquireTime\"] = lease_spec.get(\"acquireTime\")\n                expected_lease_data[\"leaseTransitions\"] = lease_spec.get(\n                    \"leaseTransitions\"\n                )\n\n        # Create or update the lease obj\n        lease_resource = {\n            \"kind\": \"Lease\",\n            \"apiVersion\": \"coordination.k8s.io/v1\",\n            \"metadata\": {\n                \"name\": self.lock_name,\n                \"namespace\": self.namespace,\n            },\n            \"spec\": expected_lease_data,\n        }\n        if lease_resource_version:\n            lease_resource[\"metadata\"][\"resourceVersion\"] = lease_resource_version\n\n        success, _ = self.deploy_manager.deploy(\n            [lease_resource], manage_owner_references=False\n        )\n        if not success:\n            log.warning(\"Unable to acquire leadership lock\")\n            self.release_lock()\n        else:\n            self.acquire_lock()\n</code></pre> <code>__init__(deploy_manager)</code> <p>Initialize class with lock_name, current namespace, and pod information</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/lease.py</code> <pre><code>def __init__(self, deploy_manager):\n    \"\"\"\n    Initialize class with lock_name, current namespace, and pod information\n    \"\"\"\n    super().__init__(deploy_manager)\n\n    # Gather lock_name, namespace and pod manifest\n    self.lock_name = (\n        config.operator_name\n        if config.operator_name\n        else config.python_watch_manager.lock.name\n    )\n    self.namespace = get_operator_namespace()\n    self.lock_identity = get_pod_name()\n    assert_config(self.lock_name, \"Unable to detect lock name\")\n    assert_config(self.namespace, \"Unable to detect operator namespace\")\n    assert_config(self.lock_identity, \"Unable to detect lock identity\")\n</code></pre> <code>renew_or_acquire()</code> <p>Renew or acquire lock by checking the current lease status</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/lease.py</code> <pre><code>def renew_or_acquire(self):\n    \"\"\"\n    Renew or acquire lock by checking the current lease status\n    \"\"\"\n\n    # Template out the expected lease. This is edited based on the current\n    # lease status\n    current_time = datetime.now(timezone.utc)\n    lease_resource_version = None\n    expected_lease_data = {\n        \"holderIdentity\": self.lock_identity,\n        \"acquireTime\": current_time.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\n        \"leaseDurationSeconds\": round(\n            parse_time_delta(\n                config.python_watch_manager.lock.duration\n            ).total_seconds()\n        ),\n        \"leaseTransitions\": 1,\n        \"renewTime\": current_time.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\"),\n    }\n\n    # Get current lease\n    success, lease_obj = self.deploy_manager.get_object_current_state(\n        kind=\"Lease\",\n        name=self.lock_name,\n        namespace=self.namespace,\n        api_version=\"coordination.k8s.io/v1\",\n    )\n    if not success:\n        log.warning(\"Unable to fetch lease %s/%s\", self.namespace, self.lock_name)\n\n    # If lease exists then verify current holder is valid or update the expected\n    # lease with the proper values\n    if lease_obj and lease_obj.get(\"spec\"):\n        log.debug2(\n            \"Lease object %s already exists, checking holder\", self.lock_name\n        )\n\n        lease_resource_version = lease_obj.get(\"metadata\", {}).get(\n            \"resourceVersion\"\n        )\n        lease_spec = lease_obj.get(\"spec\")\n        lock_holder = lease_spec.get(\"holderIdentity\")\n\n        if lock_holder != self.lock_identity:\n            renew_time = parse(lease_spec.get(\"renewTime\"))\n            lease_duration = timedelta(\n                seconds=lease_spec.get(\"leaseDurationSeconds\")\n            )\n\n            # If the renew+lease is after the current time than the other\n            # lease holder is still valid\n            if (renew_time + lease_duration) &gt; current_time:\n                self.release_lock()\n                return\n\n            log.info(\"Taking leadership from %s\", lock_holder)\n            # Increment leaseTransitions as we're taking ownership\n            expected_lease_data[\"leaseTransitions\"] = (\n                lease_spec.get(\"leaseTransitions\", 1) + 1\n            )\n\n        # If we're the current holder than keep the current acquire time\n        else:\n            log.debug2(\n                \"Lease object already owned. Reusing acquireTime and transitions\"\n            )\n            expected_lease_data[\"acquireTime\"] = lease_spec.get(\"acquireTime\")\n            expected_lease_data[\"leaseTransitions\"] = lease_spec.get(\n                \"leaseTransitions\"\n            )\n\n    # Create or update the lease obj\n    lease_resource = {\n        \"kind\": \"Lease\",\n        \"apiVersion\": \"coordination.k8s.io/v1\",\n        \"metadata\": {\n            \"name\": self.lock_name,\n            \"namespace\": self.namespace,\n        },\n        \"spec\": expected_lease_data,\n    }\n    if lease_resource_version:\n        lease_resource[\"metadata\"][\"resourceVersion\"] = lease_resource_version\n\n    success, _ = self.deploy_manager.deploy(\n        [lease_resource], manage_owner_references=False\n    )\n    if not success:\n        log.warning(\"Unable to acquire leadership lock\")\n        self.release_lock()\n    else:\n        self.acquire_lock()\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.leader_election.life","title":"<code>life</code>","text":"<p>Implementation of the Leader-for-Life LeaderElection</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.leader_election.life.LeaderForLifeManager","title":"<code>LeaderForLifeManager</code>","text":"<p>               Bases: <code>ThreadedLeaderManagerBase</code></p> <p>LeaderForLifeManager Class implements the old \"leader-for-life\" operator-sdk lock type. This lock creates a configmap with the operator pod as owner in the current namespace. This way when the pod is deleted or list so is the configmap.</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/life.py</code> <pre><code>class LeaderForLifeManager(ThreadedLeaderManagerBase):\n    \"\"\"\n    LeaderForLifeManager Class implements the old \"leader-for-life\" operator-sdk\n    lock type. This lock creates a configmap with the operator pod as owner in\n    the current namespace. This way when the pod is deleted or list so is the\n    configmap.\n    \"\"\"\n\n    def __init__(self, deploy_manager):\n        \"\"\"\n        Initialize class with lock_name, current namespace, and pod information\n        \"\"\"\n        super().__init__(deploy_manager)\n\n        # Gather lock_name, namespace and pod manifest\n        self.lock_name = (\n            config.operator_name\n            if config.operator_name\n            else config.python_watch_manager.lock.name\n        )\n\n        self.namespace = get_operator_namespace()\n        pod_name = get_pod_name()\n        assert_config(self.lock_name, \"Unable to detect lock name\")\n        assert_config(self.namespace, \"Unable to detect operator namespace\")\n        assert_config(pod_name, \"Unable to detect pod name\")\n\n        # Get the current pod context which is used in the lock configmap\n        log.debug(\"Gathering pod context information\")\n        success, pod_obj = self.deploy_manager.get_object_current_state(\n            kind=\"Pod\", name=pod_name, namespace=self.namespace, api_version=\"v1\"\n        )\n        if not success or not pod_obj:\n            log.error(\n                \"Unable to fetch pod %s/%s Unable to use leader-for-life without ownerReference\",\n                self.namespace,\n                pod_name,\n            )\n            raise ConfigError(\n                f\"Unable to fetch pod {self.namespace}/{pod_name}.\"\n                \"Unable to use leader-for-life without ownerReference\"\n            )\n\n        self.pod_manifest = ManagedObject(pod_obj)\n\n    def renew_or_acquire(self):\n        \"\"\"\n        Renew or acquire lock by checking the current configmap status\n        \"\"\"\n        # Get current config map\n        success, cluster_config_map = self.deploy_manager.get_object_current_state(\n            kind=\"ConfigMap\",\n            name=self.lock_name,\n            namespace=self.namespace,\n            api_version=\"v1\",\n        )\n        if not success:\n            log.warning(\n                \"Unable to fetch config map %s/%s\", self.namespace, self.lock_name\n            )\n\n        # If configmap exists then verify owner ref\n        if cluster_config_map:\n            log.debug2(\n                f\"ConfigMap Lock {cluster_config_map} already exists, checking ownership\"\n            )\n            owner_ref_list = nested_get(\n                cluster_config_map, \"metadata.ownerReferences\", []\n            )\n            if len(owner_ref_list) != 1:\n                log.error(\n                    \"Invalid leadership config map detected. Only one owner allowed\"\n                )\n                self.release_lock()\n                return\n\n            if owner_ref_list[0].get(\"uid\") == self.pod_manifest.uid:\n                self.acquire_lock()\n            else:\n                self.release_lock()\n\n        # Create configmap if it doesn't exist\n        else:\n            log.debug2(f\"ConfigMap Lock {cluster_config_map} does not exist, creating\")\n            config_map = {\n                \"kind\": \"ConfigMap\",\n                \"apiVersion\": \"v1\",\n                \"metadata\": {\n                    \"name\": self.lock_name,\n                    \"namespace\": self.namespace,\n                },\n            }\n            update_owner_references(\n                self.deploy_manager, self.pod_manifest.definition, config_map\n            )\n            success, _ = self.deploy_manager.deploy(\n                [config_map], manage_owner_references=False\n            )\n            if not success:\n                log.warning(\"Unable to acquire leadership lock\")\n                self.release_lock()\n            else:\n                self.acquire_lock()\n</code></pre> <code>__init__(deploy_manager)</code> <p>Initialize class with lock_name, current namespace, and pod information</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/life.py</code> <pre><code>def __init__(self, deploy_manager):\n    \"\"\"\n    Initialize class with lock_name, current namespace, and pod information\n    \"\"\"\n    super().__init__(deploy_manager)\n\n    # Gather lock_name, namespace and pod manifest\n    self.lock_name = (\n        config.operator_name\n        if config.operator_name\n        else config.python_watch_manager.lock.name\n    )\n\n    self.namespace = get_operator_namespace()\n    pod_name = get_pod_name()\n    assert_config(self.lock_name, \"Unable to detect lock name\")\n    assert_config(self.namespace, \"Unable to detect operator namespace\")\n    assert_config(pod_name, \"Unable to detect pod name\")\n\n    # Get the current pod context which is used in the lock configmap\n    log.debug(\"Gathering pod context information\")\n    success, pod_obj = self.deploy_manager.get_object_current_state(\n        kind=\"Pod\", name=pod_name, namespace=self.namespace, api_version=\"v1\"\n    )\n    if not success or not pod_obj:\n        log.error(\n            \"Unable to fetch pod %s/%s Unable to use leader-for-life without ownerReference\",\n            self.namespace,\n            pod_name,\n        )\n        raise ConfigError(\n            f\"Unable to fetch pod {self.namespace}/{pod_name}.\"\n            \"Unable to use leader-for-life without ownerReference\"\n        )\n\n    self.pod_manifest = ManagedObject(pod_obj)\n</code></pre> <code>renew_or_acquire()</code> <p>Renew or acquire lock by checking the current configmap status</p> Source code in <code>oper8/watch_manager/python_watch_manager/leader_election/life.py</code> <pre><code>def renew_or_acquire(self):\n    \"\"\"\n    Renew or acquire lock by checking the current configmap status\n    \"\"\"\n    # Get current config map\n    success, cluster_config_map = self.deploy_manager.get_object_current_state(\n        kind=\"ConfigMap\",\n        name=self.lock_name,\n        namespace=self.namespace,\n        api_version=\"v1\",\n    )\n    if not success:\n        log.warning(\n            \"Unable to fetch config map %s/%s\", self.namespace, self.lock_name\n        )\n\n    # If configmap exists then verify owner ref\n    if cluster_config_map:\n        log.debug2(\n            f\"ConfigMap Lock {cluster_config_map} already exists, checking ownership\"\n        )\n        owner_ref_list = nested_get(\n            cluster_config_map, \"metadata.ownerReferences\", []\n        )\n        if len(owner_ref_list) != 1:\n            log.error(\n                \"Invalid leadership config map detected. Only one owner allowed\"\n            )\n            self.release_lock()\n            return\n\n        if owner_ref_list[0].get(\"uid\") == self.pod_manifest.uid:\n            self.acquire_lock()\n        else:\n            self.release_lock()\n\n    # Create configmap if it doesn't exist\n    else:\n        log.debug2(f\"ConfigMap Lock {cluster_config_map} does not exist, creating\")\n        config_map = {\n            \"kind\": \"ConfigMap\",\n            \"apiVersion\": \"v1\",\n            \"metadata\": {\n                \"name\": self.lock_name,\n                \"namespace\": self.namespace,\n            },\n        }\n        update_owner_references(\n            self.deploy_manager, self.pod_manifest.definition, config_map\n        )\n        success, _ = self.deploy_manager.deploy(\n            [config_map], manage_owner_references=False\n        )\n        if not success:\n            log.warning(\"Unable to acquire leadership lock\")\n            self.release_lock()\n        else:\n            self.acquire_lock()\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.python_watch_manager","title":"<code>python_watch_manager</code>","text":"<p>Python-based implementation of the WatchManager</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.python_watch_manager.PythonWatchManager","title":"<code>PythonWatchManager</code>","text":"<p>               Bases: <code>WatchManagerBase</code></p> <p>The PythonWatchManager uses the kubernetes watch client to watch a particular Controller and execute reconciles. It does the following two things</p> <ol> <li>Request a generic watch request for each namespace</li> <li>Start a reconcile thread to start reconciliation subprocesses</li> </ol> Source code in <code>oper8/watch_manager/python_watch_manager/python_watch_manager.py</code> <pre><code>class PythonWatchManager(WatchManagerBase):\n    \"\"\"The PythonWatchManager uses the kubernetes watch client to watch\n    a particular Controller and execute reconciles. It does the following\n    two things\n\n    1. Request a generic watch request for each namespace\n    2. Start a reconcile thread to start reconciliation subprocesses\n    \"\"\"\n\n    def __init__(\n        self,\n        controller_type: Type[Controller],\n        deploy_manager: Optional[OpenshiftDeployManager] = None,\n        namespace_list: Optional[List[str]] = None,\n    ):\n        \"\"\"Initialize the required threads and submit the watch requests\n        Args:\n            controller_type: Type[Controller]\n                The controller to be watched\n            deploy_manager: Optional[OpenshiftDeployManager] = None\n                An optional DeployManager override\n            namespace_list: Optional[List[str]] = []\n                A list of namespaces to watch\n        \"\"\"\n        super().__init__(controller_type)\n\n        # Handle functional args\n        if deploy_manager is None:\n            log.debug(\"Using OpenshiftDeployManager\")\n            deploy_manager = OpenshiftDeployManager()\n        self.deploy_manager = deploy_manager\n\n        # Setup watch namespace\n        self.namespace_list = namespace_list or []\n        if not namespace_list and config.watch_namespace != \"\":\n            self.namespace_list = config.watch_namespace.split(\",\")\n\n        # Setup Control variables\n        self.shutdown = threading.Event()\n\n        # Setup Threads. These are both singleton instances and will be\n        # the same across all PythonWatchManagers\n        self.leadership_manager: LeadershipManagerBase = get_leader_election_class()(\n            self.deploy_manager\n        )\n        self.reconcile_thread: ReconcileThread = ReconcileThread(\n            deploy_manager=self.deploy_manager,\n            leadership_manager=self.leadership_manager,\n        )\n        self.heartbeat_thread: Optional[HeartbeatThread] = None\n        if config.python_watch_manager.heartbeat_file:\n            self.heartbeat_thread = HeartbeatThread(\n                config.python_watch_manager.heartbeat_file,\n                config.python_watch_manager.heartbeat_period,\n            )\n\n        # Start thread for each resource watch\n        self.controller_watches: List[WatchThread] = []\n        if len(self.namespace_list) == 0 or \"*\" in self.namespace_list:\n            self.controller_watches.append(self._add_resource_watch())\n        else:\n            for namespace in self.namespace_list:\n                self.controller_watches.append(self._add_resource_watch(namespace))\n\n    ## Interface ###############################################################\n\n    def watch(self) -&gt; bool:\n        \"\"\"Check for leadership and start all threads\n\n        Returns:\n            success:  bool\n                True if all threads process are running correctly\n        \"\"\"\n        log.info(\"Starting PythonWatchManager: %s\", self)\n\n        if not self.leadership_manager.is_leader():\n            log.debug(\"Acquiring Leadership lock before starting %s\", self)\n            self.leadership_manager.acquire()\n\n        # If watch has been shutdown then exit before starting threads\n        if self.shutdown.is_set():\n            return False\n\n        # Start reconcile thread and all watch threads\n        self.reconcile_thread.start_thread()\n        for watch_thread in self.controller_watches:\n            log.debug(\"Starting watch_thread: %s\", watch_thread)\n            watch_thread.start_thread()\n        if self.heartbeat_thread:\n            log.debug(\"Starting heartbeat_thread\")\n            self.heartbeat_thread.start_thread()\n        return True\n\n    def wait(self):\n        \"\"\"Wait shutdown to be signaled\"\"\"\n        self.shutdown.wait()\n\n    def stop(self):\n        \"\"\"Stop all threads. This waits for all reconciles\n        to finish\n        \"\"\"\n\n        log.info(\n            \"Stopping PythonWatchManager for %s/%s/%s\",\n            self.group,\n            self.version,\n            self.kind,\n        )\n\n        # Set shutdown and acquire leadership to clear any deadlocks\n        self.shutdown.set()\n        self.leadership_manager.acquire(force=True)\n\n        # Stop all threads\n        for watch in get_resource_watches():\n            watch.stop_thread()\n        self.reconcile_thread.stop_thread()\n        self.leadership_manager.release()\n        if self.heartbeat_thread:\n            self.heartbeat_thread.stop_thread()\n\n    ## Helper Functions ###############################################################\n\n    def _add_resource_watch(self, namespace: Optional[str] = None):\n        \"\"\"Request a generic watch request. Optionally for a specific namespace\n\n        Args:\n            namespace: Optional[str] = None\n                An optional namespace to watch\n        \"\"\"\n        log.debug3(\"Adding %s request for %s\", namespace if namespace else \"\", self)\n\n        # In the global watch manager the controller is both\n        # the watched and the requested objects\n        resource_id = ResourceId.from_controller(self.controller_type, namespace)\n        request = WatchRequest(\n            controller_type=self.controller_type,\n            watched=resource_id,\n            requester=resource_id,\n            filters=get_filters_for_resource_id(self.controller_type, resource_id),\n        )\n        return create_resource_watch(\n            request,\n            self.reconcile_thread,\n            self.deploy_manager,\n            self.leadership_manager,\n        )\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.python_watch_manager.PythonWatchManager.__init__","title":"<code>__init__(controller_type, deploy_manager=None, namespace_list=None)</code>","text":"<p>Initialize the required threads and submit the watch requests Args:     controller_type: Type[Controller]         The controller to be watched     deploy_manager: Optional[OpenshiftDeployManager] = None         An optional DeployManager override     namespace_list: Optional[List[str]] = []         A list of namespaces to watch</p> Source code in <code>oper8/watch_manager/python_watch_manager/python_watch_manager.py</code> <pre><code>def __init__(\n    self,\n    controller_type: Type[Controller],\n    deploy_manager: Optional[OpenshiftDeployManager] = None,\n    namespace_list: Optional[List[str]] = None,\n):\n    \"\"\"Initialize the required threads and submit the watch requests\n    Args:\n        controller_type: Type[Controller]\n            The controller to be watched\n        deploy_manager: Optional[OpenshiftDeployManager] = None\n            An optional DeployManager override\n        namespace_list: Optional[List[str]] = []\n            A list of namespaces to watch\n    \"\"\"\n    super().__init__(controller_type)\n\n    # Handle functional args\n    if deploy_manager is None:\n        log.debug(\"Using OpenshiftDeployManager\")\n        deploy_manager = OpenshiftDeployManager()\n    self.deploy_manager = deploy_manager\n\n    # Setup watch namespace\n    self.namespace_list = namespace_list or []\n    if not namespace_list and config.watch_namespace != \"\":\n        self.namespace_list = config.watch_namespace.split(\",\")\n\n    # Setup Control variables\n    self.shutdown = threading.Event()\n\n    # Setup Threads. These are both singleton instances and will be\n    # the same across all PythonWatchManagers\n    self.leadership_manager: LeadershipManagerBase = get_leader_election_class()(\n        self.deploy_manager\n    )\n    self.reconcile_thread: ReconcileThread = ReconcileThread(\n        deploy_manager=self.deploy_manager,\n        leadership_manager=self.leadership_manager,\n    )\n    self.heartbeat_thread: Optional[HeartbeatThread] = None\n    if config.python_watch_manager.heartbeat_file:\n        self.heartbeat_thread = HeartbeatThread(\n            config.python_watch_manager.heartbeat_file,\n            config.python_watch_manager.heartbeat_period,\n        )\n\n    # Start thread for each resource watch\n    self.controller_watches: List[WatchThread] = []\n    if len(self.namespace_list) == 0 or \"*\" in self.namespace_list:\n        self.controller_watches.append(self._add_resource_watch())\n    else:\n        for namespace in self.namespace_list:\n            self.controller_watches.append(self._add_resource_watch(namespace))\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.python_watch_manager.PythonWatchManager.stop","title":"<code>stop()</code>","text":"<p>Stop all threads. This waits for all reconciles to finish</p> Source code in <code>oper8/watch_manager/python_watch_manager/python_watch_manager.py</code> <pre><code>def stop(self):\n    \"\"\"Stop all threads. This waits for all reconciles\n    to finish\n    \"\"\"\n\n    log.info(\n        \"Stopping PythonWatchManager for %s/%s/%s\",\n        self.group,\n        self.version,\n        self.kind,\n    )\n\n    # Set shutdown and acquire leadership to clear any deadlocks\n    self.shutdown.set()\n    self.leadership_manager.acquire(force=True)\n\n    # Stop all threads\n    for watch in get_resource_watches():\n        watch.stop_thread()\n    self.reconcile_thread.stop_thread()\n    self.leadership_manager.release()\n    if self.heartbeat_thread:\n        self.heartbeat_thread.stop_thread()\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.python_watch_manager.PythonWatchManager.wait","title":"<code>wait()</code>","text":"<p>Wait shutdown to be signaled</p> Source code in <code>oper8/watch_manager/python_watch_manager/python_watch_manager.py</code> <pre><code>def wait(self):\n    \"\"\"Wait shutdown to be signaled\"\"\"\n    self.shutdown.wait()\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.python_watch_manager.PythonWatchManager.watch","title":"<code>watch()</code>","text":"<p>Check for leadership and start all threads</p> <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool True if all threads process are running correctly</p> Source code in <code>oper8/watch_manager/python_watch_manager/python_watch_manager.py</code> <pre><code>def watch(self) -&gt; bool:\n    \"\"\"Check for leadership and start all threads\n\n    Returns:\n        success:  bool\n            True if all threads process are running correctly\n    \"\"\"\n    log.info(\"Starting PythonWatchManager: %s\", self)\n\n    if not self.leadership_manager.is_leader():\n        log.debug(\"Acquiring Leadership lock before starting %s\", self)\n        self.leadership_manager.acquire()\n\n    # If watch has been shutdown then exit before starting threads\n    if self.shutdown.is_set():\n        return False\n\n    # Start reconcile thread and all watch threads\n    self.reconcile_thread.start_thread()\n    for watch_thread in self.controller_watches:\n        log.debug(\"Starting watch_thread: %s\", watch_thread)\n        watch_thread.start_thread()\n    if self.heartbeat_thread:\n        log.debug(\"Starting heartbeat_thread\")\n        self.heartbeat_thread.start_thread()\n    return True\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.reconcile_process_entrypoint","title":"<code>reconcile_process_entrypoint</code>","text":"<p>ReconcileProcessEntrypoint for all PWM reconciles</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.reconcile_process_entrypoint.ReconcileProcessDeployManager","title":"<code>ReconcileProcessDeployManager</code>","text":"<p>               Bases: <code>OpenshiftDeployManager</code></p> <p>ReconcileProcessEntrypoint deploy manager is a helper deploy manager that allows the PWM to insert functionality during a reconcile. This is used for things like watching dependent resources and subsystem rollout</p> Source code in <code>oper8/watch_manager/python_watch_manager/reconcile_process_entrypoint.py</code> <pre><code>class ReconcileProcessDeployManager(OpenshiftDeployManager):\n    \"\"\"ReconcileProcessEntrypoint deploy manager is a helper deploy manager\n    that allows the PWM to insert functionality during a reconcile. This\n    is used for things like watching dependent resources and subsystem rollout\"\"\"\n\n    def __init__(\n        self,\n        controller_type: Type[Controller],\n        controller_resource: aconfig.Config,\n        result_pipe: Connection,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Initalize the ReconcileProcessEntrypoint DeployManger and gather start-up configurations\n\n        Args:\n            controller_type: Type[Controller]\n                The Controller being reconciled\n            controller_resource: aconfig.Config\n                The resource being reconciled\n            result_pipe: Connection\n                The pipe to send dependent watch requests to\n            *args:\n                Extendable arguments to pass to to parent\n            **kwargs:\n                Extendable key word arguments to pass to parent\n        \"\"\"\n        # Initialize ReconcileProcessEntrypoint Deploy Manager\n        super().__init__(*args, owner_cr=controller_resource, **kwargs)\n\n        # Initialize required variables\n        self.requested_watches = set()\n        self.result_pipe = result_pipe\n        self.controller_type = controller_type\n\n        # Setup Subsystems\n        self.subsystems = self._gather_subsystems(controller_type)\n        self.reconcile_manager = ReconcileManager(deploy_manager=self)\n\n    # Functional Overrides\n\n    def _apply_resource(self, resource_definition: dict) -&gt; dict:\n        \"\"\"Override apply resource for handling watch_dependent_resources and subsystem rollout\"\"\"\n        resource = super()._apply_resource(resource_definition)\n        resource_id = ResourceId.from_resource(resource)\n\n        # Send watch request if watch_dependent_resources is enabled\n        # and/or handle subsystem rollout\n        if config.python_watch_manager.watch_dependent_resources:\n            log.debug2(\"Handling dependent resource %s\", resource_id)\n            self._handle_dependent_resource(resource_id)\n\n        if (\n            config.python_watch_manager.subsystem_rollout\n            and resource_id.global_id in self.subsystems\n        ):\n            log.debug2(\"Rolling out subsystem %s\", resource_id.global_id)\n            self._handle_subsystem(\n                resource, self.subsystems[resource_id.global_id], False\n            )\n\n        return resource\n\n    def _replace_resource(self, resource_definition: dict) -&gt; dict:\n        \"\"\"Override replace resource for handling watch_dependent_resources and subsystem rollout\"\"\"\n        resource = super()._replace_resource(resource_definition)\n        resource_id = ResourceId.from_resource(resource)\n\n        # Send watch request if watch_dependent_resources is enabled\n        # and/or handle subsystem rollout\n        if config.python_watch_manager.watch_dependent_resources:\n            log.debug2(\"Handling dependent resource %s\", resource_id)\n            self._handle_dependent_resource(resource_id)\n\n        if (\n            config.python_watch_manager.subsystem_rollout\n            and resource_id.global_id in self.subsystems\n        ):\n            log.debug2(\"Rolling out subsystem %s\", resource_id.global_id)\n            self._handle_subsystem(\n                resource, self.subsystems[resource_id.global_id], False\n            )\n\n        return resource\n\n    def _disable(self, resource_definition: dict) -&gt; bool:\n        \"\"\"Override disable to insert subsystem logic\"\"\"\n\n        changed = super()._disable(resource_definition)\n        if not changed:\n            return changed\n\n        resource_id = ResourceId.from_resource(resource_definition)\n\n        # If deleted resource is a subsystem then run reconcile with finalizer\n        if (\n            config.python_watch_manager.subsystem_rollout\n            and resource_id.global_id in self.subsystems\n        ):\n            success, current_state = self.get_object_current_state(\n                kind=resource_id.kind,\n                name=resource_id.name,\n                namespace=resource_id.namespace,\n                api_version=resource_id.api_version,\n            )\n            if not success or not current_state:\n                log.warning(\n                    \"Unable to fetch owner resource %s/%s/%s/%s\",\n                    resource_id.kind,\n                    resource_id.api_version,\n                    resource_id.namespace,\n                    resource_id.name,\n                )\n                return changed\n\n            self._handle_subsystem(\n                current_state, self.subsystems[resource_id.global_id], True\n            )\n\n        return changed\n\n    def _handle_subsystem(self, resource, controller_type, is_finalizer):\n        \"\"\"Handle rolling out a subsystem for a specific controller, resource, and finalizer\"\"\"\n\n        # Copy a ref of the current logging format to restore to\n        log_formatters = {}\n        for handler in logging.getLogger().handlers:\n            log_formatters[handler] = handler.formatter\n\n        # Update the current owner\n        current_owner = self._owner_cr\n        self._owner_cr = resource\n        current_controller_type = self.controller_type\n        self.controller_type = controller_type\n\n        # Add the new controllers subsystems to the current dictionary\n        # this simplifies future look ups\n        current_subsystems = self.subsystems\n        self.subsystems = (self._gather_subsystems(controller_type),)\n\n        self.reconcile_manager.safe_reconcile(controller_type, resource, is_finalizer)\n\n        # Reset owner_cr, logging, and subsystems\n        self._owner_cr = current_owner\n        self.controller_type = current_controller_type\n        self.subsystems = current_subsystems\n        for handler, formatter in log_formatters.items():\n            handler.setFormatter(formatter)\n\n    def _handle_dependent_resource(self, watched_id):\n        \"\"\"Handling request a watch for a deployed resource\"\"\"\n        # Create requester id\n        resource_id = ResourceId.from_resource(self._owner_cr)\n\n        # Remove name from watched_id so it captures\n        # any resource of that kind with this owner\n        watched_id = copy.deepcopy(watched_id)\n        watched_id = dataclasses.replace(watched_id, name=None)\n\n        filters = DependentWatchFilter\n        if controller_filters := get_filters_for_resource_id(\n            self.controller_type, watched_id\n        ):\n            filters = AndFilter(DependentWatchFilter, controller_filters)\n        watch_filters = FilterManager.to_info(filters)\n\n        watch_request = WatchRequest(\n            requester=resource_id,\n            watched=watched_id,\n            # Use controller info to avoid issues between vcs and pickling\n            controller_info=ClassInfo.from_type(self.controller_type),\n            filters_info=watch_filters,\n        )\n\n        # Only send each watch request once\n        if watch_request not in self.requested_watches:\n            log.debug3(f\"Sending watch request {watch_request}\")\n            self.result_pipe.send(watch_request)\n            self.requested_watches.add(watch_request)\n\n    def _gather_subsystems(self, controller_type: Type[Controller]):\n        \"\"\"Gather the list of subsystems for a controller\"\"\"\n        subsystem_controllers = getattr(controller_type, \"pwm_subsystems\", [])\n        subsystems = {\n            ResourceId.from_controller(controller).global_id: controller\n            for controller in subsystem_controllers\n        }\n        log.debug3(f\"Gathered subsystems: {subsystems}\")\n        return subsystems\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.reconcile_process_entrypoint.ReconcileProcessDeployManager.__init__","title":"<code>__init__(controller_type, controller_resource, result_pipe, *args, **kwargs)</code>","text":"<p>Initalize the ReconcileProcessEntrypoint DeployManger and gather start-up configurations</p> <p>Parameters:</p> Name Type Description Default <code>controller_type</code> <code>Type[Controller]</code> <p>Type[Controller] The Controller being reconciled</p> required <code>controller_resource</code> <code>Config</code> <p>aconfig.Config The resource being reconciled</p> required <code>result_pipe</code> <code>Connection</code> <p>Connection The pipe to send dependent watch requests to</p> required <code>*args</code> <p>Extendable arguments to pass to to parent</p> <code>()</code> <code>**kwargs</code> <p>Extendable key word arguments to pass to parent</p> <code>{}</code> Source code in <code>oper8/watch_manager/python_watch_manager/reconcile_process_entrypoint.py</code> <pre><code>def __init__(\n    self,\n    controller_type: Type[Controller],\n    controller_resource: aconfig.Config,\n    result_pipe: Connection,\n    *args,\n    **kwargs,\n):\n    \"\"\"Initalize the ReconcileProcessEntrypoint DeployManger and gather start-up configurations\n\n    Args:\n        controller_type: Type[Controller]\n            The Controller being reconciled\n        controller_resource: aconfig.Config\n            The resource being reconciled\n        result_pipe: Connection\n            The pipe to send dependent watch requests to\n        *args:\n            Extendable arguments to pass to to parent\n        **kwargs:\n            Extendable key word arguments to pass to parent\n    \"\"\"\n    # Initialize ReconcileProcessEntrypoint Deploy Manager\n    super().__init__(*args, owner_cr=controller_resource, **kwargs)\n\n    # Initialize required variables\n    self.requested_watches = set()\n    self.result_pipe = result_pipe\n    self.controller_type = controller_type\n\n    # Setup Subsystems\n    self.subsystems = self._gather_subsystems(controller_type)\n    self.reconcile_manager = ReconcileManager(deploy_manager=self)\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.reconcile_process_entrypoint.ReconcileProcessEntrypoint","title":"<code>ReconcileProcessEntrypoint</code>","text":"<p>The ReconcileProcessEntrypoint Class is the main start place for a reconciliation. It configures some watch manager specific settings like multiprocess logging, and signal handling then it hands off control to the ReconcileManager</p> Source code in <code>oper8/watch_manager/python_watch_manager/reconcile_process_entrypoint.py</code> <pre><code>class ReconcileProcessEntrypoint:  # pylint: disable=too-few-public-methods\n    \"\"\"The ReconcileProcessEntrypoint Class is the main start place for a\n    reconciliation. It configures some watch manager specific settings like\n    multiprocess logging, and signal handling then it hands off control to the\n    ReconcileManager\"\"\"\n\n    def __init__(\n        self,\n        controller_type: Type[Controller],\n        deploy_manager: DeployManagerBase = None,\n    ):\n        \"\"\"Initializer for the entrypoint class\n\n        Args:\n            controller_type: Type[Controller]\n                The Controller type being reconciled\n            deploy_manager: DeployManagerBase = None\n                An optional deploy manager override\n        \"\"\"\n        self.controller_type = controller_type\n        self.deploy_manager = deploy_manager\n\n        # Initialize the reconcile manager in start\n        self.reconcile_manager = None\n\n    def start(\n        self,\n        request: ReconcileRequest,\n        result_pipe: Connection,\n    ):\n        \"\"\"Main entrypoint for the class\n\n        Args:\n            request: ReconcileRequest\n                The reconcile request that trigger this reconciliation\n            result_pipe: Connection\n                The connection to send results back to\n        \"\"\"\n        # Parse the request and setup local variables\n        log.debug4(\"Setting up resource\")\n        resource = request.resource\n        resource_id = ResourceId.from_resource(resource)\n\n        # Set a unique thread name for each reconcile\n        thread_name = f\"entrypoint_{resource_id.get_id()}/{resource_id.name}\"\n        log.debug4(\"Setting thread name: %s\", thread_name)\n        threading.current_thread().name = thread_name\n\n        # Reset signal handlers to default function\n        log.debug4(\"Resetting signals\")\n        signal.signal(signal.SIGINT, signal.SIG_DFL)\n        signal.signal(signal.SIGTERM, signal.SIG_DFL)\n\n        # Replace stdout and stderr with a null stream as all messages should be passed via\n        # the queue and any data in the buffer could cause the process to hang. This can\n        # make it difficult to debug subprocesses if they fail before setting up the handler\n        log.debug4(\"Redirecting to /dev/null\")\n        with open(os.devnull, \"w\", encoding=\"utf-8\") as null_file:\n            sys.stdout = null_file\n            sys.stderr = null_file\n\n            log.info(\n                \"ReconcileProcessEntrypoint for %s and with type: %s\",\n                self.controller_type,\n                request.type,\n            )\n\n            # If controller_type has subsystems than set reconciliation to standalone mode.\n            # This forces the reconcile to be single threaded but allows for recursive reconciles\n            log.debug4(\"Checking for subsystem rollout\")\n            if (\n                getattr(self.controller_type, \"pwm_subsystems\", [])\n                and config.python_watch_manager.subsystem_rollout\n            ):\n                config.standalone = True\n\n            # Create a custom deploy manager so we can insert functionality\n            deploy_manager = self.deploy_manager\n            if not deploy_manager:\n                deploy_manager = ReconcileProcessDeployManager(\n                    result_pipe=result_pipe,\n                    controller_resource=resource.definition,\n                    controller_type=self.controller_type,\n                )\n\n            # Create a reconciliation manager and start the reconcile\n            self.reconcile_manager = ReconcileManager(deploy_manager=deploy_manager)\n\n            finalize = request.type == KubeEventType.DELETED or resource.metadata.get(\n                \"deletionTimestamp\"\n            )\n            reconcile_result = self.reconcile_manager.safe_reconcile(\n                self.controller_type,\n                resource.definition,\n                finalize,\n            )\n            # Clear exception as it's not always pickleable\n            reconcile_result.exception = None\n\n            # Complete the reconcile by sending the result back up the pipe\n            # and explicitly close all remaining descriptors\n            log.info(\"Finished Reconcile for %s\", resource_id)\n            log.debug3(\"Sending reconciliation result back to main process\")\n            result_pipe.send(reconcile_result)\n            result_pipe.close()\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.reconcile_process_entrypoint.ReconcileProcessEntrypoint.__init__","title":"<code>__init__(controller_type, deploy_manager=None)</code>","text":"<p>Initializer for the entrypoint class</p> <p>Parameters:</p> Name Type Description Default <code>controller_type</code> <code>Type[Controller]</code> <p>Type[Controller] The Controller type being reconciled</p> required <code>deploy_manager</code> <code>DeployManagerBase</code> <p>DeployManagerBase = None An optional deploy manager override</p> <code>None</code> Source code in <code>oper8/watch_manager/python_watch_manager/reconcile_process_entrypoint.py</code> <pre><code>def __init__(\n    self,\n    controller_type: Type[Controller],\n    deploy_manager: DeployManagerBase = None,\n):\n    \"\"\"Initializer for the entrypoint class\n\n    Args:\n        controller_type: Type[Controller]\n            The Controller type being reconciled\n        deploy_manager: DeployManagerBase = None\n            An optional deploy manager override\n    \"\"\"\n    self.controller_type = controller_type\n    self.deploy_manager = deploy_manager\n\n    # Initialize the reconcile manager in start\n    self.reconcile_manager = None\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.reconcile_process_entrypoint.ReconcileProcessEntrypoint.start","title":"<code>start(request, result_pipe)</code>","text":"<p>Main entrypoint for the class</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ReconcileRequest</code> <p>ReconcileRequest The reconcile request that trigger this reconciliation</p> required <code>result_pipe</code> <code>Connection</code> <p>Connection The connection to send results back to</p> required Source code in <code>oper8/watch_manager/python_watch_manager/reconcile_process_entrypoint.py</code> <pre><code>def start(\n    self,\n    request: ReconcileRequest,\n    result_pipe: Connection,\n):\n    \"\"\"Main entrypoint for the class\n\n    Args:\n        request: ReconcileRequest\n            The reconcile request that trigger this reconciliation\n        result_pipe: Connection\n            The connection to send results back to\n    \"\"\"\n    # Parse the request and setup local variables\n    log.debug4(\"Setting up resource\")\n    resource = request.resource\n    resource_id = ResourceId.from_resource(resource)\n\n    # Set a unique thread name for each reconcile\n    thread_name = f\"entrypoint_{resource_id.get_id()}/{resource_id.name}\"\n    log.debug4(\"Setting thread name: %s\", thread_name)\n    threading.current_thread().name = thread_name\n\n    # Reset signal handlers to default function\n    log.debug4(\"Resetting signals\")\n    signal.signal(signal.SIGINT, signal.SIG_DFL)\n    signal.signal(signal.SIGTERM, signal.SIG_DFL)\n\n    # Replace stdout and stderr with a null stream as all messages should be passed via\n    # the queue and any data in the buffer could cause the process to hang. This can\n    # make it difficult to debug subprocesses if they fail before setting up the handler\n    log.debug4(\"Redirecting to /dev/null\")\n    with open(os.devnull, \"w\", encoding=\"utf-8\") as null_file:\n        sys.stdout = null_file\n        sys.stderr = null_file\n\n        log.info(\n            \"ReconcileProcessEntrypoint for %s and with type: %s\",\n            self.controller_type,\n            request.type,\n        )\n\n        # If controller_type has subsystems than set reconciliation to standalone mode.\n        # This forces the reconcile to be single threaded but allows for recursive reconciles\n        log.debug4(\"Checking for subsystem rollout\")\n        if (\n            getattr(self.controller_type, \"pwm_subsystems\", [])\n            and config.python_watch_manager.subsystem_rollout\n        ):\n            config.standalone = True\n\n        # Create a custom deploy manager so we can insert functionality\n        deploy_manager = self.deploy_manager\n        if not deploy_manager:\n            deploy_manager = ReconcileProcessDeployManager(\n                result_pipe=result_pipe,\n                controller_resource=resource.definition,\n                controller_type=self.controller_type,\n            )\n\n        # Create a reconciliation manager and start the reconcile\n        self.reconcile_manager = ReconcileManager(deploy_manager=deploy_manager)\n\n        finalize = request.type == KubeEventType.DELETED or resource.metadata.get(\n            \"deletionTimestamp\"\n        )\n        reconcile_result = self.reconcile_manager.safe_reconcile(\n            self.controller_type,\n            resource.definition,\n            finalize,\n        )\n        # Clear exception as it's not always pickleable\n        reconcile_result.exception = None\n\n        # Complete the reconcile by sending the result back up the pipe\n        # and explicitly close all remaining descriptors\n        log.info(\"Finished Reconcile for %s\", resource_id)\n        log.debug3(\"Sending reconciliation result back to main process\")\n        result_pipe.send(reconcile_result)\n        result_pipe.close()\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.reconcile_process_entrypoint.create_and_start_entrypoint","title":"<code>create_and_start_entrypoint(logging_queue, request, result_pipe, deploy_manager=None)</code>","text":"<p>Function to create and start an entrypoint while catching any unexpected errors Args:     logging_queue: multiprocessing.Queue         The queue to send log messages to     request: ReconcileRequest         The request that triggered this reconciliation     result_pipe: Connection         The pipe to send a result back with     deploy_manager: DeployManagerBase = None         An optional DeployManager override</p> Source code in <code>oper8/watch_manager/python_watch_manager/reconcile_process_entrypoint.py</code> <pre><code>def create_and_start_entrypoint(\n    logging_queue: multiprocessing.Queue,\n    request: ReconcileRequest,\n    result_pipe: Connection,\n    deploy_manager: DeployManagerBase = None,\n):\n    \"\"\"Function to create and start an entrypoint while catching any unexpected errors\n    Args:\n        logging_queue: multiprocessing.Queue\n            The queue to send log messages to\n        request: ReconcileRequest\n            The request that triggered this reconciliation\n        result_pipe: Connection\n            The pipe to send a result back with\n        deploy_manager: DeployManagerBase = None\n            An optional DeployManager override\n    \"\"\"\n    try:\n        # Set the logging library to utilize the multiprocessing logging queue. Do this before\n        # any logging messages are sent since that might cause a deadlock\n        root_logger = logging.getLogger()\n        root_logger.handlers.clear()\n        handler = LogQueueHandler(logging_queue, request.resource)\n        root_logger.addHandler(handler)\n\n        log.debug3(\"Creating entrypoint\")\n        entry = ReconcileProcessEntrypoint(\n            request.controller_type, deploy_manager=deploy_manager\n        )\n        log.debug3(\"Starting entrypoint\")\n        entry.start(request, result_pipe)\n    except Exception as exc:  # pylint: disable=broad-exception-caught\n        log.error(\"Uncaught exception '%s'\", exc, exc_info=True)\n\n    # Close the logging queue to ensure all messages are sent before process end\n    logging_queue.close()\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.threads","title":"<code>threads</code>","text":"<p>Import the ThreadBase and subclasses</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.threads.base","title":"<code>base</code>","text":"<p>Module for the ThreadBase Class</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.threads.base.ThreadBase","title":"<code>ThreadBase</code>","text":"<p>               Bases: <code>Thread</code></p> <p>Base class for all other thread classes. This class handles generic starting, stopping, and leadership functions</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/base.py</code> <pre><code>class ThreadBase(threading.Thread):\n    \"\"\"Base class for all other thread classes. This class handles generic starting, stopping,\n    and leadership functions\"\"\"\n\n    def __init__(\n        self,\n        name: str = None,\n        daemon: bool = None,\n        deploy_manager: DeployManagerBase = None,\n        leadership_manager: LeadershipManagerBase = None,\n    ):\n        \"\"\"Initialize class and store required instance variables. This function\n        is normally overriden by subclasses that pass in static name/daemon variables\n\n        Args:\n            name:str=None\n                The name of the thread to manager\n            daemon:bool=None\n                Whether python should wait for this thread to stop before exiting\n            deploy_manager: DeployManagerBase = None\n                The deploy manager available to this thread during start()\n            leadership_manager: LeadershipManagerBase = None\n                The leadership_manager for tracking elections\n        \"\"\"\n        self.deploy_manager = deploy_manager\n        self.leadership_manager = leadership_manager or DryRunLeadershipManager()\n        self.shutdown = threading.Event()\n        super().__init__(name=name, daemon=daemon)\n\n    ## Abstract Interface ######################################################\n    #\n    # These functions must be implemented by child classes\n    ##\n    def run(self):\n        \"\"\"Control loop for the thread. Once this function exits the thread stops\"\"\"\n        raise NotImplementedError()\n\n    ## Base Class Interface ####################################################\n    #\n    # These methods MAY be implemented by children, but contain default\n    # implementations that are appropriate for simple cases.\n    #\n    ##\n\n    def start_thread(self):\n        \"\"\"If the thread is not already alive start it\"\"\"\n        if not self.is_alive():\n            log.info(\"Starting %s: %s\", self.__class__.__name__, self.name)\n            self.start()\n\n    def stop_thread(self):\n        \"\"\"Set the shutdown event\"\"\"\n        log.info(\"Stopping %s: %s\", self.__class__.__name__, self.name)\n        self.shutdown.set()\n\n    def should_stop(self) -&gt; bool:\n        \"\"\"Helper to determine if a thread should shutdown\"\"\"\n        return self.shutdown.is_set()\n\n    def check_preconditions(self) -&gt; bool:\n        \"\"\"Helper function to check if the thread should shutdown or reacquire leadership\"\"\"\n        if self.should_stop():\n            return False\n\n        if self.leadership_manager and not self.leadership_manager.is_leader():\n            log.debug3(\"Waiting for leadership\")\n            self.leadership_manager.acquire()\n\n        return True\n\n    def wait_on_precondition(self, timeout: float) -&gt; bool:\n        \"\"\"Helper function to allow threads to wait for a certain period of time\n        only being interrupted for preconditions\"\"\"\n        self.shutdown.wait(timeout)\n\n        return self.check_preconditions()\n</code></pre> <code>__init__(name=None, daemon=None, deploy_manager=None, leadership_manager=None)</code> <p>Initialize class and store required instance variables. This function is normally overriden by subclasses that pass in static name/daemon variables</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>str=None The name of the thread to manager</p> <code>None</code> <code>daemon</code> <code>bool</code> <p>bool=None Whether python should wait for this thread to stop before exiting</p> <code>None</code> <code>deploy_manager</code> <code>DeployManagerBase</code> <p>DeployManagerBase = None The deploy manager available to this thread during start()</p> <code>None</code> <code>leadership_manager</code> <code>LeadershipManagerBase</code> <p>LeadershipManagerBase = None The leadership_manager for tracking elections</p> <code>None</code> Source code in <code>oper8/watch_manager/python_watch_manager/threads/base.py</code> <pre><code>def __init__(\n    self,\n    name: str = None,\n    daemon: bool = None,\n    deploy_manager: DeployManagerBase = None,\n    leadership_manager: LeadershipManagerBase = None,\n):\n    \"\"\"Initialize class and store required instance variables. This function\n    is normally overriden by subclasses that pass in static name/daemon variables\n\n    Args:\n        name:str=None\n            The name of the thread to manager\n        daemon:bool=None\n            Whether python should wait for this thread to stop before exiting\n        deploy_manager: DeployManagerBase = None\n            The deploy manager available to this thread during start()\n        leadership_manager: LeadershipManagerBase = None\n            The leadership_manager for tracking elections\n    \"\"\"\n    self.deploy_manager = deploy_manager\n    self.leadership_manager = leadership_manager or DryRunLeadershipManager()\n    self.shutdown = threading.Event()\n    super().__init__(name=name, daemon=daemon)\n</code></pre> <code>check_preconditions()</code> <p>Helper function to check if the thread should shutdown or reacquire leadership</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/base.py</code> <pre><code>def check_preconditions(self) -&gt; bool:\n    \"\"\"Helper function to check if the thread should shutdown or reacquire leadership\"\"\"\n    if self.should_stop():\n        return False\n\n    if self.leadership_manager and not self.leadership_manager.is_leader():\n        log.debug3(\"Waiting for leadership\")\n        self.leadership_manager.acquire()\n\n    return True\n</code></pre> <code>run()</code> <p>Control loop for the thread. Once this function exits the thread stops</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/base.py</code> <pre><code>def run(self):\n    \"\"\"Control loop for the thread. Once this function exits the thread stops\"\"\"\n    raise NotImplementedError()\n</code></pre> <code>should_stop()</code> <p>Helper to determine if a thread should shutdown</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/base.py</code> <pre><code>def should_stop(self) -&gt; bool:\n    \"\"\"Helper to determine if a thread should shutdown\"\"\"\n    return self.shutdown.is_set()\n</code></pre> <code>start_thread()</code> <p>If the thread is not already alive start it</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/base.py</code> <pre><code>def start_thread(self):\n    \"\"\"If the thread is not already alive start it\"\"\"\n    if not self.is_alive():\n        log.info(\"Starting %s: %s\", self.__class__.__name__, self.name)\n        self.start()\n</code></pre> <code>stop_thread()</code> <p>Set the shutdown event</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/base.py</code> <pre><code>def stop_thread(self):\n    \"\"\"Set the shutdown event\"\"\"\n    log.info(\"Stopping %s: %s\", self.__class__.__name__, self.name)\n    self.shutdown.set()\n</code></pre> <code>wait_on_precondition(timeout)</code> <p>Helper function to allow threads to wait for a certain period of time only being interrupted for preconditions</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/base.py</code> <pre><code>def wait_on_precondition(self, timeout: float) -&gt; bool:\n    \"\"\"Helper function to allow threads to wait for a certain period of time\n    only being interrupted for preconditions\"\"\"\n    self.shutdown.wait(timeout)\n\n    return self.check_preconditions()\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.threads.heartbeat","title":"<code>heartbeat</code>","text":"<p>Thread class that will dump a heartbeat to a file periodically</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.threads.heartbeat.HeartbeatThread","title":"<code>HeartbeatThread</code>","text":"<p>               Bases: <code>TimerThread</code></p> <p>The HeartbeatThread acts as a pulse for the PythonWatchManager.</p> <p>This thread will periodically dump the value of \"now\" to a file which can be read by an observer such as a liveness/readiness probe to ensure that the manager is functioning well.</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/heartbeat.py</code> <pre><code>class HeartbeatThread(TimerThread):\n    \"\"\"The HeartbeatThread acts as a pulse for the PythonWatchManager.\n\n    This thread will periodically dump the value of \"now\" to a file which can be\n    read by an observer such as a liveness/readiness probe to ensure that the\n    manager is functioning well.\n    \"\"\"\n\n    # This format is designed to be read using `date -d $(cat heartbeat.txt)`\n    # using the GNU date utility\n    # CITE: https://www.gnu.org/software/coreutils/manual/html_node/Examples-of-date.html\n    _DATE_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n\n    def __init__(self, heartbeat_file: str, heartbeat_period: str):\n        \"\"\"Initialize with the file location for the heartbeat output\n\n        Args:\n            heartbeat_file: str\n                The fully-qualified path to the heartbeat file\n            heartbeat_period: str\n                Time delta string representing period delay between beats.\n                NOTE: The GNU `date` utility cannot parse sub-seconds easily, so\n                    the expected configuration for this is to be &gt;= 1s\n        \"\"\"\n        self._heartbeat_file = heartbeat_file\n        self._offset = parse_time_delta(heartbeat_period)\n        self._beat_lock = threading.Lock()\n        self._beat_event = threading.Event()\n        super().__init__(name=\"heartbeat_thread\")\n\n    def run(self):\n        self._run_heartbeat()\n        return super().run()\n\n    def wait_for_beat(self):\n        \"\"\"Wait for the next beat\"\"\"\n        # Make sure the beat lock is not held before starting wait. This\n        # prevents beats that are immediately ready\n        with self._beat_lock:\n            pass\n\n        # Wait for the next beat\n        self._beat_event.wait()\n\n    def _run_heartbeat(self):\n        \"\"\"Run the heartbeat dump to the heartbeat file and put the next beat\"\"\"\n        now = datetime.now()\n        log.debug3(\"Heartbeat %s\", now)\n\n        # Save the beat to disk\n        try:\n            with open(self._heartbeat_file, \"w\", encoding=\"utf-8\") as handle:\n                handle.write(now.strftime(self._DATE_FORMAT))\n                handle.flush()\n        except Exception as err:\n            log.warning(\"Failed to write heartbeat file: %s\", err, exc_info=True)\n\n        # Unblock and reset the wait condition\n        with self._beat_lock:\n            self._beat_event.set()\n            self._beat_event.clear()\n\n        # Put the next beat if not stopped\n        if not self.should_stop():\n            self.put_event(now + self._offset, self._run_heartbeat)\n</code></pre> <code>__init__(heartbeat_file, heartbeat_period)</code> <p>Initialize with the file location for the heartbeat output</p> <p>Parameters:</p> Name Type Description Default <code>heartbeat_file</code> <code>str</code> <p>str The fully-qualified path to the heartbeat file</p> required <code>heartbeat_period</code> <code>str</code> <p>str Time delta string representing period delay between beats. NOTE: The GNU <code>date</code> utility cannot parse sub-seconds easily, so     the expected configuration for this is to be &gt;= 1s</p> required Source code in <code>oper8/watch_manager/python_watch_manager/threads/heartbeat.py</code> <pre><code>def __init__(self, heartbeat_file: str, heartbeat_period: str):\n    \"\"\"Initialize with the file location for the heartbeat output\n\n    Args:\n        heartbeat_file: str\n            The fully-qualified path to the heartbeat file\n        heartbeat_period: str\n            Time delta string representing period delay between beats.\n            NOTE: The GNU `date` utility cannot parse sub-seconds easily, so\n                the expected configuration for this is to be &gt;= 1s\n    \"\"\"\n    self._heartbeat_file = heartbeat_file\n    self._offset = parse_time_delta(heartbeat_period)\n    self._beat_lock = threading.Lock()\n    self._beat_event = threading.Event()\n    super().__init__(name=\"heartbeat_thread\")\n</code></pre> <code>wait_for_beat()</code> <p>Wait for the next beat</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/heartbeat.py</code> <pre><code>def wait_for_beat(self):\n    \"\"\"Wait for the next beat\"\"\"\n    # Make sure the beat lock is not held before starting wait. This\n    # prevents beats that are immediately ready\n    with self._beat_lock:\n        pass\n\n    # Wait for the next beat\n    self._beat_event.wait()\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.threads.reconcile","title":"<code>reconcile</code>","text":"<p>The ReconcileThread is the heart of the PythonWatchManager and controls reconciling resources and managing any subprocesses</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.threads.reconcile.ReconcileThread","title":"<code>ReconcileThread</code>","text":"<p>               Bases: <code>ThreadBase</code></p> <p>This class is the core reconciliation class that handles starting subprocesses, tracking their status, and handles any results. This thread also kicks of requeue requests and requests dependent resource watches</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/reconcile.py</code> <pre><code>class ReconcileThread(\n    ThreadBase, metaclass=Singleton\n):  # pylint: disable=too-many-instance-attributes\n    \"\"\"This class is the core reconciliation class that handles starting subprocesses,\n    tracking their status, and handles any results. This thread also kicks of requeue\n    requests and requests dependent resource watches\"\"\"\n\n    def __init__(\n        self,\n        deploy_manager: DeployManagerBase = None,\n        leadership_manager: LeadershipManagerBase = None,\n    ):\n        \"\"\"Initialize the required queues, helper threads, and reconcile tracking. Also\n        gather any onetime configuration options\n\n        Args:\n            deploy_manager: DeployManagerBase = None\n                The deploy manager used throughout the thread\n            leadership_manager: LeadershipManagerBase = None\n                The leadership_manager for tracking elections\n        \"\"\"\n        super().__init__(\n            name=\"reconcile_thread\",\n            deploy_manager=deploy_manager,\n            leadership_manager=leadership_manager,\n        )\n\n        # Configure the multiprocessing process spawning context\n        context = config.python_watch_manager.process_context\n        if context not in multiprocessing.get_all_start_methods():\n            raise ConfigError(f\"Invalid process_context: '{context}'\")\n\n        if context == \"fork\":\n            log.warning(\n                \"The fork multiprocessing context is known to cause deadlocks in certain\"\n                \" environments due to OpenSSL. Consider using spawn for more reliable\"\n                \" reconciling\"\n            )\n\n        self.spawn_ctx = multiprocessing.get_context(context)\n\n        # Setup required queues\n        self.request_queue = self.spawn_ctx.Queue()\n        self.logging_queue = self.spawn_ctx.Queue()\n\n        # Setup helper threads\n        self.timer_thread: TimerThread = TimerThread()\n        self.log_listener_thread: QueueListener = QueueListener(\n            self.logging_queue, *get_logging_handlers(), respect_handler_level=False\n        )\n\n        # Setup reconcile, request, and event mappings\n        self.running_reconciles: Dict[str, ReconcileProcess] = {}\n        self.pending_reconciles: Dict[str, ReconcileRequest] = {}\n        self.event_map: Dict[str, TimerEvent] = {}\n\n        # Setup control variables\n        self.process_overload = threading.Event()\n\n        # Configure the max number of concurrent reconciles via either config\n        # or number of cpus\n        if config.python_watch_manager.max_concurrent_reconciles:\n            self.max_concurrent_reconciles = (\n                config.python_watch_manager.max_concurrent_reconciles\n            )\n        else:\n            self.max_concurrent_reconciles = os.cpu_count()\n\n    def run(self):\n        \"\"\"The reconcile threads control flow is to first wait for\n        either a new reconcile request or for a process to end. If its a reconcile request\n        the thread checks if one is already running for the resource and if not starts a\n        new one. If a reconcile is already running or the thread couldn't start a new one\n        the request gets added to the pending reconciles. There can only be one pending\n        reconcile per resource. If the reconcile thread received a process end event it\n        checks the exit code and handles the result queue.\n        \"\"\"\n        while True:  # pylint: disable=too-many-nested-blocks\n            if not self.check_preconditions():\n                return\n\n            # Wait for a change with the reconcile processes or reconcile queue.\n            # Use _reader and Process.sentinel objects, so we can utilize select.select\n            listen_objs = [\n                *self.running_reconciles.values(),\n                self.request_queue._reader,  # pylint: disable=protected-access\n            ]\n            log.debug3(\"Waiting on %s\", listen_objs)\n            ready_objs = multiprocessing.connection.wait(listen_objs)\n\n            # Check preconditions both before and after waiting\n            if not self.check_preconditions():\n                return\n\n            # For each object that triggered the connection\n            for obj in ready_objs:\n                log.debug3(\"Processing object %s with type %s\", obj, type(obj))\n\n                # Handle reconcile process end events\n                if isinstance(obj, ReconcileProcess):\n                    if self._handle_process_end(obj):\n                        # If process overload is set than we need to check all resources for\n                        # pending reconciles otherwise just check if the completed resource\n                        # has a pending request.\n                        if self.process_overload.is_set():\n                            for uid in list(self.pending_reconciles.keys()):\n                                if not self._handle_pending_reconcile(uid):\n                                    break\n                        else:\n                            self._handle_pending_reconcile(obj.uid())\n\n                # Handle all of the events in the queue\n                elif isinstance(obj, Connection):\n                    self._handle_request_queue()\n\n    ## Class Interface ###################################################\n\n    def start_thread(self):\n        \"\"\"Override start_thread to start helper threads\"\"\"\n        self.timer_thread.start_thread()\n        self.log_listener_thread.start()\n        super().start_thread()\n\n    def stop_thread(self):\n        \"\"\"Override stop_thread to ensure reconciles finish correctly\"\"\"\n        super().stop_thread()\n\n        if not self.is_alive() and not self.running_reconciles:\n            log.debug(\"Reconcile Thread already stopped\")\n            return\n\n        # Reawaken reconcile thread to stop\n        log.debug(\"Pushing stop reconcile request\")\n        self.push_request(ReconcileRequest(None, ReconcileRequestType.STOPPED, {}))\n\n        log.debug(\"Waiting for reconcile thread to finish\")\n        while self.is_alive():\n            time.sleep(0.001)\n\n        # Wait until all reconciles have completed\n        log.info(\"Waiting for Running Reconciles to end\")\n        while self.running_reconciles:\n            log.debug2(\"Waiting for reconciles %s to end\", self.running_reconciles)\n            for reconcile_process in list(self.running_reconciles.values()):\n                # attempt to join process before trying the next one\n                reconcile_process.process.join(JOIN_PROCESS_TIMEOUT)\n                if reconcile_process.process.exitcode is not None:\n                    log.debug(\n                        \"Joined reconciles process %s with exitcode: %s for request object %s\",\n                        reconcile_process.process.pid,\n                        reconcile_process.process.exitcode,\n                        reconcile_process.request,\n                    )\n                    self.running_reconciles.pop(reconcile_process.uid())\n                    reconcile_process.process.close()\n\n            # Pause for slight delay between checking processes\n            sleep(SHUTDOWN_RECONCILE_POLL_TIME)\n\n        # Close the logging queue to indicate no more logging events\n        self.logging_queue.close()\n        # Skip stopping the listener thread as it can hang on the join, this isn't\n        # too bad as the listener thread is daemon anyways\n        # self.log_listener_thread.stop()\n\n    ## Public Interface ###################################################\n\n    def push_request(self, request: ReconcileRequest):\n        \"\"\"Push request to reconcile queue\n\n        Args:\n            request: ReconcileRequest\n                the ReconcileRequest to add to the queue\n        \"\"\"\n        log.info(\n            \"Pushing request '%s' to reconcile queue\",\n            request,\n            extra={\"resource\": request.resource},\n        )\n        self.request_queue.put(request)\n\n    ## Event Handlers ###################################################\n\n    def _handle_request_queue(self):\n        \"\"\"The function attempts to start a reconcile for every reconcile requests in the queue.\n        If it can't start a reconcile or one is already running then it pushes it to the pending\n        queue\"\"\"\n\n        # Get all events from the queue\n        pending_requests = self._get_all_requests()\n\n        # Start a reconcile for each pending request\n        for request in pending_requests:\n            if request.type == ReconcileRequestType.STOPPED:\n                break\n\n            log.debug3(\"Got request %s from queue\", request)\n\n            # If reconcile is not running then start the process. Otherwise\n            # or if starting failed push to the pending reconcile queue\n            if request.resource.uid not in self.running_reconciles:\n                if not self._start_reconcile_for_request(request):\n                    self._push_to_pending_reconcile(request)\n            else:\n                self._push_to_pending_reconcile(request)\n\n    def _handle_process_end(self, reconcile_process: ReconcileProcess) -&gt; str:\n        \"\"\"Handle a process end event. The function joins the finished process,\n        manages any events in the pipe, and creates a requeue/periodic event if\n        one is needed.\n\n        Args:\n            reconcile_process: ReconcileProcess\n                The process that ended\n\n        Returns:\n            uid: str\n                The uid of the resource that ended\n        \"\"\"\n        # Parse process variables\n        uid = reconcile_process.uid()\n        reconcile_request = reconcile_process.request\n        process = reconcile_process.process\n        pipe = reconcile_process.pipe\n\n        # Attempt to join the process\n        log.debug(\n            \"Joining process for request %s\",\n            reconcile_request,\n            extra={\"resource\": reconcile_request.resource},\n        )\n        process.join(JOIN_PROCESS_TIMEOUT)\n        exit_code = process.exitcode\n\n        # If its still then exit and process   will be cleaned up on next iteration\n        if exit_code is None:\n            log.debug(\"Process is still alive after join. Skipping cleanup\")\n            return None\n\n        if exit_code != 0:\n            log.warning(\n                \"Reconcile did not complete successfully: %s\",\n                reconcile_request,\n                extra={\"resource\": reconcile_request.resource},\n            )\n\n        # Remove reconcile from map and release resource lock\n        self.running_reconciles.pop(uid)\n        self.leadership_manager.release_resource(reconcile_request.resource)\n\n        # Handle any events passed via the process pipe including the reconcile result and\n        # close the pipe once done\n        reconcile_result = self._handle_process_pipe(pipe)\n        process.close()\n\n        # Print reconciliation result\n        log.info(\n            \"Reconcile completed with result %s\",\n            reconcile_result if reconcile_result else exit_code,\n            extra={\"resource\": reconcile_request.resource},\n        )\n\n        # Cancel any existing requeue events\n        if uid in self.event_map:\n            log.debug2(\"Marking event as stale: %s\", self.event_map[uid])\n            self.event_map[uid].cancel()\n\n        # Create a new timer event if one is needed\n        event = self._create_timer_event_for_request(\n            reconcile_request, reconcile_result\n        )\n        if event:\n            self.event_map[uid] = event\n\n        return uid\n\n    def _handle_process_pipe(self, pipe: Connection) -&gt; ReconciliationResult:\n        \"\"\"Handle any objects in a connection pipe and return the reconciliation result\n\n        Args:\n            pipe: Connection\n                the pipe to read results from\n\n        Returns:\n            reconcile_result: ReconciliationResult\n                The result gathered from the pipe\n        \"\"\"\n        reconcile_result = None\n        while pipe.poll():\n            # EOFError is raised when the pipe is closed which is expected after the reconcile\n            # process has been joined\n            try:\n                pipe_obj = pipe.recv()\n            except EOFError:\n                break\n\n            log.debug3(\"Received obj %s from process pipe\", pipe_obj)\n\n            # Handle any watch requests received\n            if isinstance(pipe_obj, WatchRequest):\n                self._handle_watch_request(pipe_obj)\n\n            # We only expect one reconciliation result per process\n            elif isinstance(pipe_obj, ReconciliationResult):\n                reconcile_result = pipe_obj\n\n        # Close the reconcile pipe and release the rest of the process resources\n        pipe.close()\n\n        return reconcile_result\n\n    def _handle_watch_request(self, request: WatchRequest):\n        \"\"\"Create a resource watch for a given watch request. This function also\n        handles converting controller_info into a valid controller_type\n\n        Args:\n            request: WatchRequest\n                The requested WatchRequest to be created\n        \"\"\"\n        # Parse the controller info into a type\n        if request.controller_info and not request.controller_type:\n            request.controller_type = request.controller_info.to_class()\n\n        # Parse any filter infos into types\n        if request.filters_info:\n            request.filters = FilterManager.from_info(request.filters_info)\n\n        create_resource_watch(\n            request,\n            self,\n            self.deploy_manager,\n            self.leadership_manager,\n        )\n\n    def _create_timer_event_for_request(\n        self, request: ReconcileRequest, result: ReconciliationResult = None\n    ) -&gt; Optional[TimerEvent]:\n        \"\"\"Enqueue either a requeue or periodic reconcile request for a given\n        result.\n\n        Args:\n            request: ReconcileRequest\n                The original reconcile request that triggered this process\n            result: ReconciliationResult = None\n                The result of the reconcile\n\n        Returns:\n            timer_event: Optional[TimerEvent]\n                The timer event if one was created\n        \"\"\"\n\n        # Short circuit if event is not needed, if resource was deleted,\n        # or if theres already a pending reconcile\n        if (\n            not result or not result.requeue\n        ) and not config.python_watch_manager.reconcile_period:\n            return None\n\n        if result and not result.requeue and request.type == KubeEventType.DELETED:\n            return None\n\n        if request.resource.uid in self.pending_reconciles:\n            return None\n\n        # Create requeue_time and type based on result/config\n        request_type = None\n        requeue_time = None\n        if result and result.requeue:\n            requeue_time = datetime.now() + result.requeue_params.requeue_after\n            request_type = ReconcileRequestType.REQUEUED\n        elif config.python_watch_manager.reconcile_period:\n            requeue_time = datetime.now() + parse_time_delta(\n                config.python_watch_manager.reconcile_period\n            )\n            request_type = ReconcileRequestType.PERIODIC\n\n        future_request = ReconcileRequest(\n            request.controller_type, request_type, request.resource\n        )\n        log.debug3(\"Pushing requeue request to timer: %s\", future_request)\n\n        return self.timer_thread.put_event(\n            requeue_time, self._push_updated_request, future_request\n        )\n\n    def _push_updated_request(self, request: ReconcileRequest):\n        \"\"\"_push_updated_request is a helper function to ensure that when submitting a requeue\n        event we always fetch the latest resource.\n\n        Args:\n            request (ReconcileRequest): The request to push\n        \"\"\"\n\n        success, current_resource = self.deploy_manager.get_object_current_state(\n            kind=request.resource.kind,\n            api_version=request.resource.api_version,\n            name=request.resource.name,\n            namespace=request.resource.namespace,\n        )\n        if not success:\n            log.debug2(\"Unable to locate resource for requeue request: %s\", request)\n            return\n\n        request.resource = ManagedObject(current_resource)\n        self.push_request(request)\n\n    ## Pending Event Helpers ###################################################\n\n    def _handle_pending_reconcile(self, uid: str) -&gt; bool:\n        \"\"\"Start reconcile for pending request if there is one\n\n        Args:\n             uid:str\n                The uid of the resource being reconciled\n\n        Returns:\n            successful_start:bool\n                If there was a pending reconcile that got started\"\"\"\n        # Check if resource has pending request\n        if uid in self.running_reconciles or uid not in self.pending_reconciles:\n            return False\n\n        # Start reconcile for request\n        request = self.pending_reconciles[uid]\n        log.debug4(\"Got request %s from pending reconciles\", request)\n        if self._start_reconcile_for_request(request):\n            self.pending_reconciles.pop(uid)\n            return True\n        return False\n\n    def _push_to_pending_reconcile(self, request: ReconcileRequest):\n        \"\"\"Push a request to the pending queue if it's newer than the current event\n\n        Args:\n            request:  ReconcileRequest\n                The request to possibly add to the pending_reconciles\n        \"\"\"\n        uid = request.uid()\n        # Only update queue if request is newer\n        if uid in self.pending_reconciles:\n            if request.timestamp &gt; self.pending_reconciles[uid].timestamp:\n                log.debug3(\"Updating reconcile queue with event %s\", request)\n                self.pending_reconciles[uid] = request\n            else:\n                log.debug4(\"Event in queue is newer than event %s\", request)\n        else:\n            log.debug3(\"Adding event %s to reconcile queue\", request)\n            self.pending_reconciles[uid] = request\n\n    ## Process functions ##################################################\n\n    def _start_reconcile_for_request(self, request: ReconcileRequest) -&gt; bool:\n        \"\"\"Start a reconciliation process for a given request\n\n        Args:\n            request: ReconcileRequest\n                The request to attempt to start\n\n        Returns:\n            successfully_started: bool\n                If a process could be started\n        \"\"\"\n        # If thread is supposed to shutdown then don't start process\n        if self.should_stop():\n            return False\n\n        # Check if there are too many reconciles running\n        if len(self.running_reconciles.keys()) &gt;= self.max_concurrent_reconciles:\n            log.warning(\"Unable to start reconcile, max concurrent jobs reached\")\n            self.process_overload.set()\n            return False\n\n        # Attempt to acquire lock on resource. If failed skip starting\n        if not self.leadership_manager.acquire_resource(request.resource):\n            log.debug(\"Unable to obtain leadership lock for %s\", request)\n            return False\n\n        self.process_overload.clear()\n        log.info(\n            \"Starting reconcile for request %s\",\n            request,\n            extra={\"resource\": request.resource},\n        )\n\n        # Create the send and return pipe\n        recv_pipe, send_pipe = self.spawn_ctx.Pipe()\n\n        process = self._start_reconcile_process(request, send_pipe)\n\n        # Generate the reconcile process and update map\n        reconcile_process = ReconcileProcess(\n            process=process, request=request, pipe=recv_pipe\n        )\n\n        self.running_reconciles[request.uid()] = reconcile_process\n        return True\n\n    def _start_reconcile_process(\n        self, request: ReconcileRequest, pipe: Connection\n    ) -&gt; multiprocessing.Process:\n        \"\"\"Helper function to generate and start the reconcile process. This\n        was largely created to ease the testing and mocking process\n\n         Args:\n             request: ReconcileRequest\n                The request to start the process with\n             pipe: Connection\n                The result pipe for this reconcile\n\n        Returns:\n            process: multiprocessing.Process\n                The started process\n        \"\"\"\n\n        # Create and start the reconcile process\n        process = self.spawn_ctx.Process(\n            target=create_and_start_entrypoint,\n            args=[self.logging_queue, request, pipe],\n        )\n        process.start()\n        log.debug3(\"Started child process with pid: %s\", process.pid)\n        return process\n\n    ## Queue Functions ##################################################\n\n    def _get_all_requests(\n        self, timeout: Optional[int] = None\n    ) -&gt; List[ReconcileRequest]:\n        \"\"\"Get all of the requests from the reconcile queue\n\n        Args:\n            timeout:Optional[int]=None\n                The timeout to wait for an event. If None it returns immediately\n\n        Returns:\n            requests: List[ReconcileRequest]\n                The list of requests gathered from the queue\n        \"\"\"\n        request_list = []\n        while not self.request_queue.empty():\n            try:\n                request = self.request_queue.get(block=False, timeout=timeout)\n\n                # If there is a stop request then immediately return it\n                if request.type == ReconcileRequestType.STOPPED:\n                    return [request]\n                request_list.append(request)\n            except queue.Empty:\n                break\n        return request_list\n</code></pre> <code>__init__(deploy_manager=None, leadership_manager=None)</code> <p>Initialize the required queues, helper threads, and reconcile tracking. Also gather any onetime configuration options</p> <p>Parameters:</p> Name Type Description Default <code>deploy_manager</code> <code>DeployManagerBase</code> <p>DeployManagerBase = None The deploy manager used throughout the thread</p> <code>None</code> <code>leadership_manager</code> <code>LeadershipManagerBase</code> <p>LeadershipManagerBase = None The leadership_manager for tracking elections</p> <code>None</code> Source code in <code>oper8/watch_manager/python_watch_manager/threads/reconcile.py</code> <pre><code>def __init__(\n    self,\n    deploy_manager: DeployManagerBase = None,\n    leadership_manager: LeadershipManagerBase = None,\n):\n    \"\"\"Initialize the required queues, helper threads, and reconcile tracking. Also\n    gather any onetime configuration options\n\n    Args:\n        deploy_manager: DeployManagerBase = None\n            The deploy manager used throughout the thread\n        leadership_manager: LeadershipManagerBase = None\n            The leadership_manager for tracking elections\n    \"\"\"\n    super().__init__(\n        name=\"reconcile_thread\",\n        deploy_manager=deploy_manager,\n        leadership_manager=leadership_manager,\n    )\n\n    # Configure the multiprocessing process spawning context\n    context = config.python_watch_manager.process_context\n    if context not in multiprocessing.get_all_start_methods():\n        raise ConfigError(f\"Invalid process_context: '{context}'\")\n\n    if context == \"fork\":\n        log.warning(\n            \"The fork multiprocessing context is known to cause deadlocks in certain\"\n            \" environments due to OpenSSL. Consider using spawn for more reliable\"\n            \" reconciling\"\n        )\n\n    self.spawn_ctx = multiprocessing.get_context(context)\n\n    # Setup required queues\n    self.request_queue = self.spawn_ctx.Queue()\n    self.logging_queue = self.spawn_ctx.Queue()\n\n    # Setup helper threads\n    self.timer_thread: TimerThread = TimerThread()\n    self.log_listener_thread: QueueListener = QueueListener(\n        self.logging_queue, *get_logging_handlers(), respect_handler_level=False\n    )\n\n    # Setup reconcile, request, and event mappings\n    self.running_reconciles: Dict[str, ReconcileProcess] = {}\n    self.pending_reconciles: Dict[str, ReconcileRequest] = {}\n    self.event_map: Dict[str, TimerEvent] = {}\n\n    # Setup control variables\n    self.process_overload = threading.Event()\n\n    # Configure the max number of concurrent reconciles via either config\n    # or number of cpus\n    if config.python_watch_manager.max_concurrent_reconciles:\n        self.max_concurrent_reconciles = (\n            config.python_watch_manager.max_concurrent_reconciles\n        )\n    else:\n        self.max_concurrent_reconciles = os.cpu_count()\n</code></pre> <code>push_request(request)</code> <p>Push request to reconcile queue</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ReconcileRequest</code> <p>ReconcileRequest the ReconcileRequest to add to the queue</p> required Source code in <code>oper8/watch_manager/python_watch_manager/threads/reconcile.py</code> <pre><code>def push_request(self, request: ReconcileRequest):\n    \"\"\"Push request to reconcile queue\n\n    Args:\n        request: ReconcileRequest\n            the ReconcileRequest to add to the queue\n    \"\"\"\n    log.info(\n        \"Pushing request '%s' to reconcile queue\",\n        request,\n        extra={\"resource\": request.resource},\n    )\n    self.request_queue.put(request)\n</code></pre> <code>run()</code> <p>The reconcile threads control flow is to first wait for either a new reconcile request or for a process to end. If its a reconcile request the thread checks if one is already running for the resource and if not starts a new one. If a reconcile is already running or the thread couldn't start a new one the request gets added to the pending reconciles. There can only be one pending reconcile per resource. If the reconcile thread received a process end event it checks the exit code and handles the result queue.</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/reconcile.py</code> <pre><code>def run(self):\n    \"\"\"The reconcile threads control flow is to first wait for\n    either a new reconcile request or for a process to end. If its a reconcile request\n    the thread checks if one is already running for the resource and if not starts a\n    new one. If a reconcile is already running or the thread couldn't start a new one\n    the request gets added to the pending reconciles. There can only be one pending\n    reconcile per resource. If the reconcile thread received a process end event it\n    checks the exit code and handles the result queue.\n    \"\"\"\n    while True:  # pylint: disable=too-many-nested-blocks\n        if not self.check_preconditions():\n            return\n\n        # Wait for a change with the reconcile processes or reconcile queue.\n        # Use _reader and Process.sentinel objects, so we can utilize select.select\n        listen_objs = [\n            *self.running_reconciles.values(),\n            self.request_queue._reader,  # pylint: disable=protected-access\n        ]\n        log.debug3(\"Waiting on %s\", listen_objs)\n        ready_objs = multiprocessing.connection.wait(listen_objs)\n\n        # Check preconditions both before and after waiting\n        if not self.check_preconditions():\n            return\n\n        # For each object that triggered the connection\n        for obj in ready_objs:\n            log.debug3(\"Processing object %s with type %s\", obj, type(obj))\n\n            # Handle reconcile process end events\n            if isinstance(obj, ReconcileProcess):\n                if self._handle_process_end(obj):\n                    # If process overload is set than we need to check all resources for\n                    # pending reconciles otherwise just check if the completed resource\n                    # has a pending request.\n                    if self.process_overload.is_set():\n                        for uid in list(self.pending_reconciles.keys()):\n                            if not self._handle_pending_reconcile(uid):\n                                break\n                    else:\n                        self._handle_pending_reconcile(obj.uid())\n\n            # Handle all of the events in the queue\n            elif isinstance(obj, Connection):\n                self._handle_request_queue()\n</code></pre> <code>start_thread()</code> <p>Override start_thread to start helper threads</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/reconcile.py</code> <pre><code>def start_thread(self):\n    \"\"\"Override start_thread to start helper threads\"\"\"\n    self.timer_thread.start_thread()\n    self.log_listener_thread.start()\n    super().start_thread()\n</code></pre> <code>stop_thread()</code> <p>Override stop_thread to ensure reconciles finish correctly</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/reconcile.py</code> <pre><code>def stop_thread(self):\n    \"\"\"Override stop_thread to ensure reconciles finish correctly\"\"\"\n    super().stop_thread()\n\n    if not self.is_alive() and not self.running_reconciles:\n        log.debug(\"Reconcile Thread already stopped\")\n        return\n\n    # Reawaken reconcile thread to stop\n    log.debug(\"Pushing stop reconcile request\")\n    self.push_request(ReconcileRequest(None, ReconcileRequestType.STOPPED, {}))\n\n    log.debug(\"Waiting for reconcile thread to finish\")\n    while self.is_alive():\n        time.sleep(0.001)\n\n    # Wait until all reconciles have completed\n    log.info(\"Waiting for Running Reconciles to end\")\n    while self.running_reconciles:\n        log.debug2(\"Waiting for reconciles %s to end\", self.running_reconciles)\n        for reconcile_process in list(self.running_reconciles.values()):\n            # attempt to join process before trying the next one\n            reconcile_process.process.join(JOIN_PROCESS_TIMEOUT)\n            if reconcile_process.process.exitcode is not None:\n                log.debug(\n                    \"Joined reconciles process %s with exitcode: %s for request object %s\",\n                    reconcile_process.process.pid,\n                    reconcile_process.process.exitcode,\n                    reconcile_process.request,\n                )\n                self.running_reconciles.pop(reconcile_process.uid())\n                reconcile_process.process.close()\n\n        # Pause for slight delay between checking processes\n        sleep(SHUTDOWN_RECONCILE_POLL_TIME)\n\n    # Close the logging queue to indicate no more logging events\n    self.logging_queue.close()\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.threads.timer","title":"<code>timer</code>","text":"<p>The TimerThread is a helper class used to run schedule events</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.threads.timer.TimerThread","title":"<code>TimerThread</code>","text":"<p>               Bases: <code>ThreadBase</code></p> <p>The TimerThread class is a helper class to run scheduled actions. This is very similar to threading.Timer stdlib class except that it uses one shared thread for all events instead of a thread per event.</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/timer.py</code> <pre><code>class TimerThread(ThreadBase, metaclass=Singleton):\n    \"\"\"The TimerThread class is a helper class to run scheduled actions. This is very similar\n    to threading.Timer stdlib class except that it uses one shared thread for all events\n    instead of a thread per event.\"\"\"\n\n    def __init__(self, name: Optional[str] = None):\n        \"\"\"Initialize a priorityqueue like object and a synchronization object\"\"\"\n        super().__init__(name=name or \"timer_thread\", daemon=True)\n\n        # Use a heap queue instead of a queue.PriorityQueue as we're already handling\n        # synchronization with the notify condition\n        # https://docs.python.org/3/library/heapq.html?highlight=heap#priority-queue-implementation-notes\n        self.timer_heap = []\n        self.notify_condition = threading.Condition()\n\n    def run(self):\n        \"\"\"The TimerThread's control loop sleeps until the next schedule\n        event and executes all pending actions.\"\"\"\n        if not self.check_preconditions():\n            return\n\n        while True:\n            # Wait until the next event or a new event is pushed\n            with self.notify_condition:\n                time_to_sleep = self._get_time_to_sleep()\n                if time_to_sleep:\n                    log.debug2(\n                        \"Timer waiting %ss until next scheduled event\", time_to_sleep\n                    )\n                else:\n                    log.debug2(\"Timer waiting until event queued\")\n                self.notify_condition.wait(timeout=time_to_sleep)\n\n            if not self.check_preconditions():\n                return\n\n            # Get all the events to be executed\n            event_list = self._get_all_current_events()\n            for event in event_list:\n                log.debug(\"Timer executing action for event: %s\", event)\n                event.action(*event.args, **event.kwargs)\n\n    ## Class Interface ###################################################\n\n    def stop_thread(self):\n        \"\"\"Override stop_thread to wake the control loop\"\"\"\n        super().stop_thread()\n        # Notify timer thread of shutdown\n        log.debug2(\"Acquiring notify condition for shutdown\")\n        with self.notify_condition:\n            log.debug(\"Notifying TimerThread of shutdown\")\n            self.notify_condition.notify_all()\n\n    ## Public Interface ###################################################\n\n    def put_event(\n        self, time: datetime, action: Callable, *args: Any, **kwargs: Dict\n    ) -&gt; Optional[TimerEvent]:\n        \"\"\"Push an event to the timer\n\n        Args:\n            time: datetime\n                The datetime to execute the event at\n            action: Callable\n                The action to execute\n            *args: Any\n                Args to pass to the action\n            **kwargs: Dict\n                Kwargs to pass to the action\n\n        Returns:\n            event: Optional[TimerEvent]\n                TimerEvent describing the event and can be cancelled\n        \"\"\"\n        # Don't allow pushing to a stopped thread\n        if self.should_stop():\n            return None\n\n        # Create a timer event and push it to the heap\n        event = TimerEvent(time=time, action=action, args=args, kwargs=kwargs)\n        with self.notify_condition:\n            heappush(self.timer_heap, event)\n            self.notify_condition.notify_all()\n        return event\n\n    ## Time Functions  ###################################################\n\n    def _get_time_to_sleep(self) -&gt; Optional[int]:\n        \"\"\"Calculate the time to sleep based on the current queue\n\n        Returns:\n            time_to_wait: Optional[int]\n               The time to wait if there's an object in the queue\"\"\"\n        with self.notify_condition:\n            obj = self._peak_next_event()\n            if obj:\n                time_to_sleep = (obj.time - datetime.now()).total_seconds()\n                if time_to_sleep &lt; MIN_SLEEP_TIME:\n                    return MIN_SLEEP_TIME\n                return time_to_sleep\n\n            return None\n\n    ## Queue Functions  ###################################################\n\n    def _get_all_current_events(self) -&gt; List[TimerEvent]:\n        \"\"\"Get all the current events that should execute\n\n        Returns:\n            current_events: List[TimerEvent]\n                List of timer events to execute\n        \"\"\"\n        event_list = []\n        # With lock preview the next object\n        with self.notify_condition:\n            while len(self.timer_heap) != 0:\n                obj_preview = self._peak_next_event()\n                # If object exists and should've already executed then remove object from queue\n                # and add it to return list\n                if obj_preview and obj_preview.time &lt; datetime.now():\n                    try:\n                        obj = heappop(self.timer_heap)\n                        if obj.stale:\n                            log.debug2(\"Skipping timer event %s\", obj)\n                            continue\n                        event_list.append(obj)\n                    except queue.Empty:\n                        break\n                else:\n                    break\n        return event_list\n\n    def _peak_next_event(self) -&gt; Optional[TimerEvent]:\n        \"\"\"Get the next timer event without removing it from the queue\n\n        Returns:\n            next_event: TimerEvent\n                The next timer event if one exists\n        \"\"\"\n        with self.notify_condition:\n            if self.timer_heap:\n                return self.timer_heap[0]\n            return None\n</code></pre> <code>__init__(name=None)</code> <p>Initialize a priorityqueue like object and a synchronization object</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/timer.py</code> <pre><code>def __init__(self, name: Optional[str] = None):\n    \"\"\"Initialize a priorityqueue like object and a synchronization object\"\"\"\n    super().__init__(name=name or \"timer_thread\", daemon=True)\n\n    # Use a heap queue instead of a queue.PriorityQueue as we're already handling\n    # synchronization with the notify condition\n    # https://docs.python.org/3/library/heapq.html?highlight=heap#priority-queue-implementation-notes\n    self.timer_heap = []\n    self.notify_condition = threading.Condition()\n</code></pre> <code>put_event(time, action, *args, **kwargs)</code> <p>Push an event to the timer</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>datetime</code> <p>datetime The datetime to execute the event at</p> required <code>action</code> <code>Callable</code> <p>Callable The action to execute</p> required <code>*args</code> <code>Any</code> <p>Any Args to pass to the action</p> <code>()</code> <code>**kwargs</code> <code>Dict</code> <p>Dict Kwargs to pass to the action</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>event</code> <code>Optional[TimerEvent]</code> <p>Optional[TimerEvent] TimerEvent describing the event and can be cancelled</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/timer.py</code> <pre><code>def put_event(\n    self, time: datetime, action: Callable, *args: Any, **kwargs: Dict\n) -&gt; Optional[TimerEvent]:\n    \"\"\"Push an event to the timer\n\n    Args:\n        time: datetime\n            The datetime to execute the event at\n        action: Callable\n            The action to execute\n        *args: Any\n            Args to pass to the action\n        **kwargs: Dict\n            Kwargs to pass to the action\n\n    Returns:\n        event: Optional[TimerEvent]\n            TimerEvent describing the event and can be cancelled\n    \"\"\"\n    # Don't allow pushing to a stopped thread\n    if self.should_stop():\n        return None\n\n    # Create a timer event and push it to the heap\n    event = TimerEvent(time=time, action=action, args=args, kwargs=kwargs)\n    with self.notify_condition:\n        heappush(self.timer_heap, event)\n        self.notify_condition.notify_all()\n    return event\n</code></pre> <code>run()</code> <p>The TimerThread's control loop sleeps until the next schedule event and executes all pending actions.</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/timer.py</code> <pre><code>def run(self):\n    \"\"\"The TimerThread's control loop sleeps until the next schedule\n    event and executes all pending actions.\"\"\"\n    if not self.check_preconditions():\n        return\n\n    while True:\n        # Wait until the next event or a new event is pushed\n        with self.notify_condition:\n            time_to_sleep = self._get_time_to_sleep()\n            if time_to_sleep:\n                log.debug2(\n                    \"Timer waiting %ss until next scheduled event\", time_to_sleep\n                )\n            else:\n                log.debug2(\"Timer waiting until event queued\")\n            self.notify_condition.wait(timeout=time_to_sleep)\n\n        if not self.check_preconditions():\n            return\n\n        # Get all the events to be executed\n        event_list = self._get_all_current_events()\n        for event in event_list:\n            log.debug(\"Timer executing action for event: %s\", event)\n            event.action(*event.args, **event.kwargs)\n</code></pre> <code>stop_thread()</code> <p>Override stop_thread to wake the control loop</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/timer.py</code> <pre><code>def stop_thread(self):\n    \"\"\"Override stop_thread to wake the control loop\"\"\"\n    super().stop_thread()\n    # Notify timer thread of shutdown\n    log.debug2(\"Acquiring notify condition for shutdown\")\n    with self.notify_condition:\n        log.debug(\"Notifying TimerThread of shutdown\")\n        self.notify_condition.notify_all()\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.threads.watch","title":"<code>watch</code>","text":"<p>The WatchThread Class is responsible for monitoring the cluster for resource events</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.threads.watch.WatchThread","title":"<code>WatchThread</code>","text":"<p>               Bases: <code>ThreadBase</code></p> <p>The WatchThread monitors the cluster for changes to a specific GroupVersionKind either cluster-wide or for a particular namespace. When it detects a change it checks the event against the registered Filters and submits a ReconcileRequest if it passes. Every resource that has at least one watch request gets a corresponding WatchedResource object whose main job is to store the current Filter status</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/watch.py</code> <pre><code>class WatchThread(ThreadBase):  # pylint: disable=too-many-instance-attributes\n    \"\"\"The WatchThread monitors the cluster for changes to a specific GroupVersionKind either\n    cluster-wide or for a particular namespace. When it detects a change it checks the event\n    against the registered Filters and submits a ReconcileRequest if it passes. Every resource\n    that has at least one watch request gets a corresponding WatchedResource object whose main\n    job is to store the current Filter status\n    \"\"\"\n\n    def __init__(  # pylint: disable=too-many-arguments\n        self,\n        reconcile_thread: RECONCILE_THREAD_TYPE,\n        kind: str,\n        api_version: str,\n        namespace: Optional[str] = None,\n        deploy_manager: DeployManagerBase = None,\n        leadership_manager: LeadershipManagerBase = None,\n    ):\n        \"\"\"Initialize a WatchThread by assigning instance variables and creating maps\n\n        Args:\n            reconcile_thread: ReconcileThread\n                The reconcile thread to submit requests to\n            kind: str\n                The kind to watch\n            api_version: str\n                The api_version to watch\n            namespace: Optional[str] = None\n                The namespace to watch. If none then cluster-wide\n            deploy_manager: DeployManagerBase = None\n                The deploy_manager to watch events\n            leadership_manager: LeadershipManagerBase = None\n                The leadership manager to use for elections\n        \"\"\"\n        # Setup initial variables\n        self.reconcile_thread = reconcile_thread\n        self.kind = kind\n        self.api_version = api_version\n        self.namespace = namespace\n\n        name = f\"watch_thread_{self.api_version}_{self.kind}\"\n        if self.namespace:\n            name = name + f\"_{self.namespace}\"\n        super().__init__(\n            name=name,\n            daemon=True,\n            deploy_manager=deploy_manager,\n            leadership_manager=leadership_manager,\n        )\n\n        # Setup kubernetes watch resource\n        self.kubernetes_watch = watch.Watch()\n\n        # Setup watched resources and request mappings. watched_resources\n        # is used to track the current status of a resource in a cluster and also includes\n        # the current filters. watch_request tracks all of the Controllers that have watched\n        # a specific resource or groupings of resources\n        self.watched_resources: Dict[str, WatchedResource] = {}\n        self.watch_requests: Dict[str, Set[WatchRequest]] = {}\n\n        # Lock for adding/gathering watch requests\n        self.watch_request_lock = Lock()\n\n        # Variables for tracking retries\n        self.attempts_left = config.python_watch_manager.watch_retry_count\n        self.retry_delay = parse_time_delta(\n            config.python_watch_manager.watch_retry_delay or \"\"\n        )\n\n    def run(self):\n        \"\"\"The WatchThread's control loop continuously watches the DeployManager for any new\n        events. For every event it gets it gathers all the WatchRequests whose `watched` value\n        applies. The thread then initializes a WatchedObject if one doesn't already exist and\n        tests the event against each request's Filter. Finally, it submits a ReconcileRequest\n        for all events that pass\n        \"\"\"\n\n        # Check for leadership and shutdown at the start\n        list_resource_version = 0\n        while True:\n            try:\n                if not self.check_preconditions():\n                    log.debug(\"Checking preconditions failed. Shutting down\")\n                    return\n\n                for event in self.deploy_manager.watch_objects(\n                    self.kind,\n                    self.api_version,\n                    namespace=self.namespace,\n                    resource_version=list_resource_version,\n                    watch_manager=self.kubernetes_watch,\n                ):\n                    # Validate leadership on each event\n                    if not self.check_preconditions():\n                        log.debug(\"Checking preconditions failed. Shutting down\")\n                        return\n\n                    resource = event.resource\n\n                    # Gather all the watch requests which apply to this event\n                    watch_requests = self._gather_resource_requests(resource)\n                    if not watch_requests:\n                        log.debug2(\"Skipping resource without requested watch\")\n                        self._clean_event(event)\n                        continue\n\n                    # Ensure a watched object exists for every resource\n                    if resource.uid not in self.watched_resources:\n                        self._create_watched_resource(resource, watch_requests)\n\n                    # Check both global and watch specific filters\n                    watch_requests = self._check_filters(\n                        watch_requests, resource, event.type\n                    )\n                    if not watch_requests:\n                        log.debug2(\n                            \"Skipping event %s as all requests failed filters\", event\n                        )\n                        self._clean_event(event)\n                        continue\n\n                    # Push a reconcile request for each watch requested\n                    for watch_request in watch_requests:\n                        log.debug(\n                            \"Requesting reconcile for %s\",\n                            resource,\n                            extra={\"resource\": watch_request.requester.get_resource()},\n                        )\n                        self._request_reconcile(event, watch_request)\n\n                    # Clean up any resources used for the event\n                    self._clean_event(event)\n\n                # Update the resource version to only get new events\n                list_resource_version = self.kubernetes_watch.resource_version\n            except Exception as exc:\n                log.info(\n                    \"Exception raised when attempting to watch %s\",\n                    repr(exc),\n                    exc_info=exc,\n                )\n                if self.attempts_left &lt;= 0:\n                    log.error(\n                        \"Unable to start watch within %d attempts\",\n                        config.python_watch_manager.watch_retry_count,\n                    )\n                    os._exit(1)\n\n                if not self.wait_on_precondition(self.retry_delay.total_seconds()):\n                    log.debug(\n                        \"Checking preconditions failed during retry. Shutting down\"\n                    )\n                    return\n                self.attempts_left = self.attempts_left - 1\n                log.info(\"Restarting watch with %d attempts left\", self.attempts_left)\n\n    ## Class Interface ###################################################\n\n    def stop_thread(self):\n        \"\"\"Override stop_thread to stop the kubernetes client's Watch as well\"\"\"\n        super().stop_thread()\n        self.kubernetes_watch.stop()\n\n    ## Public Interface ###################################################\n\n    def request_watch(self, watch_request: WatchRequest):\n        \"\"\"Add a watch request if it doesn't exist\n\n        Args:\n            watch_request: WatchRequest\n                The watch_request to add\n        \"\"\"\n        requester_id = watch_request.requester\n\n        # Acquire the watch request lock before starting work\n        with self.watch_request_lock:\n            if watch_request in self.watch_requests.get(requester_id.global_id, []):\n                log.debug3(\"Request already added\")\n                return\n\n            # Create watch request for this kind/api_version. Use global id\n            # as watch thread is already namespaced/global\n            log.debug3(\"Adding action with key %s\", requester_id.global_id)\n            self.watch_requests.setdefault(requester_id.global_id, set()).add(\n                watch_request\n            )\n\n    ## WatchRequest Functions  ###################################################\n\n    def _gather_resource_requests(self, resource: ManagedObject) -&gt; List[WatchRequest]:\n        \"\"\"Gather the list of actions that apply to this specific Kube event based on\n        the ownerRefs and the resource itself.\n\n        Args:\n            resource: ManagedObject\n                The resource for this event\n\n        Returns:\n            request_list: List[WatchRequest]\n                The list of watch requests that apply\n        \"\"\"\n\n        request_list = []\n\n        # Acquire the watch request lock\n        with self.watch_request_lock:\n            # Check if the event resource can be reconciled directly like in the case of\n            # Controllers\n            resource_id = ResourceId.from_resource(resource)\n            for request in self.watch_requests.get(resource_id.global_id, []):\n                # Check if request has a specific name and if this event matches\n                if request.requester.name and request.requester.name != resource.name:\n                    continue\n\n                unique_request = copy.deepcopy(request)\n                if not unique_request.requester.name:\n                    unique_request.requester = dataclasses.replace(\n                        unique_request.requester, name=resource_id.name\n                    )\n\n                log.debug3(\n                    \"Gathering request for controller %s from %s\",\n                    unique_request.controller_type,\n                    resource_id.global_id,\n                )\n                request_list.append(unique_request)\n\n            # Check for any owners watching this resource\n            for owner_ref in resource.metadata.get(\"ownerReferences\", []):\n                owner_id = ResourceId.from_owner_ref(\n                    owner_ref, namespace=resource_id.namespace\n                )\n\n                if owner_id.global_id not in self.watch_requests:\n                    log.debug3(\"Skipping event with owner_key: %s\", owner_id.global_id)\n                    continue\n\n                for request in self.watch_requests.get(owner_id.global_id, []):\n                    # If request has a specific name then ensure it matches\n                    if (\n                        request.requester.name\n                        and request.requester.name != owner_ref.get(\"name\")\n                    ):\n                        continue\n\n                    # If request doesn't already have a name then force\n                    # this resource. This allows multiple controllers with\n                    # the same kind/api_version to own the same resource\n                    unique_request = copy.deepcopy(request)\n                    if not unique_request.requester.name:\n                        unique_request.requester = dataclasses.replace(\n                            unique_request.requester, name=owner_id.name\n                        )\n\n                    log.debug3(\n                        \"Gathering request for controller %s from %s\",\n                        unique_request.controller_type,\n                        owner_ref,\n                    )\n                    request_list.append(unique_request)\n\n        return request_list\n\n    def _request_reconcile(self, event: KubeWatchEvent, request: WatchRequest):\n        \"\"\"Request a reconcile for a kube event\n\n        Args:\n            event: KubeWatchEvent\n                The KubeWatchEvent that triggered the reconcile\n            request: WatchRequest\n                The object that's requested a reconcile\n        \"\"\"\n\n        resource = event.resource\n        event_type = event.type\n        requester_id = request.requester\n\n        # If the watch request is for a different object (e.g dependent watch) then\n        # fetch the correct resource to reconcile\n        if (\n            requester_id.kind != event.resource.kind\n            or requester_id.api_version != event.resource.api_version\n            or (requester_id.name and requester_id.name != event.resource.name)\n        ):\n            success, obj = self.deploy_manager.get_object_current_state(\n                kind=requester_id.kind,\n                name=requester_id.name,\n                namespace=event.resource.namespace,\n                api_version=requester_id.api_version,\n            )\n            if not success or not obj:\n                log.warning(\n                    \"Unable to fetch owner resource %s\", requester_id.get_named_id()\n                )\n                return\n\n            resource = ManagedObject(obj)\n            event_type = ReconcileRequestType.DEPENDENT\n\n        # Generate the request and push one for each watched action to the reconcile thread\n        request = ReconcileRequest(request.controller_type, event_type, resource)\n        self.reconcile_thread.push_request(request)\n\n    ## Watched Resource Functions  ###################################################\n\n    def _create_watched_resource(\n        self,\n        resource: ManagedObject,\n        watch_requests: List[WatchRequest],\n    ):\n        \"\"\"Create a WatchedResource and initialize it's filters\n\n        Args:\n            resource: ManagedObject\n                The resource being watched\n            watch_requests: List[WatchRequest]\n                The list of requests that apply to this resource\n\n        \"\"\"\n        # update the watched resources dict\n        if resource.uid in self.watched_resources:\n            return\n\n        # Setup filter dict with global filters\n        filter_dict = {None: FilterManager(get_configured_filter(), resource)}\n        for request in watch_requests:\n            filter_dict[request.requester.get_named_id()] = FilterManager(\n                request.filters, resource\n            )\n\n        # Add watched resource to mapping\n        self.watched_resources[resource.uid] = WatchedResource(\n            gvk=ResourceId.from_resource(resource), filters=filter_dict\n        )\n\n    def _clean_event(self, event: KubeWatchEvent):\n        \"\"\"Call this function after processing every event to clean any leftover resources\n\n        Args:\n            event: KubeWatchEvent\n                The kube event to clean up\n        \"\"\"\n        if event.type == KubeEventType.DELETED:\n            self.watched_resources.pop(event.resource.uid, None)\n\n    ## Filter Functions  ###################################################\n\n    def _check_filters(\n        self,\n        watch_requests: List[WatchRequest],\n        resource: ManagedObject,\n        event: KubeEventType,\n    ) -&gt; List[WatchRequest]:\n        \"\"\"Check a resource and event against both global and request specific filters\n\n        Args:\n            watch_requests: List[WatchRequest]\n                List of watch requests whose filters should be checked\n            resource: ManagedObject\n                The resource being filtered\n            event: KubeEventType\n                THe event type being filtered\n\n        Returns:\n            successful_requests: List[WatchRequest]\n                The list of requests that passed the filter\n\n        \"\"\"\n\n        if resource.uid not in self.watched_resources:\n            return []\n\n        # If the default watched resource filter fails then no need to\n        # check any watch requests\n        watched_resource = self.watched_resources[resource.uid]\n        if not watched_resource.filters[None].update_and_test(resource, event):\n            return []\n\n        output_requests = []\n\n        # Check the watch requests for any of their filters\n        for request in watch_requests:\n            requester_id = request.requester.get_named_id()\n\n            # If this is the first time this watched resource has seen this request then\n            # initialize the filters\n            if requester_id not in watched_resource.filters:\n                watched_resource.filters[requester_id] = FilterManager(\n                    request.filters, resource\n                )\n\n            if not watched_resource.filters[requester_id].update_and_test(\n                resource, event\n            ):\n                continue\n\n            output_requests.append(request)\n\n        return output_requests\n</code></pre> <code>__init__(reconcile_thread, kind, api_version, namespace=None, deploy_manager=None, leadership_manager=None)</code> <p>Initialize a WatchThread by assigning instance variables and creating maps</p> <p>Parameters:</p> Name Type Description Default <code>reconcile_thread</code> <code>RECONCILE_THREAD_TYPE</code> <p>ReconcileThread The reconcile thread to submit requests to</p> required <code>kind</code> <code>str</code> <p>str The kind to watch</p> required <code>api_version</code> <code>str</code> <p>str The api_version to watch</p> required <code>namespace</code> <code>Optional[str]</code> <p>Optional[str] = None The namespace to watch. If none then cluster-wide</p> <code>None</code> <code>deploy_manager</code> <code>DeployManagerBase</code> <p>DeployManagerBase = None The deploy_manager to watch events</p> <code>None</code> <code>leadership_manager</code> <code>LeadershipManagerBase</code> <p>LeadershipManagerBase = None The leadership manager to use for elections</p> <code>None</code> Source code in <code>oper8/watch_manager/python_watch_manager/threads/watch.py</code> <pre><code>def __init__(  # pylint: disable=too-many-arguments\n    self,\n    reconcile_thread: RECONCILE_THREAD_TYPE,\n    kind: str,\n    api_version: str,\n    namespace: Optional[str] = None,\n    deploy_manager: DeployManagerBase = None,\n    leadership_manager: LeadershipManagerBase = None,\n):\n    \"\"\"Initialize a WatchThread by assigning instance variables and creating maps\n\n    Args:\n        reconcile_thread: ReconcileThread\n            The reconcile thread to submit requests to\n        kind: str\n            The kind to watch\n        api_version: str\n            The api_version to watch\n        namespace: Optional[str] = None\n            The namespace to watch. If none then cluster-wide\n        deploy_manager: DeployManagerBase = None\n            The deploy_manager to watch events\n        leadership_manager: LeadershipManagerBase = None\n            The leadership manager to use for elections\n    \"\"\"\n    # Setup initial variables\n    self.reconcile_thread = reconcile_thread\n    self.kind = kind\n    self.api_version = api_version\n    self.namespace = namespace\n\n    name = f\"watch_thread_{self.api_version}_{self.kind}\"\n    if self.namespace:\n        name = name + f\"_{self.namespace}\"\n    super().__init__(\n        name=name,\n        daemon=True,\n        deploy_manager=deploy_manager,\n        leadership_manager=leadership_manager,\n    )\n\n    # Setup kubernetes watch resource\n    self.kubernetes_watch = watch.Watch()\n\n    # Setup watched resources and request mappings. watched_resources\n    # is used to track the current status of a resource in a cluster and also includes\n    # the current filters. watch_request tracks all of the Controllers that have watched\n    # a specific resource or groupings of resources\n    self.watched_resources: Dict[str, WatchedResource] = {}\n    self.watch_requests: Dict[str, Set[WatchRequest]] = {}\n\n    # Lock for adding/gathering watch requests\n    self.watch_request_lock = Lock()\n\n    # Variables for tracking retries\n    self.attempts_left = config.python_watch_manager.watch_retry_count\n    self.retry_delay = parse_time_delta(\n        config.python_watch_manager.watch_retry_delay or \"\"\n    )\n</code></pre> <code>request_watch(watch_request)</code> <p>Add a watch request if it doesn't exist</p> <p>Parameters:</p> Name Type Description Default <code>watch_request</code> <code>WatchRequest</code> <p>WatchRequest The watch_request to add</p> required Source code in <code>oper8/watch_manager/python_watch_manager/threads/watch.py</code> <pre><code>def request_watch(self, watch_request: WatchRequest):\n    \"\"\"Add a watch request if it doesn't exist\n\n    Args:\n        watch_request: WatchRequest\n            The watch_request to add\n    \"\"\"\n    requester_id = watch_request.requester\n\n    # Acquire the watch request lock before starting work\n    with self.watch_request_lock:\n        if watch_request in self.watch_requests.get(requester_id.global_id, []):\n            log.debug3(\"Request already added\")\n            return\n\n        # Create watch request for this kind/api_version. Use global id\n        # as watch thread is already namespaced/global\n        log.debug3(\"Adding action with key %s\", requester_id.global_id)\n        self.watch_requests.setdefault(requester_id.global_id, set()).add(\n            watch_request\n        )\n</code></pre> <code>run()</code> <p>The WatchThread's control loop continuously watches the DeployManager for any new events. For every event it gets it gathers all the WatchRequests whose <code>watched</code> value applies. The thread then initializes a WatchedObject if one doesn't already exist and tests the event against each request's Filter. Finally, it submits a ReconcileRequest for all events that pass</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/watch.py</code> <pre><code>def run(self):\n    \"\"\"The WatchThread's control loop continuously watches the DeployManager for any new\n    events. For every event it gets it gathers all the WatchRequests whose `watched` value\n    applies. The thread then initializes a WatchedObject if one doesn't already exist and\n    tests the event against each request's Filter. Finally, it submits a ReconcileRequest\n    for all events that pass\n    \"\"\"\n\n    # Check for leadership and shutdown at the start\n    list_resource_version = 0\n    while True:\n        try:\n            if not self.check_preconditions():\n                log.debug(\"Checking preconditions failed. Shutting down\")\n                return\n\n            for event in self.deploy_manager.watch_objects(\n                self.kind,\n                self.api_version,\n                namespace=self.namespace,\n                resource_version=list_resource_version,\n                watch_manager=self.kubernetes_watch,\n            ):\n                # Validate leadership on each event\n                if not self.check_preconditions():\n                    log.debug(\"Checking preconditions failed. Shutting down\")\n                    return\n\n                resource = event.resource\n\n                # Gather all the watch requests which apply to this event\n                watch_requests = self._gather_resource_requests(resource)\n                if not watch_requests:\n                    log.debug2(\"Skipping resource without requested watch\")\n                    self._clean_event(event)\n                    continue\n\n                # Ensure a watched object exists for every resource\n                if resource.uid not in self.watched_resources:\n                    self._create_watched_resource(resource, watch_requests)\n\n                # Check both global and watch specific filters\n                watch_requests = self._check_filters(\n                    watch_requests, resource, event.type\n                )\n                if not watch_requests:\n                    log.debug2(\n                        \"Skipping event %s as all requests failed filters\", event\n                    )\n                    self._clean_event(event)\n                    continue\n\n                # Push a reconcile request for each watch requested\n                for watch_request in watch_requests:\n                    log.debug(\n                        \"Requesting reconcile for %s\",\n                        resource,\n                        extra={\"resource\": watch_request.requester.get_resource()},\n                    )\n                    self._request_reconcile(event, watch_request)\n\n                # Clean up any resources used for the event\n                self._clean_event(event)\n\n            # Update the resource version to only get new events\n            list_resource_version = self.kubernetes_watch.resource_version\n        except Exception as exc:\n            log.info(\n                \"Exception raised when attempting to watch %s\",\n                repr(exc),\n                exc_info=exc,\n            )\n            if self.attempts_left &lt;= 0:\n                log.error(\n                    \"Unable to start watch within %d attempts\",\n                    config.python_watch_manager.watch_retry_count,\n                )\n                os._exit(1)\n\n            if not self.wait_on_precondition(self.retry_delay.total_seconds()):\n                log.debug(\n                    \"Checking preconditions failed during retry. Shutting down\"\n                )\n                return\n            self.attempts_left = self.attempts_left - 1\n            log.info(\"Restarting watch with %d attempts left\", self.attempts_left)\n</code></pre> <code>stop_thread()</code> <p>Override stop_thread to stop the kubernetes client's Watch as well</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/watch.py</code> <pre><code>def stop_thread(self):\n    \"\"\"Override stop_thread to stop the kubernetes client's Watch as well\"\"\"\n    super().stop_thread()\n    self.kubernetes_watch.stop()\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.threads.watch.create_resource_watch","title":"<code>create_resource_watch(watch_request, reconcile_thread, deploy_manager, leadership_manager)</code>","text":"<p>Create or request a watch for a resource. This function will either append the request to an existing thread or create a new one. This function will also start the thread if any other watch threads have already been started.</p> <p>Parameters:</p> Name Type Description Default <code>watch_request</code> <code>WatchRequest</code> <p>WatchRequest The watch request to submit</p> required <code>reconcile_thread</code> <code>RECONCILE_THREAD_TYPE</code> <p>ReconcileThread The ReconcileThread to submit ReconcileRequests to</p> required <code>deploy_manager</code> <code>DeployManagerBase</code> <p>DeployManagerBase The DeployManager to use with the Thread</p> required <code>leadership_manager</code> <code>LeadershipManagerBase</code> <p>LeadershipManagerBase The LeadershipManager to use for election</p> required <p>Returns:</p> Name Type Description <code>watch_thread</code> <code>WatchThread</code> <p>WatchThread The watch_thread that is watching the request</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/watch.py</code> <pre><code>def create_resource_watch(\n    watch_request: WatchRequest,\n    reconcile_thread: RECONCILE_THREAD_TYPE,\n    deploy_manager: DeployManagerBase,\n    leadership_manager: LeadershipManagerBase,\n) -&gt; WatchThread:\n    \"\"\"Create or request a watch for a resource. This function will either append the request to\n    an existing thread or create a new one. This function will also start the thread if any\n    other watch threads have already been started.\n\n    Args:\n        watch_request: WatchRequest\n            The watch request to submit\n        reconcile_thread: ReconcileThread\n            The ReconcileThread to submit ReconcileRequests to\n        deploy_manager: DeployManagerBase\n            The DeployManager to use with the Thread\n        leadership_manager: LeadershipManagerBase\n            The LeadershipManager to use for election\n\n    Returns:\n        watch_thread: WatchThread\n            The watch_thread that is watching the request\n    \"\"\"\n    watch_thread = None\n    watched_id = watch_request.watched\n\n    # First check for a global watch before checking for a specific namespace watch\n    if watched_id.global_id in watch_threads:\n        log.debug2(\"Found existing global watch thread for %s\", watch_request)\n        watch_thread = watch_threads[watched_id.global_id]\n\n    elif watched_id.namespace and watched_id.namespaced_id in watch_threads:\n        log.debug2(\"Found existing namespaced watch thread for %s\", watch_request)\n        watch_thread = watch_threads[watched_id.namespaced_id]\n\n    # Create a watch thread if it doesn't exist\n    if not watch_thread:\n        log.debug2(\"Creating new WatchThread for %s\", watch_request)\n        watch_thread = WatchThread(\n            reconcile_thread,\n            watched_id.kind,\n            watched_id.api_version,\n            watched_id.namespace,\n            deploy_manager,\n            leadership_manager,\n        )\n\n        watch_key = watched_id.get_id()\n        watch_threads[watch_key] = watch_thread\n\n        # Only start the watch thread if another is already watching\n        for thread in watch_threads.values():\n            if thread.is_alive():\n                watch_thread.start_thread()\n                break\n\n    # Add action to controller\n    watch_thread.request_watch(watch_request)\n    return watch_thread\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.threads.watch.get_resource_watches","title":"<code>get_resource_watches()</code>","text":"<p>Get the list of all watch_threads</p> <p>Returns:</p> Name Type Description <code>list_of_watches</code> <code>List[WatchThread]</code> <p>List[WatchThread] List of watch threads</p> Source code in <code>oper8/watch_manager/python_watch_manager/threads/watch.py</code> <pre><code>def get_resource_watches() -&gt; List[WatchThread]:\n    \"\"\"Get the list of all watch_threads\n\n    Returns:\n        list_of_watches: List[WatchThread]\n            List of watch threads\n    \"\"\"\n    return watch_threads.values()\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils","title":"<code>utils</code>","text":"<p>Import All functions, constants, and class from utils module</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.common","title":"<code>common</code>","text":"<p>Shared utilities for the PythonWatchManager</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.common.get_logging_handlers","title":"<code>get_logging_handlers()</code>","text":"<p>Get the current logging handlers</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/common.py</code> <pre><code>def get_logging_handlers() -&gt; List[logging.Handler]:\n    \"\"\"Get the current logging handlers\"\"\"\n    logger = logging.getLogger()\n    if not logger.handlers:\n        handler = logging.StreamHandler()\n        logger.addHandler(handler)\n\n    return logger.handlers\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.common.get_operator_namespace","title":"<code>get_operator_namespace()</code>","text":"<p>Get the current namespace from a kubernetes file or config</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/common.py</code> <pre><code>def get_operator_namespace() -&gt; str:\n    \"\"\"Get the current namespace from a kubernetes file or config\"\"\"\n    # Default to in cluster namespace file\n    namespace_file = pathlib.Path(\n        \"/var/run/secrets/kubernetes.io/serviceaccount/namespace\"\n    )\n    if namespace_file.is_file():\n        return namespace_file.read_text(encoding=\"utf-8\")\n    return config.python_watch_manager.lock.namespace\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.common.get_pod_name","title":"<code>get_pod_name()</code>","text":"<p>Get the current pod from env variables, config, or hostname</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/common.py</code> <pre><code>def get_pod_name() -&gt; str:\n    \"\"\"Get the current pod from env variables, config, or hostname\"\"\"\n\n    pod_name = config.pod_name\n    if not pod_name:\n        log.warning(\"Pod name not detected, falling back to hostname\")\n        pod_name = platform.node().split(\".\")[0]\n\n    return pod_name\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.common.obj_to_hash","title":"<code>obj_to_hash(obj)</code>","text":"<p>Get the hash of any jsonable python object</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Any The object to hash</p> required <p>Returns:</p> Name Type Description <code>hash</code> <code>str</code> <p>str The hash of obj</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/common.py</code> <pre><code>def obj_to_hash(obj: Any) -&gt; str:\n    \"\"\"Get the hash of any jsonable python object\n\n    Args:\n        obj: Any\n            The object to hash\n\n    Returns:\n        hash: str\n            The hash of obj\n    \"\"\"\n    return hash(json.dumps(obj, sort_keys=True))\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.common.parse_time_delta","title":"<code>parse_time_delta(time_str)</code>","text":"<p>Parse a string into a timedelta. Excepts values in the following formats: 1h, 5m, 10s, etc</p> <p>Parameters:</p> Name Type Description Default <code>time_str</code> <code>str</code> <p>str The string representation of a timedelta</p> required <p>Returns:</p> Name Type Description <code>result</code> <code>Optional[timedelta]</code> <p>Optional[timedelta] The parsed timedelta if one could be found</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/common.py</code> <pre><code>def parse_time_delta(\n    time_str: str,\n) -&gt; Optional[timedelta]:  # pylint: disable=inconsistent-return-statements\n    \"\"\"Parse a string into a timedelta. Excepts values in the\n    following formats: 1h, 5m, 10s, etc\n\n    Args:\n        time_str: str\n            The string representation of a timedelta\n\n    Returns:\n        result: Optional[timedelta]\n            The parsed timedelta if one could be found\n    \"\"\"\n    parts = regex.match(time_str)\n    if not parts or all(part is None for part in parts.groupdict().values()):\n        return None\n    parts = parts.groupdict()\n    time_params = {}\n    for name, param in parts.items():\n        if param:\n            time_params[name] = float(param)\n    return timedelta(**time_params)\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.constants","title":"<code>constants</code>","text":"<p>Useful Constants</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.log_handler","title":"<code>log_handler</code>","text":"<p>Log handler helper class</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.log_handler.LogQueueHandler","title":"<code>LogQueueHandler</code>","text":"<p>               Bases: <code>QueueHandler</code></p> <p>Log Handler class to collect messages from a child processes and pass them to the root process via a multiprocess queue</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/log_handler.py</code> <pre><code>class LogQueueHandler(QueueHandler):\n    \"\"\"\n    Log Handler class to collect messages from a child processes and pass\n    them to the root process via a multiprocess queue\n    \"\"\"\n\n    def __init__(self, queue: QUEUE_TYPE, manifest: ManagedObject = None):\n        \"\"\"Initialize the queue handler and instance variables\n\n        Args:\n            queue: \"Queue[Any]\"\n                The queue to pass messages to\n            manifest: ManagedObject\n                The manifest of the current process. This is only used if it can't find\n                the resource on the current formatter\n        \"\"\"\n        super().__init__(queue)\n        self.manifest = manifest\n\n    def prepare(self, record: LogRecord) -&gt; LogRecord:\n        \"\"\"Prep a record for pickling before sending it to the queue\n\n        Args:\n            record: LogRecord\n                The record to be prepared\n\n        Returns:\n            prepared_record: LogRecord\n                The prepared record ready to be pickled\n        \"\"\"\n\n        # Duplicate record to preserve other handlers\n        record = copy.copy(record)\n\n        # get the currently used formatter\n        formatter = self.formatter if self.formatter else Formatter()\n\n        # Exceptions can't always be pickled so manually process\n        # the record but remove the exc_info This retains the\n        # the processed exc_txt but allows the parent process to reformat\n        # the message\n        if record.exc_info:\n            record.exc_text = formatter.formatException(record.exc_info)\n            record.exc_info = None\n\n        # In case there are exceptions/unpicklable objects in the logging\n        # args then manually compute the message. After computing clear the\n        # message&amp;args values to allow the parent process to reformat the\n        # record\n        record.msg = record.getMessage()\n        record.args = []\n\n        # Take the manifest from the current formatter and pass it back up\n        resource = {}\n        if hasattr(formatter, \"manifest\"):\n            resource = formatter.manifest\n        elif self.manifest:\n            resource = self.manifest\n\n        # Only copy required resource keys to the record\n        resource_metadata = resource.get(\"metadata\", {})\n        record.resource = {\n            \"kind\": resource.get(\"kind\"),\n            \"apiVersion\": resource.get(\"apiVersion\"),\n            \"metadata\": {\n                \"name\": resource_metadata.get(\"name\"),\n                \"namespace\": resource_metadata.get(\"namespace\"),\n                \"resourceVersion\": resource_metadata.get(\"resourceVersion\"),\n            },\n        }\n\n        return record\n</code></pre> <code>__init__(queue, manifest=None)</code> <p>Initialize the queue handler and instance variables</p> <p>Parameters:</p> Name Type Description Default <code>queue</code> <code>QUEUE_TYPE</code> <p>\"Queue[Any]\" The queue to pass messages to</p> required <code>manifest</code> <code>ManagedObject</code> <p>ManagedObject The manifest of the current process. This is only used if it can't find the resource on the current formatter</p> <code>None</code> Source code in <code>oper8/watch_manager/python_watch_manager/utils/log_handler.py</code> <pre><code>def __init__(self, queue: QUEUE_TYPE, manifest: ManagedObject = None):\n    \"\"\"Initialize the queue handler and instance variables\n\n    Args:\n        queue: \"Queue[Any]\"\n            The queue to pass messages to\n        manifest: ManagedObject\n            The manifest of the current process. This is only used if it can't find\n            the resource on the current formatter\n    \"\"\"\n    super().__init__(queue)\n    self.manifest = manifest\n</code></pre> <code>prepare(record)</code> <p>Prep a record for pickling before sending it to the queue</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>LogRecord</code> <p>LogRecord The record to be prepared</p> required <p>Returns:</p> Name Type Description <code>prepared_record</code> <code>LogRecord</code> <p>LogRecord The prepared record ready to be pickled</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/log_handler.py</code> <pre><code>def prepare(self, record: LogRecord) -&gt; LogRecord:\n    \"\"\"Prep a record for pickling before sending it to the queue\n\n    Args:\n        record: LogRecord\n            The record to be prepared\n\n    Returns:\n        prepared_record: LogRecord\n            The prepared record ready to be pickled\n    \"\"\"\n\n    # Duplicate record to preserve other handlers\n    record = copy.copy(record)\n\n    # get the currently used formatter\n    formatter = self.formatter if self.formatter else Formatter()\n\n    # Exceptions can't always be pickled so manually process\n    # the record but remove the exc_info This retains the\n    # the processed exc_txt but allows the parent process to reformat\n    # the message\n    if record.exc_info:\n        record.exc_text = formatter.formatException(record.exc_info)\n        record.exc_info = None\n\n    # In case there are exceptions/unpicklable objects in the logging\n    # args then manually compute the message. After computing clear the\n    # message&amp;args values to allow the parent process to reformat the\n    # record\n    record.msg = record.getMessage()\n    record.args = []\n\n    # Take the manifest from the current formatter and pass it back up\n    resource = {}\n    if hasattr(formatter, \"manifest\"):\n        resource = formatter.manifest\n    elif self.manifest:\n        resource = self.manifest\n\n    # Only copy required resource keys to the record\n    resource_metadata = resource.get(\"metadata\", {})\n    record.resource = {\n        \"kind\": resource.get(\"kind\"),\n        \"apiVersion\": resource.get(\"apiVersion\"),\n        \"metadata\": {\n            \"name\": resource_metadata.get(\"name\"),\n            \"namespace\": resource_metadata.get(\"namespace\"),\n            \"resourceVersion\": resource_metadata.get(\"resourceVersion\"),\n        },\n    }\n\n    return record\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.types","title":"<code>types</code>","text":"<p>Standard data types used through PWM</p>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.types.ABCSingletonMeta","title":"<code>ABCSingletonMeta</code>","text":"<p>               Bases: <code>Singleton</code>, <code>ABCMeta</code></p> <p>Shared metaclass for ABCMeta and Singleton</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>class ABCSingletonMeta(Singleton, abc.ABCMeta):\n    \"\"\"Shared metaclass for ABCMeta and Singleton\"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.types.ClassInfo","title":"<code>ClassInfo</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Class containing information describing a class. This is required when passing class references between processes which might have different sys paths like when using VCS</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>class ClassInfo(NamedTuple):\n    \"\"\"Class containing information describing a class. This is required when passing class\n    references between processes which might have different sys paths like when using VCS\"\"\"\n\n    moduleName: str\n    className: str\n\n    # Generation Utilities\n    @classmethod\n    def from_type(cls, class_obj: type) -&gt; \"ClassInfo\":\n        \"\"\"Create a ClassInfo from a class object\"\"\"\n        return cls(moduleName=class_obj.__module__, className=class_obj.__name__)\n\n    @classmethod\n    def from_obj(cls, obj) -&gt; \"ClassInfo\":\n        \"\"\"Create a ClassInfo from an existing object\"\"\"\n        return cls.from_type(type(obj))\n\n    # Get the class referenced described by the info\n    def to_class(self) -&gt; type:\n        \"\"\"Import and return a ClassInfo's type\"\"\"\n        module = importlib.import_module(self.moduleName)\n        if not module:\n            raise ValueError(f\"Invalid ControllerInfo Module: {self.moduleName}\")\n\n        if not hasattr(module, self.className):\n            raise ValueError(\n                f\"Invalid ControllerInfo: {self.className} not a member of {self.moduleName}\"\n            )\n\n        return getattr(module, self.className)\n</code></pre> <code>from_obj(obj)</code> <code>classmethod</code> <p>Create a ClassInfo from an existing object</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>@classmethod\ndef from_obj(cls, obj) -&gt; \"ClassInfo\":\n    \"\"\"Create a ClassInfo from an existing object\"\"\"\n    return cls.from_type(type(obj))\n</code></pre> <code>from_type(class_obj)</code> <code>classmethod</code> <p>Create a ClassInfo from a class object</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>@classmethod\ndef from_type(cls, class_obj: type) -&gt; \"ClassInfo\":\n    \"\"\"Create a ClassInfo from a class object\"\"\"\n    return cls(moduleName=class_obj.__module__, className=class_obj.__name__)\n</code></pre> <code>to_class()</code> <p>Import and return a ClassInfo's type</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>def to_class(self) -&gt; type:\n    \"\"\"Import and return a ClassInfo's type\"\"\"\n    module = importlib.import_module(self.moduleName)\n    if not module:\n        raise ValueError(f\"Invalid ControllerInfo Module: {self.moduleName}\")\n\n    if not hasattr(module, self.className):\n        raise ValueError(\n            f\"Invalid ControllerInfo: {self.className} not a member of {self.moduleName}\"\n        )\n\n    return getattr(module, self.className)\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.types.ReconcileProcess","title":"<code>ReconcileProcess</code>  <code>dataclass</code>","text":"<p>Dataclass to track a running reconcile. This includes the raw process object, the result pipe, and the request being reconciled</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>@dataclass\nclass ReconcileProcess:\n    \"\"\"Dataclass to track a running reconcile. This includes the raw process\n    object, the result pipe, and the request being reconciled\"\"\"\n\n    process: multiprocessing.Process\n    request: ReconcileRequest\n    pipe: Connection\n\n    def fileno(self):\n        \"\"\"Pass through fileno to process. Sentinel so this object can be\n        directly used by multiprocessing.connection.wait\"\"\"\n        return self.process.sentinel\n\n    def uid(self):\n        \"\"\"Get the uid for the resource being reconciled\"\"\"\n        return self.request.uid()\n</code></pre> <code>fileno()</code> <p>Pass through fileno to process. Sentinel so this object can be directly used by multiprocessing.connection.wait</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>def fileno(self):\n    \"\"\"Pass through fileno to process. Sentinel so this object can be\n    directly used by multiprocessing.connection.wait\"\"\"\n    return self.process.sentinel\n</code></pre> <code>uid()</code> <p>Get the uid for the resource being reconciled</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>def uid(self):\n    \"\"\"Get the uid for the resource being reconciled\"\"\"\n    return self.request.uid()\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.types.ReconcileRequest","title":"<code>ReconcileRequest</code>  <code>dataclass</code>","text":"<p>Class to represent one request to the ReconcileThread. This includes important information including the current resource and Controller being reconciled.</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>@dataclass\nclass ReconcileRequest:\n    \"\"\"Class to represent one request to the ReconcileThread. This includes\n    important information including the current resource and Controller being\n    reconciled.\n    \"\"\"\n\n    controller_type: Type[CONTROLLER_TYPE]\n    type: Union[ReconcileRequestType, KUBE_EVENT_TYPE_TYPE]\n    resource: ManagedObject\n    timestamp: datetime = datetime.now()\n\n    def uid(self):\n        \"\"\"Get the uid of the resource being reconciled\"\"\"\n        return self.resource.uid\n</code></pre> <code>uid()</code> <p>Get the uid of the resource being reconciled</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>def uid(self):\n    \"\"\"Get the uid of the resource being reconciled\"\"\"\n    return self.resource.uid\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.types.ReconcileRequestType","title":"<code>ReconcileRequestType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum to expand the possible KubeEventTypes to include PythonWatchManager specific events</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>class ReconcileRequestType(Enum):\n    \"\"\"Enum to expand the possible KubeEventTypes to include PythonWatchManager\n    specific events\"\"\"\n\n    # Used for events that are a requeue of an object\n    REQUEUED = \"REQUEUED\"\n\n    # Used for periodic reconcile events\n    PERIODIC = \"PERIODIC\"\n\n    # Used for when an event is a dependent resource of a controller\n    DEPENDENT = \"DEPENDENT\"\n\n    # Used as a sentinel to alert threads to stop\n    STOPPED = \"STOPPED\"\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.types.ResourceId","title":"<code>ResourceId</code>  <code>dataclass</code>","text":"<p>Class containing the information needed to identify a resource</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>@dataclass(eq=True, frozen=True)\nclass ResourceId:\n    \"\"\"Class containing the information needed to identify a resource\"\"\"\n\n    api_version: str\n    kind: str\n    name: str = None\n    namespace: str = None\n\n    # Id properties\n\n    @cached_property\n    def global_id(self) -&gt; str:\n        \"\"\"Get the global_id for a resource in the form kind.version.group\"\"\"\n        group_version = self.api_version.split(\"/\")\n        return \".\".join([self.kind, *reversed(group_version)])\n\n    @cached_property\n    def namespaced_id(self) -&gt; str:\n        \"\"\"Get the namespace specific id for a resource\"\"\"\n        return f\"{self.namespace}.{self.global_id}\"\n\n    # Helper Accessor functions\n    def get_id(self) -&gt; str:\n        \"\"\"Get the requisite id for a resource\"\"\"\n        return self.namespaced_id if self.namespace else self.global_id\n\n    def get_named_id(self) -&gt; str:\n        \"\"\"Get a named id for a resouce\"\"\"\n        return f\"{self.name}.{self.get_id()}\"\n\n    def get_resource(self) -&gt; dict:\n        \"\"\"Get a resource template from this id\"\"\"\n        return {\n            \"kind\": self.kind,\n            \"apiVersion\": self.api_version,\n            \"metadata\": {\"name\": self.name, \"namespace\": self.namespace},\n        }\n\n    # Helper Creation Functions\n    @classmethod\n    def from_resource(cls, resource: Union[ManagedObject, dict]) -&gt; \"ResourceId\":\n        \"\"\"Create a resource id from an existing resource\"\"\"\n        metadata = resource.get(\"metadata\", {})\n        return cls(\n            api_version=resource.get(\"apiVersion\"),\n            kind=resource.get(\"kind\"),\n            namespace=metadata.get(\"namespace\"),\n            name=metadata.get(\"name\"),\n        )\n\n    @classmethod\n    def from_owner_ref(cls, owner_ref: dict, namespace: str = None) -&gt; \"ResourceId\":\n        \"\"\"Create a resource id from an ownerRef\"\"\"\n        return cls(\n            api_version=owner_ref.get(\"apiVersion\"),\n            kind=owner_ref.get(\"kind\"),\n            namespace=namespace,\n            name=owner_ref.get(\"name\"),\n        )\n\n    @classmethod\n    def from_controller(\n        cls, controller: Type[CONTROLLER_TYPE], namespace: str = None\n    ) -&gt; \"ResourceId\":\n        \"\"\"Get a Controller's target as a resource id\"\"\"\n        return cls(\n            api_version=f\"{controller.group}/{controller.version}\",\n            kind=controller.kind,\n            namespace=namespace,\n        )\n</code></pre> <code>global_id</code> <code>cached</code> <code>property</code> <p>Get the global_id for a resource in the form kind.version.group</p> <code>namespaced_id</code> <code>cached</code> <code>property</code> <p>Get the namespace specific id for a resource</p> <code>from_controller(controller, namespace=None)</code> <code>classmethod</code> <p>Get a Controller's target as a resource id</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>@classmethod\ndef from_controller(\n    cls, controller: Type[CONTROLLER_TYPE], namespace: str = None\n) -&gt; \"ResourceId\":\n    \"\"\"Get a Controller's target as a resource id\"\"\"\n    return cls(\n        api_version=f\"{controller.group}/{controller.version}\",\n        kind=controller.kind,\n        namespace=namespace,\n    )\n</code></pre> <code>from_owner_ref(owner_ref, namespace=None)</code> <code>classmethod</code> <p>Create a resource id from an ownerRef</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>@classmethod\ndef from_owner_ref(cls, owner_ref: dict, namespace: str = None) -&gt; \"ResourceId\":\n    \"\"\"Create a resource id from an ownerRef\"\"\"\n    return cls(\n        api_version=owner_ref.get(\"apiVersion\"),\n        kind=owner_ref.get(\"kind\"),\n        namespace=namespace,\n        name=owner_ref.get(\"name\"),\n    )\n</code></pre> <code>from_resource(resource)</code> <code>classmethod</code> <p>Create a resource id from an existing resource</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>@classmethod\ndef from_resource(cls, resource: Union[ManagedObject, dict]) -&gt; \"ResourceId\":\n    \"\"\"Create a resource id from an existing resource\"\"\"\n    metadata = resource.get(\"metadata\", {})\n    return cls(\n        api_version=resource.get(\"apiVersion\"),\n        kind=resource.get(\"kind\"),\n        namespace=metadata.get(\"namespace\"),\n        name=metadata.get(\"name\"),\n    )\n</code></pre> <code>get_id()</code> <p>Get the requisite id for a resource</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>def get_id(self) -&gt; str:\n    \"\"\"Get the requisite id for a resource\"\"\"\n    return self.namespaced_id if self.namespace else self.global_id\n</code></pre> <code>get_named_id()</code> <p>Get a named id for a resouce</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>def get_named_id(self) -&gt; str:\n    \"\"\"Get a named id for a resouce\"\"\"\n    return f\"{self.name}.{self.get_id()}\"\n</code></pre> <code>get_resource()</code> <p>Get a resource template from this id</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>def get_resource(self) -&gt; dict:\n    \"\"\"Get a resource template from this id\"\"\"\n    return {\n        \"kind\": self.kind,\n        \"apiVersion\": self.api_version,\n        \"metadata\": {\"name\": self.name, \"namespace\": self.namespace},\n    }\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.types.Singleton","title":"<code>Singleton</code>","text":"<p>               Bases: <code>type</code></p> <p>MetaClass to limit a class to only one global instance. When the first instance is created it's attached to the Class and the next time someone initializes the class the original instance is returned</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>class Singleton(type):\n    \"\"\"MetaClass to limit a class to only one global instance. When the\n    first instance is created it's attached to the Class and the next\n    time someone initializes the class the original instance is returned\n    \"\"\"\n\n    def __call__(cls, *args, **kwargs):\n        if getattr(cls, \"_disable_singleton\", False):\n            return type.__call__(cls, *args, **kwargs)\n\n        # The _instance is attached to the class itself without looking upwards\n        # into any parent classes\n        if \"_instance\" not in cls.__dict__:\n            cls._instance = type.__call__(cls, *args, **kwargs)\n        return cls._instance\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.types.TimerEvent","title":"<code>TimerEvent</code>  <code>dataclass</code>","text":"<p>Class for keeping track of an item in the timer queue. Time is the only comparable field to support the TimerThreads priority queue</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>@dataclass(order=True)\nclass TimerEvent:\n    \"\"\"Class for keeping track of an item in the timer queue. Time is the\n    only comparable field to support the TimerThreads priority queue\"\"\"\n\n    time: datetime\n    action: callable = field(compare=False)\n    args: list = field(default_factory=list, compare=False)\n    kwargs: dict = field(default_factory=dict, compare=False)\n    stale: bool = field(default=False, compare=False)\n\n    def cancel(self):\n        \"\"\"Cancel this event. It will not be executed when read from the\n        queue\"\"\"\n        self.stale = True\n</code></pre> <code>cancel()</code> <p>Cancel this event. It will not be executed when read from the queue</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>def cancel(self):\n    \"\"\"Cancel this event. It will not be executed when read from the\n    queue\"\"\"\n    self.stale = True\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.types.WatchRequest","title":"<code>WatchRequest</code>  <code>dataclass</code>","text":"<p>A class for requesting a watch of a particular object. It contains information around the watched object, who requested the watch, the controller type to be reconciled, and any filters to be applied to just this request</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>@dataclass()\nclass WatchRequest:\n    \"\"\"A class for requesting a watch of a particular object. It contains information around the\n    watched object, who requested the watch, the controller type to be reconciled, and any filters\n    to be applied to just this request\"\"\"\n\n    watched: ResourceId\n    requester: ResourceId\n\n    # Watch request must have either type or info\n    controller_type: Type[CONTROLLER_TYPE] = None\n    controller_info: ClassInfo = None\n\n    # Don't compare filters when checking equality as we\n    # assume they're the same if they have the same controller\n    filters: List[Type[FILTER_TYPE]] = field(default_factory=list, compare=False)\n    filters_info: List[Type[ClassInfo]] = field(default_factory=list, compare=False)\n\n    def __hash__(self) -&gt; int:\n        return hash(\n            (\n                self.watched,\n                self.requester,\n                self.controller_type if self.controller_type else self.controller_info,\n            )\n        )\n</code></pre>"},{"location":"API%20References/#oper8.watch_manager.python_watch_manager.utils.types.WatchedResource","title":"<code>WatchedResource</code>  <code>dataclass</code>","text":"<p>A class for tracking a resource in the cluster. Every resource that has a requested watch will have a corresponding WatchedResource</p> Source code in <code>oper8/watch_manager/python_watch_manager/utils/types.py</code> <pre><code>@dataclass\nclass WatchedResource:\n    \"\"\"A class for tracking a resource in the cluster. Every resource that has a\n    requested watch will have a corresponding WatchedResource\"\"\"\n\n    gvk: ResourceId\n    # Each watched resource contains a dict of filters for each\n    # corresponding watch request. The key is the named_id of\n    # the requester or None for default filters. This aligns with\n    # the Controllers pwm_filters attribute\n    filters: Dict[str, FILTER_MANAGER_TYPE] = field(default_factory=dict)\n</code></pre>"},{"location":"API%20References/#oper8.x","title":"<code>x</code>","text":"<p>The oper8.x module holds common implementations of reusable patterns built on top of the abstractions in oper8. These are intended as reusable components that can be share across many operator implementations.</p> <p>One of the core principles of oper8 is that the schema for config is entirely up to the user (with the only exception being spec.version). In oper8.x, this is not the case and there are many config conventions (CRD schema and backend) that are encoded into the various utilities.</p>"},{"location":"API%20References/#oper8.x.datastores","title":"<code>datastores</code>","text":""},{"location":"API%20References/#oper8.x.datastores.connection_base","title":"<code>connection_base</code>","text":"<p>Base class definition for all datastore connections</p>"},{"location":"API%20References/#oper8.x.datastores.connection_base.DatastoreConnectionBase","title":"<code>DatastoreConnectionBase</code>","text":"<p>               Bases: <code>ABCStatic</code></p> <p>A DatastoreConnection is an object that holds all of the critical data to connect to a specific datastore type. A DatastoreConnection for a given datastore type MUST not care what implementation backs the connection.</p> Source code in <code>oper8/x/datastores/connection_base.py</code> <pre><code>class DatastoreConnectionBase(ABCStatic):\n    \"\"\"\n    A DatastoreConnection is an object that holds all of the critical data to\n    connect to a specific datastore type. A DatastoreConnection for a given\n    datastore type MUST not care what implementation backs the connection.\n    \"\"\"\n\n    ## Construction ############################################################\n\n    def __init__(self, session: Session):\n        \"\"\"Construct with the session so that it can be saved as a member\"\"\"\n        self._session = session\n\n    @property\n    def session(self) -&gt; Session:\n        return self._session\n\n    ## Abstract Interface ######################################################\n\n    @abc.abstractmethod\n    def to_dict(self) -&gt; dict:\n        \"\"\"Serialize the internal connection details to a dict object which can\n        be added directly to a subsystem's CR.\n\n        Returns:\n            config_dict:  dict\n                This dict will hold the keys and values that can be used to add\n                to a subsystem's datastores.connections section.\n        \"\"\"\n\n    @classmethod\n    @abc.abstractmethod\n    def from_dict(\n        cls, session: Session, config_dict: dict\n    ) -&gt; \"DatastoreConnectionBase\":\n        \"\"\"Parse a config_dict from a subsystem CR to create an instance of the\n        DatastoreConnection class.\n\n        Args:\n            session:  Session\n                The current deploy session\n            config_dict:  dict\n                This dict will hold the keys and values created by to_dict and\n                pulled from the subsystem CR.\n\n        Returns:\n            datastore_connection:  DatastoreConnectionBase\n                The constructed instance of the connection\n        \"\"\"\n\n    ## Shared Utilities ########################################################\n\n    def _fetch_secret_data(self, secret_name: str) -&gt; Optional[dict]:\n        \"\"\"Most connection implementations will need the ability to fetch secret\n        data from the cluster when loading from the CR dict, so this provides a\n        common implementation.\n\n        Args:\n            secret_name:  str\n                The name of the secret to fetch\n\n        Returns:\n            secret_data:  Optional[dict]\n                The content of the 'data' field in the secret with values base64\n                decoded if the secret is found, otherwise None\n        \"\"\"\n        success, content = self.session.get_object_current_state(\"Secret\", secret_name)\n        assert_cluster(success, f\"Fetching connection secret [{secret_name}] failed\")\n        if content is None:\n            return None\n        assert \"data\" in content, \"Got a secret without 'data'?\"\n        return {\n            key: common.b64_secret_decode(val) for key, val in content[\"data\"].items()\n        }\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.connection_base.DatastoreConnectionBase.__init__","title":"<code>__init__(session)</code>","text":"<p>Construct with the session so that it can be saved as a member</p> Source code in <code>oper8/x/datastores/connection_base.py</code> <pre><code>def __init__(self, session: Session):\n    \"\"\"Construct with the session so that it can be saved as a member\"\"\"\n    self._session = session\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.connection_base.DatastoreConnectionBase.from_dict","title":"<code>from_dict(session, config_dict)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Parse a config_dict from a subsystem CR to create an instance of the DatastoreConnection class.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current deploy session</p> required <code>config_dict</code> <code>dict</code> <p>dict This dict will hold the keys and values created by to_dict and pulled from the subsystem CR.</p> required <p>Returns:</p> Name Type Description <code>datastore_connection</code> <code>DatastoreConnectionBase</code> <p>DatastoreConnectionBase The constructed instance of the connection</p> Source code in <code>oper8/x/datastores/connection_base.py</code> <pre><code>@classmethod\n@abc.abstractmethod\ndef from_dict(\n    cls, session: Session, config_dict: dict\n) -&gt; \"DatastoreConnectionBase\":\n    \"\"\"Parse a config_dict from a subsystem CR to create an instance of the\n    DatastoreConnection class.\n\n    Args:\n        session:  Session\n            The current deploy session\n        config_dict:  dict\n            This dict will hold the keys and values created by to_dict and\n            pulled from the subsystem CR.\n\n    Returns:\n        datastore_connection:  DatastoreConnectionBase\n            The constructed instance of the connection\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.connection_base.DatastoreConnectionBase.to_dict","title":"<code>to_dict()</code>  <code>abstractmethod</code>","text":"<p>Serialize the internal connection details to a dict object which can be added directly to a subsystem's CR.</p> <p>Returns:</p> Name Type Description <code>config_dict</code> <code>dict</code> <p>dict This dict will hold the keys and values that can be used to add to a subsystem's datastores.connections section.</p> Source code in <code>oper8/x/datastores/connection_base.py</code> <pre><code>@abc.abstractmethod\ndef to_dict(self) -&gt; dict:\n    \"\"\"Serialize the internal connection details to a dict object which can\n    be added directly to a subsystem's CR.\n\n    Returns:\n        config_dict:  dict\n            This dict will hold the keys and values that can be used to add\n            to a subsystem's datastores.connections section.\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.cos","title":"<code>cos</code>","text":"<p>Top level imports for the cos datastore type</p>"},{"location":"API%20References/#oper8.x.datastores.cos.connection","title":"<code>connection</code>","text":"<p>The common Connection type for a COS instance</p>"},{"location":"API%20References/#oper8.x.datastores.cos.connection.CosConnection","title":"<code>CosConnection</code>","text":"<p>               Bases: <code>DatastoreConnectionBase</code></p> <p>A CosConnection holds the core connection information for a named COS instance, regardless of what ICosComponent implements it. The key pieces of information are:</p> <ul> <li> <p>General config</p> <ul> <li>hostname: The hostname where the instance can be reached</li> <li>port: The port where the instance is listening</li> <li>bucket_name: The name of the bucket within the instance</li> </ul> </li> <li> <p>Auth</p> <ul> <li>auth_secret_name: The in-cluster name for the secret holding the     access_key and secret_key</li> <li>auth_secret_access_key_field: The field within the auth secret that     holds the access_key</li> <li>auth_secret_secret_key_field: The field within the auth secret that     holds the secret_key</li> </ul> </li> <li> <p>TLS:</p> <ul> <li>tls_cert: The content of the TLS cert if tls is enabled</li> <li>tls_secret_name: The in-cluster name for the secret holding the TLS     creds if tls is enabled</li> <li>tls_secret_cert_field: The field within the tls secret that holds the     cert</li> </ul> </li> </ul> Source code in <code>oper8/x/datastores/cos/connection.py</code> <pre><code>class CosConnection(DatastoreConnectionBase):\n    \"\"\"\n    A CosConnection holds the core connection information for a named COS\n    instance, regardless of what ICosComponent implements it. The key pieces of\n    information are:\n\n    * General config\n        * hostname: The hostname where the instance can be reached\n        * port: The port where the instance is listening\n        * bucket_name: The name of the bucket within the instance\n\n    * Auth\n        * auth_secret_name: The in-cluster name for the secret holding the\n            access_key and secret_key\n        * auth_secret_access_key_field: The field within the auth secret that\n            holds the access_key\n        * auth_secret_secret_key_field: The field within the auth secret that\n            holds the secret_key\n\n    * TLS:\n        * tls_cert: The content of the TLS cert if tls is enabled\n        * tls_secret_name: The in-cluster name for the secret holding the TLS\n            creds if tls is enabled\n        * tls_secret_cert_field: The field within the tls secret that holds the\n            cert\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        hostname: str,\n        port: int,\n        bucket_name: str,\n        auth_secret_name: str,\n        auth_secret_access_key_field: str,\n        auth_secret_secret_key_field: str,\n        tls_secret_name: Optional[str] = None,\n        tls_secret_cert_field: Optional[str] = None,\n        access_key: Optional[str] = None,\n        secret_key: Optional[str] = None,\n        tls_cert: Optional[str] = None,\n    ):\n        super().__init__(session)\n\n        # These fields must be passed in directly\n        self._hostname = hostname\n        self._port = port\n        self._bucket_name = bucket_name\n        self._auth_secret_name = auth_secret_name\n        self._auth_secret_access_key_field = auth_secret_access_key_field\n        self._auth_secret_secret_key_field = auth_secret_secret_key_field\n        self._tls_secret_name = tls_secret_name\n        self._tls_secret_cert_field = tls_secret_cert_field\n\n        # The secret content may be populated or not, depending on whether this\n        # Connection is being created by the component or a CR config. If not\n        # populated now, they will be lazily populated on client request.\n        self._access_key = access_key\n        self._secret_key = secret_key\n        self._tls_cert = tls_cert\n\n        # Ensure that the TLS arguments are provided in a reasonable way. The\n        # cert may be omitted\n        tls_args = {tls_secret_name, tls_secret_cert_field}\n        assert (\n            tls_args == {None} or None not in tls_args\n        ), \"All TLS arguments must be provided if tls is enabled\"\n        self._tls_enabled = tls_args != {None}\n        assert (\n            self._tls_enabled or self._tls_cert is None\n        ), \"Cannot give a tls cert value when tls is disabled\"\n\n        # Schema is deduced based on the presence of the tls arguments\n        self._schema = \"http\" if tls_secret_name is None else \"https\"\n\n    ## Properties ##############################################################\n\n    @property\n    def schema(self) -&gt; str:\n        \"\"\"The schema (http or https)\"\"\"\n        return self._schema\n\n    @property\n    def hostname(self) -&gt; str:\n        \"\"\"The hostname (without schema)\"\"\"\n        return self._hostname\n\n    @property\n    def port(self) -&gt; int:\n        \"\"\"The numeric port\"\"\"\n        return self._port\n\n    @property\n    def endpoint(self) -&gt; int:\n        \"\"\"The fully constructed endpoint for the COS instance\"\"\"\n        return f\"{self._schema}://{self._hostname}:{self._port}\"\n\n    @property\n    def bucket_name(self) -&gt; int:\n        \"\"\"The numeric bucket_name\"\"\"\n        return self._bucket_name\n\n    @property\n    def auth_secret_name(self) -&gt; str:\n        \"\"\"Secret name containing the access_key and secret_key\"\"\"\n        return self._auth_secret_name\n\n    @property\n    def auth_secret_access_key_field(self) -&gt; str:\n        \"\"\"Field in the auth secret containing the access_key\"\"\"\n        return self._auth_secret_access_key_field\n\n    @property\n    def auth_secret_secret_key_field(self) -&gt; str:\n        \"\"\"Field in the auth secret containing the secret_key\"\"\"\n        return self._auth_secret_secret_key_field\n\n    @property\n    def tls_secret_name(self) -&gt; str:\n        \"\"\"The name of the secret holding the tls certificate (for mounting)\"\"\"\n        return self._tls_secret_name\n\n    @property\n    def tls_secret_cert_field(self) -&gt; str:\n        \"\"\"The field within the tls secret that holds the CA cert\"\"\"\n        return self._tls_secret_cert_field\n\n    @property\n    def tls_enabled(self) -&gt; bool:\n        return self._tls_enabled\n\n    ## Interface ###############################################################\n\n    _DICT_FIELDS = [\n        \"hostname\",\n        \"port\",\n        \"bucket_name\",\n        \"auth_secret_name\",\n        \"auth_secret_access_key_field\",\n        \"auth_secret_secret_key_field\",\n        \"tls_secret_name\",\n        \"tls_secret_cert_field\",\n    ]\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Return the dict representation of the object for the CR\"\"\"\n        return {field: getattr(self, f\"_{field}\") for field in self._DICT_FIELDS}\n\n    @classmethod\n    def from_dict(cls, session: Session, config_dict: dict) -&gt; \"CosConnection\":\n        kwargs = {\"session\": session}\n        config_dict = common.camelcase_to_snake_case(config_dict)\n\n        uri_secret = config_dict.get(\"uri_secret\")\n        uri_hostname_field = config_dict.get(\"uri_secret_hostname_field\")\n        uri_port_field = config_dict.get(\"uri_secret_port_field\")\n        uri_bucketname_field = config_dict.get(\"uri_secret_bucketname_field\")\n\n        # First pull provided hostname/port secret if available and fill in\n        # hostname/port fields into config_dict\n        if (\n            uri_secret\n            and uri_hostname_field\n            and uri_port_field\n            and uri_bucketname_field\n        ):\n            # If we have provided host/port credentials, we need to extract them\n            # and place these values in our config dict\n            success, secret_content = session.get_object_current_state(\n                \"Secret\", uri_secret\n            )\n            assert_cluster(success, f\"Fetching connection secret [{uri_secret}] failed\")\n            assert \"data\" in secret_content, \"Got a secret without 'data'?\"\n            secret_content = secret_content.get(\"data\", {})\n            assert_precondition(\n                secret_content,\n                f\"Missing expected Secret/{uri_secret} holding [hostname] and [port]\",\n            )\n            hostname = common.b64_secret_decode(secret_content.get(uri_hostname_field))\n            port = common.b64_secret_decode(secret_content.get(uri_port_field))\n            bucketname = common.b64_secret_decode(\n                secret_content.get(uri_bucketname_field)\n            )\n            if None in [hostname, port, bucketname]:\n                log.debug2(\n                    \"Failed to find hostname/port/bucketname in uri secret [%s]\",\n                    uri_secret,\n                )\n\n            config_dict[\"hostname\"], config_dict[\"port\"], config_dict[\"bucket_name\"] = (\n                hostname,\n                int(port),\n                bucketname,\n            )\n\n        for field in cls._DICT_FIELDS:\n            if field not in config_dict:\n                raise ValueError(f\"Missing required connection element [{field}]\")\n\n            # Set the kwargs (using None in place of empty strings)\n            kwargs[field] = config_dict[field] or None\n        return cls(**kwargs)\n\n    ## Client Utilities ########################################################\n\n    def get_auth_keys(self) -&gt; Tuple[Optional[str], Optional[str]]:\n        \"\"\"Get the current access_key/secret_key pair from the auth secret if\n        available\n\n        Returns:\n            access_key:  str or None\n                The plain-text access_key (not encoded) if available\n            secret_key:  str or None\n                The plain-text secret_key (not encoded) if available\n        \"\"\"\n        if None in [self._access_key, self._secret_key]:\n            secret_content = self._fetch_secret_data(self._auth_secret_name) or {}\n            log.debug4(\"Auth secret content: %s\", secret_content)\n            log.debug3(\n                \"Looking for [%s/%s]\",\n                self._auth_secret_access_key_field,\n                self._auth_secret_secret_key_field,\n            )\n            access_key = secret_content.get(self._auth_secret_access_key_field)\n            secret_key = secret_content.get(self._auth_secret_secret_key_field)\n            if None in [access_key, secret_key]:\n                log.debug2(\n                    \"Failed to find access_key/secret_key in auth secret [%s]\",\n                    self._auth_secret_name,\n                )\n                return None, None\n            self._access_key = access_key\n            self._secret_key = secret_key\n        return self._access_key, self._secret_key\n\n    def get_tls_cert(self) -&gt; Optional[str]:\n        \"\"\"Get the current TLS certificate for a client connection if TLS is\n        enabled\n\n        If TLS is enabled, but the cert is not found, this function will raise\n        an AssertionError\n\n        Returns:\n            tls_cert: str or None\n                PEM encoded cert string (not base64-encoded) if found, otherwise\n                None\n        \"\"\"\n        if self._tls_enabled:\n            if self._tls_cert is None:\n                secret_data = self._fetch_secret_data(self._tls_secret_name)\n                if secret_data is not None:\n                    self._tls_cert = secret_data.get(self._tls_secret_cert_field)\n            return self._tls_cert\n\n        return None\n\n    def get_connection_string(self) -&gt; str:\n        \"\"\"Get the formatted s3 connection string to connect to the given bucket\n        in the instance\n\n        Returns:\n            connection_string:  str\n                The formatted connection string\n        \"\"\"\n        access_key, secret_key = self.get_auth_keys()\n        assert_precondition(\n            None not in [access_key, secret_key],\n            \"No auth keys available for COS connection string\",\n        )\n        return (\n            \"s3,endpoint={}://{}:{},accesskey={},secretkey={},bucketsuffix={}\".format(\n                self._schema,\n                self._hostname,\n                self._port,\n                access_key,\n                secret_key,\n                self._bucket_name,\n            )\n        )\n</code></pre> <code>auth_secret_access_key_field</code> <code>property</code> <p>Field in the auth secret containing the access_key</p> <code>auth_secret_name</code> <code>property</code> <p>Secret name containing the access_key and secret_key</p> <code>auth_secret_secret_key_field</code> <code>property</code> <p>Field in the auth secret containing the secret_key</p> <code>bucket_name</code> <code>property</code> <p>The numeric bucket_name</p> <code>endpoint</code> <code>property</code> <p>The fully constructed endpoint for the COS instance</p> <code>hostname</code> <code>property</code> <p>The hostname (without schema)</p> <code>port</code> <code>property</code> <p>The numeric port</p> <code>schema</code> <code>property</code> <p>The schema (http or https)</p> <code>tls_secret_cert_field</code> <code>property</code> <p>The field within the tls secret that holds the CA cert</p> <code>tls_secret_name</code> <code>property</code> <p>The name of the secret holding the tls certificate (for mounting)</p> <code>get_auth_keys()</code> <p>Get the current access_key/secret_key pair from the auth secret if available</p> <p>Returns:</p> Name Type Description <code>access_key</code> <code>Optional[str]</code> <p>str or None The plain-text access_key (not encoded) if available</p> <code>secret_key</code> <code>Optional[str]</code> <p>str or None The plain-text secret_key (not encoded) if available</p> Source code in <code>oper8/x/datastores/cos/connection.py</code> <pre><code>def get_auth_keys(self) -&gt; Tuple[Optional[str], Optional[str]]:\n    \"\"\"Get the current access_key/secret_key pair from the auth secret if\n    available\n\n    Returns:\n        access_key:  str or None\n            The plain-text access_key (not encoded) if available\n        secret_key:  str or None\n            The plain-text secret_key (not encoded) if available\n    \"\"\"\n    if None in [self._access_key, self._secret_key]:\n        secret_content = self._fetch_secret_data(self._auth_secret_name) or {}\n        log.debug4(\"Auth secret content: %s\", secret_content)\n        log.debug3(\n            \"Looking for [%s/%s]\",\n            self._auth_secret_access_key_field,\n            self._auth_secret_secret_key_field,\n        )\n        access_key = secret_content.get(self._auth_secret_access_key_field)\n        secret_key = secret_content.get(self._auth_secret_secret_key_field)\n        if None in [access_key, secret_key]:\n            log.debug2(\n                \"Failed to find access_key/secret_key in auth secret [%s]\",\n                self._auth_secret_name,\n            )\n            return None, None\n        self._access_key = access_key\n        self._secret_key = secret_key\n    return self._access_key, self._secret_key\n</code></pre> <code>get_connection_string()</code> <p>Get the formatted s3 connection string to connect to the given bucket in the instance</p> <p>Returns:</p> Name Type Description <code>connection_string</code> <code>str</code> <p>str The formatted connection string</p> Source code in <code>oper8/x/datastores/cos/connection.py</code> <pre><code>def get_connection_string(self) -&gt; str:\n    \"\"\"Get the formatted s3 connection string to connect to the given bucket\n    in the instance\n\n    Returns:\n        connection_string:  str\n            The formatted connection string\n    \"\"\"\n    access_key, secret_key = self.get_auth_keys()\n    assert_precondition(\n        None not in [access_key, secret_key],\n        \"No auth keys available for COS connection string\",\n    )\n    return (\n        \"s3,endpoint={}://{}:{},accesskey={},secretkey={},bucketsuffix={}\".format(\n            self._schema,\n            self._hostname,\n            self._port,\n            access_key,\n            secret_key,\n            self._bucket_name,\n        )\n    )\n</code></pre> <code>get_tls_cert()</code> <p>Get the current TLS certificate for a client connection if TLS is enabled</p> <p>If TLS is enabled, but the cert is not found, this function will raise an AssertionError</p> <p>Returns:</p> Name Type Description <code>tls_cert</code> <code>Optional[str]</code> <p>str or None PEM encoded cert string (not base64-encoded) if found, otherwise None</p> Source code in <code>oper8/x/datastores/cos/connection.py</code> <pre><code>def get_tls_cert(self) -&gt; Optional[str]:\n    \"\"\"Get the current TLS certificate for a client connection if TLS is\n    enabled\n\n    If TLS is enabled, but the cert is not found, this function will raise\n    an AssertionError\n\n    Returns:\n        tls_cert: str or None\n            PEM encoded cert string (not base64-encoded) if found, otherwise\n            None\n    \"\"\"\n    if self._tls_enabled:\n        if self._tls_cert is None:\n            secret_data = self._fetch_secret_data(self._tls_secret_name)\n            if secret_data is not None:\n                self._tls_cert = secret_data.get(self._tls_secret_cert_field)\n        return self._tls_cert\n\n    return None\n</code></pre> <code>to_dict()</code> <p>Return the dict representation of the object for the CR</p> Source code in <code>oper8/x/datastores/cos/connection.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Return the dict representation of the object for the CR\"\"\"\n    return {field: getattr(self, f\"_{field}\") for field in self._DICT_FIELDS}\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.cos.factory","title":"<code>factory</code>","text":"<p>COS instance factory</p>"},{"location":"API%20References/#oper8.x.datastores.cos.factory.CosFactory","title":"<code>CosFactory</code>","text":"<p>               Bases: <code>DatastoreSingletonFactoryBase</code></p> <p>The common factory that will manage instances of COS for each deploy</p> Source code in <code>oper8/x/datastores/cos/factory.py</code> <pre><code>class CosFactory(DatastoreSingletonFactoryBase):\n    \"\"\"The common factory that will manage instances of COS for each deploy\"\"\"\n\n    DATASTORE_TYPE = \"cos\"\n    CONNECTION_TYPE = CosConnection\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.cos.interfaces","title":"<code>interfaces</code>","text":"<p>Base class interface for a Cloud Object Store (cos) component</p>"},{"location":"API%20References/#oper8.x.datastores.cos.interfaces.ICosComponentBase","title":"<code>ICosComponentBase</code>","text":"<p>               Bases: <code>Datastore</code></p> <p>A COS chart provides access to a single running S3-compatible object store instance</p> Source code in <code>oper8/x/datastores/cos/interfaces.py</code> <pre><code>@component(COMPONENT_NAME)\nclass ICosComponentBase(Datastore):\n    \"\"\"A COS chart provides access to a single running S3-compatible object\n    store instance\n    \"\"\"\n\n    ## Parent Interface ########################################################\n\n    def get_connection(self) -&gt; CosConnection:\n        \"\"\"Get the connection object for this instance\"\"\"\n        return CosConnection(\n            session=self.session,\n            hostname=self._get_hostname(),\n            port=self._get_port(),\n            bucket_name=self._get_bucket_name(),\n            auth_secret_name=self._get_auth_secret_name(),\n            auth_secret_access_key_field=self._get_auth_secret_access_key_field(),\n            auth_secret_secret_key_field=self._get_auth_secret_secret_key_field(),\n            tls_secret_name=self._get_tls_secret_name(),\n            tls_secret_cert_field=self._get_tls_secret_cert_field(),\n            access_key=self._get_access_key(),\n            secret_key=self._get_secret_key(),\n            tls_cert=self._get_tls_cert(),\n        )\n\n    ## Abstract Interface ######################################################\n    #\n    # This is the interface that needs to be implemented by a child in order to\n    # provide the common information that a client will use.\n    ##\n\n    ##################\n    ## General Info ##\n    ##################\n\n    @abstractmethod\n    def _get_hostname(self) -&gt; str:\n        \"\"\"Gets the hotsname for the connection. Can be IP address as well.\n\n        Returns:\n            hostname:  str\n                The hostname (without schema) for the service\n        \"\"\"\n\n    @abstractmethod\n    def _get_port(self) -&gt; int:\n        \"\"\"Gets the port where the service is listening\n\n        Returns:\n            port:  int\n                The port where the service is listening\n        \"\"\"\n\n    @abstractmethod\n    def _get_bucket_name(self) -&gt; str:\n        \"\"\"Gets the bucket name for the connection\n\n        Returns:\n            bucket_name:  str\n                The default bucket name for this instance\n        \"\"\"\n\n    ###############\n    ## Auth Info ##\n    ###############\n\n    @abstractmethod\n    def _get_auth_secret_name(self) -&gt; str:\n        \"\"\"Get the Auth secret name with any scoping applied\n\n        Returns:\n            auth_secret_name:  str\n                The name of the secret containing the auth secret\n        \"\"\"\n\n    @abstractmethod\n    def _get_auth_secret_access_key_field(self) -&gt; str:\n        \"\"\"Get the field form within the auth secret that contains the\n        access_key\n\n        Returns:\n            access_key_field:  str\n                The field within the auth secret that contains the access_key\n        \"\"\"\n\n    @abstractmethod\n    def _get_auth_secret_secret_key_field(self) -&gt; str:\n        \"\"\"Get the field form within the auth secret that contains the\n        secret_key\n\n        Returns:\n            secret_key_field:  str\n                The field within the auth secret that contains the secret_key\n        \"\"\"\n\n    @abstractmethod\n    def _get_access_key(self) -&gt; Optional[str]:\n        \"\"\"Get the un-encoded content of the access_key if available in-memory.\n        Components which proxy an external secret don't need to fetch this\n        content from the cluster.\n\n        Returns:\n            access_key:  Optional[str]\n                The content of the access_key if known\n        \"\"\"\n\n    @abstractmethod\n    def _get_secret_key(self) -&gt; Optional[str]:\n        \"\"\"Get the un-encoded content of the secret_key if available in-memory.\n        Components which proxy an external secret don't need to fetch this\n        content from the cluster.\n\n        Returns:\n            secret_key:  Optional[str]\n                The content of the secret_key if known\n        \"\"\"\n\n    ##############\n    ## TLS Info ##\n    ##############\n\n    @abstractmethod\n    def _get_tls_secret_name(self) -&gt; Optional[str]:\n        \"\"\"Get the TLS secret name with any scoping applied if tls is enabled\n\n        Returns:\n            tls_secret_name:  Optional[str]\n                If tls is enabled, returns the name of the secret, otherwise\n                None\n        \"\"\"\n\n    @abstractmethod\n    def _get_tls_secret_cert_field(self) -&gt; Optional[str]:\n        \"\"\"Get the field from within the tls secret that contains the CA\n        certificate a client would need to use to connect\n\n        Returns:\n            cert_field:  Optional[str]\n                The field within the tls secret where the CA certificate lives\n        \"\"\"\n\n    @abstractmethod\n    def _get_tls_cert(self) -&gt; Optional[str]:\n        \"\"\"Get the un-encoded content of the TLS cert if TLS is enabled and\n        available in-memory. Components which proxy an external secret don't\n        need to fetch this content from the cluster.\n\n        Returns:\n            cert_content:  Optional[str]\n                The content of the cert if tls is enabled\n        \"\"\"\n</code></pre> <code>get_connection()</code> <p>Get the connection object for this instance</p> Source code in <code>oper8/x/datastores/cos/interfaces.py</code> <pre><code>def get_connection(self) -&gt; CosConnection:\n    \"\"\"Get the connection object for this instance\"\"\"\n    return CosConnection(\n        session=self.session,\n        hostname=self._get_hostname(),\n        port=self._get_port(),\n        bucket_name=self._get_bucket_name(),\n        auth_secret_name=self._get_auth_secret_name(),\n        auth_secret_access_key_field=self._get_auth_secret_access_key_field(),\n        auth_secret_secret_key_field=self._get_auth_secret_secret_key_field(),\n        tls_secret_name=self._get_tls_secret_name(),\n        tls_secret_cert_field=self._get_tls_secret_cert_field(),\n        access_key=self._get_access_key(),\n        secret_key=self._get_secret_key(),\n        tls_cert=self._get_tls_cert(),\n    )\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.factory_base","title":"<code>factory_base</code>","text":"<p>The DatastoreSingletonFactoryBase class defines the common functionality that all datastore type factories will use. It implements common logic for constructing named singleton instances of a given datastore type.</p>"},{"location":"API%20References/#oper8.x.datastores.factory_base.DatastoreSingletonFactoryBase","title":"<code>DatastoreSingletonFactoryBase</code>","text":"<p>The DatastoreSingletonFactoryBase manages instances of all datastore types as singletons on a per-deployment basis. It provides functionality for derived classes to define a specific DATASTORE_TYPE (e.g. redis) and register implementations of that type.</p> <p>The instances of each type are held as singletons scoped to the individual deployment (session.deploy_id). This is done to support multiple calls to fetch a named instance within a given deployment without reconstructing, but to allow configuration to change between deploys.</p> Source code in <code>oper8/x/datastores/factory_base.py</code> <pre><code>class DatastoreSingletonFactoryBase:\n    \"\"\"The DatastoreSingletonFactoryBase manages instances of all datastore\n    types as singletons on a per-deployment basis. It provides functionality for\n    derived classes to define a specific DATASTORE_TYPE (e.g. redis) and\n    register implementations of that type.\n\n    The instances of each type are held as singletons scoped to the individual\n    deployment (session.deploy_id). This is done to support multiple calls to\n    fetch a named instance within a given deployment without reconstructing, but\n    to allow configuration to change between deploys.\n    \"\"\"\n\n    ## Private Members #########################################################\n\n    # Singleton dict of constructors for each implementation type\n    _type_constructors = {}\n\n    # Singleton dict of named components\n    _components = {}\n\n    # Singleton dict of named connections\n    _connections = {}\n\n    # Class attribute that all individual factory types must have.\n    # NOTE: This will be used as the key in the CR's datastores section\n    _DATASTORE_TYPE_ATTRIBUTE_NAME = \"DATASTORE_TYPE\"\n\n    # Class attribute that must be defined on an implementation to define the\n    # common connection type\n    _CONNECTION_TYPE_ATTRIBUTE = \"CONNECTION_TYPE\"\n\n    ## Public interface ########################################################\n\n    @classproperty\n    def datastore_type(cls):\n        return getattr(cls, cls._DATASTORE_TYPE_ATTRIBUTE_NAME)\n\n    @classproperty\n    def connection_type(cls):\n        return getattr(cls, cls._CONNECTION_TYPE_ATTRIBUTE)\n\n    @classmethod\n    def get_component(\n        cls,\n        session: Session,\n        name: Optional[str] = None,\n        disabled: bool = False,\n        config_overrides: Optional[dict] = None,\n    ) -&gt; Optional[Component]:\n        \"\"\"Construct an instance of the datastore type's component\n\n        Args:\n            session:  Session\n                The session for the current deployment\n            name:  Optional[str]\n                The name of the singleton instance to get. If not provided, a\n                top-level instance is used (e.g. datastores.postgres.type)\n            disabled:  bool\n                Whether or not the component is disabled in this deployment\n            config_overrides:  Optional[dict]\n                Optional runtime config values. These will overwrite any values\n                pulled from the session.config\n\n        Returns:\n            instance:  Optional[Component]\n                The constructed component if one is needed\n        \"\"\"\n        return cls._get_component(\n            session=session,\n            name=name,\n            disabled=disabled,\n            config_overrides=config_overrides,\n        )\n\n    @classmethod\n    def get_connection(\n        cls,\n        session: Session,\n        name: Optional[str] = None,\n        allow_from_component: bool = True,\n    ) -&gt; DatastoreConnectionBase:\n        \"\"\"Get the connection details for a named instance of the datastore type\n\n        If not pre-constructed by the creation of the Component, connection\n        details are pulled from the CR directly\n        (spec.datastores.&lt;datastore_type&gt;.[&lt;name&gt;].connection)\n\n        Args:\n            session:  Session\n                The session for the current deployment\n            name:  Optional[str]\n                The name of the singleton instance to get. If not provided, a\n                top-level instance is used\n            allow_from_component:  bool\n                If True, use connection info from the component\n\n        Returns:\n            connection:  DatastoreConnectionBase\n                The connection for this instance\n        \"\"\"\n        return cls._get_connection(session, name, allow_from_component)\n\n    @classmethod\n    def register_type(cls, type_class: Datastore):\n        \"\"\"Register a new type constructor\n\n        Args:\n            type_class:  Datastore\n                The class that will be constructed with the config for\n        \"\"\"\n        cls._validate_class_attributes()\n\n        assert issubclass(\n            type_class, Datastore\n        ), \"Datastore types use component_class=Datastore\"\n        datastore_type_classes = cls._type_constructors.setdefault(\n            cls.datastore_type, {}\n        )\n        if type_class.TYPE_LABEL in datastore_type_classes:\n            log.warning(\"Got duplicate registration for %s\", type_class.TYPE_LABEL)\n        datastore_type_classes[type_class.TYPE_LABEL] = type_class\n\n    ## Implementation Details ##################################################\n\n    @classmethod\n    def _validate_class_attributes(cls):\n        \"\"\"Since this class is always used statically, this helper makes sure\n        the expected class attributes\n        \"\"\"\n        assert isinstance(\n            getattr(\n                cls, DatastoreSingletonFactoryBase._DATASTORE_TYPE_ATTRIBUTE_NAME, None\n            ),\n            str,\n        ), \"Incorrectly configured datastore [{}]. Must define str [{}]\".format(\n            cls, DatastoreSingletonFactoryBase._DATASTORE_TYPE_ATTRIBUTE_NAME\n        )\n        connection_type = getattr(\n            cls, DatastoreSingletonFactoryBase._CONNECTION_TYPE_ATTRIBUTE, None\n        )\n        assert isinstance(connection_type, type) and issubclass(\n            connection_type, DatastoreConnectionBase\n        ), (\n            f\"Incorrectly configured datastore [{cls}]. Must define \"\n            f\"[{DatastoreSingletonFactoryBase._CONNECTION_TYPE_ATTRIBUTE}] as \"\n            \"a DatastoreConnectionBase type\"\n        )\n\n    @classmethod\n    def _get_component(\n        cls,\n        session: Session,\n        name: Optional[str] = None,\n        disabled: bool = False,\n        config_overrides: Optional[dict] = None,\n        allow_instantiation: bool = True,\n    ) -&gt; Optional[Component]:\n        \"\"\"Implementation detail for get_component which can be called by\n        get_connection and disallow lazy creation of the singleton.\n        \"\"\"\n        cls._validate_class_attributes()\n\n        # First, check to see if there's a connection already available based on\n        # conneciton details in the CR. If so, we won't create the component\n        conn = cls._get_connection(session, name, allow_from_component=False)\n        if conn is not None:\n            log.debug(\n                \"Found connection for [%s] in the CR. Not constructing the component.\",\n                cls.datastore_type,\n            )\n            return None\n\n        # Get the pre-existing instances for this datastore type (keyed by the\n        # datastore subclass)\n        datastore_components = cls._components.setdefault(cls.datastore_type, {})\n\n        # Get the app config section for this instance by name\n        instance_config = merge_configs(\n            session.config.get(cls.datastore_type, {}),\n            config_overrides or {},\n        )\n        log.debug4(\"Full config: %s\", instance_config)\n        if name is not None:\n            instance_config = instance_config.get(name)\n        assert (\n            instance_config is not None\n        ), f\"Cannot construct unknown [{cls.datastore_type}] instance: {name}\"\n        assert (\n            \"type\" in instance_config\n        ), f\"Missing required [type] key for [{cls.datastore_type}/{name}]\"\n\n        # Fetch the current instance/deploy_id\n        instance, deploy_id = datastore_components.get(name, (None, None))\n\n        # If the deploy_id has changed, remove any current instance\n        if deploy_id != session.id:\n            instance = None\n            datastore_components.pop(name, None)\n\n        # If there is not a valid instance and it's allowed, construct it\n        if not instance and allow_instantiation:\n            log.debug2(\n                \"Constructing [%s]/%s for the first time for deploy [%s]\",\n                cls.datastore_type,\n                name,\n                session.id,\n            )\n            type_key = instance_config.type\n\n            # Fetch the class for this type of the datastore\n            datastore_type_classes = cls._type_constructors.get(cls.datastore_type, {})\n            type_class = datastore_type_classes.get(type_key)\n            assert (\n                type_class is not None\n            ), f\"Cannot construct unsupported type [{type_key}]\"\n\n            # If there is a name provided, create a wrapper component with the\n            # given name\n            if name is not None:\n                instance_class_name = f\"{type_class.name}-{name}\"\n                log.debug2(\"Wrapping %s with instance name override\", type_class)\n\n                class InstanceClass(type_class):\n                    \"\"\"Wrapper for {}/{} with instance naming\"\"\".format(\n                        cls.datastore_type, type_key\n                    )\n\n                    name = instance_class_name\n\n            else:\n                log.debug2(\"No instance name wrapping needed for %s\", type_class)\n                InstanceClass = type_class\n            log.debug(\"Constructing %s\", type_key)\n            instance = InstanceClass(\n                session=session,\n                config=instance_config,\n                instance_name=name,\n                disabled=disabled,\n            )\n            datastore_components[name] = (instance, session.id)\n\n        # Return the singleton\n        return instance\n\n    @classmethod\n    def _get_connection(\n        cls,\n        session: Session,\n        name: Optional[str] = None,\n        allow_from_component: bool = True,\n    ) -&gt; Optional[DatastoreConnectionBase]:\n        \"\"\"Implementation for get_connection that can be used by _get_component\n        to fetch connections from the CR\n        \"\"\"\n        cls._validate_class_attributes()\n\n        # Get the pre-existing instances for this datastore type (keyed by the\n        # datastore subclass)\n        connection, deploy_id = cls._connections.get(cls.datastore_type, {}).get(\n            name, (None, None)\n        )\n\n        # If there is no connection for this deploy already, deserialize it from\n        # the CR\n        if connection is None or deploy_id != session.id:\n            log.debug(\"Constructing %s connection from config\", cls.datastore_type)\n\n            # Get the CR config for this datastore type\n            ds_config = session.spec.get(constants.SPEC_DATASTORES, {}).get(\n                cls.datastore_type, {}\n            )\n            if name is not None:\n                ds_config = ds_config.get(name, {})\n            ds_config = ds_config.get(constants.SPEC_DATASTORE_CONNECTION)\n            log.debug3(\n                \"%s/%s connection config: %s\", cls.datastore_type, name, ds_config\n            )\n\n            if ds_config is not None:\n                # Deserialize connection from sub-cr connection specification\n                connection = cls.connection_type.from_dict(session, ds_config)\n                cls._connections.setdefault(cls.datastore_type, {})[name] = (\n                    connection,\n                    session.id,\n                )\n            elif allow_from_component:\n                # Add the connection information for this instance\n                instance = cls._get_component(session, name, allow_instantiation=False)\n                assert (\n                    instance is not None\n                ), f\"No instance or config available for {cls.datastore_type}\"\n                connection = instance.get_connection()\n                assert isinstance(\n                    connection, cls.connection_type\n                ), f\"Got incorrect [{cls.datastore_type}] connection type: {type(connection)}\"\n                cls._connections.setdefault(cls.datastore_type, {})[name] = (\n                    connection,\n                    session.id,\n                )\n            else:\n                log.debug2(\n                    \"No connection details for %s found in CR\", cls.datastore_type\n                )\n                return None\n\n        # Return the connection singleton\n        return cls._connections[cls.datastore_type][name][0]\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.factory_base.DatastoreSingletonFactoryBase.get_component","title":"<code>get_component(session, name=None, disabled=False, config_overrides=None)</code>  <code>classmethod</code>","text":"<p>Construct an instance of the datastore type's component</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The session for the current deployment</p> required <code>name</code> <code>Optional[str]</code> <p>Optional[str] The name of the singleton instance to get. If not provided, a top-level instance is used (e.g. datastores.postgres.type)</p> <code>None</code> <code>disabled</code> <code>bool</code> <p>bool Whether or not the component is disabled in this deployment</p> <code>False</code> <code>config_overrides</code> <code>Optional[dict]</code> <p>Optional[dict] Optional runtime config values. These will overwrite any values pulled from the session.config</p> <code>None</code> <p>Returns:</p> Name Type Description <code>instance</code> <code>Optional[Component]</code> <p>Optional[Component] The constructed component if one is needed</p> Source code in <code>oper8/x/datastores/factory_base.py</code> <pre><code>@classmethod\ndef get_component(\n    cls,\n    session: Session,\n    name: Optional[str] = None,\n    disabled: bool = False,\n    config_overrides: Optional[dict] = None,\n) -&gt; Optional[Component]:\n    \"\"\"Construct an instance of the datastore type's component\n\n    Args:\n        session:  Session\n            The session for the current deployment\n        name:  Optional[str]\n            The name of the singleton instance to get. If not provided, a\n            top-level instance is used (e.g. datastores.postgres.type)\n        disabled:  bool\n            Whether or not the component is disabled in this deployment\n        config_overrides:  Optional[dict]\n            Optional runtime config values. These will overwrite any values\n            pulled from the session.config\n\n    Returns:\n        instance:  Optional[Component]\n            The constructed component if one is needed\n    \"\"\"\n    return cls._get_component(\n        session=session,\n        name=name,\n        disabled=disabled,\n        config_overrides=config_overrides,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.factory_base.DatastoreSingletonFactoryBase.get_connection","title":"<code>get_connection(session, name=None, allow_from_component=True)</code>  <code>classmethod</code>","text":"<p>Get the connection details for a named instance of the datastore type</p> <p>If not pre-constructed by the creation of the Component, connection details are pulled from the CR directly (spec.datastores..[].connection) <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The session for the current deployment</p> required <code>name</code> <code>Optional[str]</code> <p>Optional[str] The name of the singleton instance to get. If not provided, a top-level instance is used</p> <code>None</code> <code>allow_from_component</code> <code>bool</code> <p>bool If True, use connection info from the component</p> <code>True</code> <p>Returns:</p> Name Type Description <code>connection</code> <code>DatastoreConnectionBase</code> <p>DatastoreConnectionBase The connection for this instance</p> Source code in <code>oper8/x/datastores/factory_base.py</code> <pre><code>@classmethod\ndef get_connection(\n    cls,\n    session: Session,\n    name: Optional[str] = None,\n    allow_from_component: bool = True,\n) -&gt; DatastoreConnectionBase:\n    \"\"\"Get the connection details for a named instance of the datastore type\n\n    If not pre-constructed by the creation of the Component, connection\n    details are pulled from the CR directly\n    (spec.datastores.&lt;datastore_type&gt;.[&lt;name&gt;].connection)\n\n    Args:\n        session:  Session\n            The session for the current deployment\n        name:  Optional[str]\n            The name of the singleton instance to get. If not provided, a\n            top-level instance is used\n        allow_from_component:  bool\n            If True, use connection info from the component\n\n    Returns:\n        connection:  DatastoreConnectionBase\n            The connection for this instance\n    \"\"\"\n    return cls._get_connection(session, name, allow_from_component)\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.factory_base.DatastoreSingletonFactoryBase.register_type","title":"<code>register_type(type_class)</code>  <code>classmethod</code>","text":"<p>Register a new type constructor</p> <p>Parameters:</p> Name Type Description Default <code>type_class</code> <code>Datastore</code> <p>Datastore The class that will be constructed with the config for</p> required Source code in <code>oper8/x/datastores/factory_base.py</code> <pre><code>@classmethod\ndef register_type(cls, type_class: Datastore):\n    \"\"\"Register a new type constructor\n\n    Args:\n        type_class:  Datastore\n            The class that will be constructed with the config for\n    \"\"\"\n    cls._validate_class_attributes()\n\n    assert issubclass(\n        type_class, Datastore\n    ), \"Datastore types use component_class=Datastore\"\n    datastore_type_classes = cls._type_constructors.setdefault(\n        cls.datastore_type, {}\n    )\n    if type_class.TYPE_LABEL in datastore_type_classes:\n        log.warning(\"Got duplicate registration for %s\", type_class.TYPE_LABEL)\n    datastore_type_classes[type_class.TYPE_LABEL] = type_class\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.factory_base.classproperty","title":"<code>classproperty</code>","text":"<p>@classmethod+@property CITE: https://stackoverflow.com/a/22729414</p> Source code in <code>oper8/x/datastores/factory_base.py</code> <pre><code>class classproperty:\n    \"\"\"@classmethod+@property\n    CITE: https://stackoverflow.com/a/22729414\n    \"\"\"\n\n    def __init__(self, func):\n        self.func = classmethod(func)\n\n    def __get__(self, *args):\n        return self.func.__get__(*args)()\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.interfaces","title":"<code>interfaces</code>","text":"<p>Base class for all Datastore component implementations</p>"},{"location":"API%20References/#oper8.x.datastores.interfaces.Datastore","title":"<code>Datastore</code>","text":"<p>               Bases: <code>Oper8xComponent</code></p> <p>The Datastore baseclass defines the interface that any datastore must conform to. It is a oper8 Component and should be constructed via a per-type factory.</p> Source code in <code>oper8/x/datastores/interfaces.py</code> <pre><code>class Datastore(Oper8xComponent):\n    \"\"\"\n    The Datastore baseclass defines the interface that any datastore must\n    conform to. It is a oper8 Component and should be constructed via a per-type\n    factory.\n    \"\"\"\n\n    _TYPE_LABEL_ATTRIBUTE = \"TYPE_LABEL\"\n\n    def __init__(\n        self,\n        session: Session,\n        config: aconfig.Config,\n        instance_name: Optional[str] = None,\n        disabled: bool = False,\n    ):\n        \"\"\"This passthrough constructor enforces that all datastores have a\n        class attribute TYPE_LABEL (str)\n        \"\"\"\n        type_label = getattr(self, self._TYPE_LABEL_ATTRIBUTE, None)\n        assert isinstance(\n            type_label, str\n        ), f\"All datastores types must define {self._TYPE_LABEL_ATTRIBUTE} as a str\"\n        super().__init__(session=session, disabled=disabled)\n        self._config = config\n        self.instance_name = instance_name\n\n    @property\n    def config(self) -&gt; aconfig.Config:\n        \"\"\"The config for this instance of the datastore\"\"\"\n        return self._config\n\n    @abc.abstractmethod\n    def get_connection(self) -&gt; DatastoreConnectionBase:\n        \"\"\"Get the connection object for this datastore instance. Each datastore\n        type must manage a common abstraction for a connection which clients\n        will use to connect to the datastore.\n        \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.interfaces.Datastore.config","title":"<code>config</code>  <code>property</code>","text":"<p>The config for this instance of the datastore</p>"},{"location":"API%20References/#oper8.x.datastores.interfaces.Datastore.__init__","title":"<code>__init__(session, config, instance_name=None, disabled=False)</code>","text":"<p>This passthrough constructor enforces that all datastores have a class attribute TYPE_LABEL (str)</p> Source code in <code>oper8/x/datastores/interfaces.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    config: aconfig.Config,\n    instance_name: Optional[str] = None,\n    disabled: bool = False,\n):\n    \"\"\"This passthrough constructor enforces that all datastores have a\n    class attribute TYPE_LABEL (str)\n    \"\"\"\n    type_label = getattr(self, self._TYPE_LABEL_ATTRIBUTE, None)\n    assert isinstance(\n        type_label, str\n    ), f\"All datastores types must define {self._TYPE_LABEL_ATTRIBUTE} as a str\"\n    super().__init__(session=session, disabled=disabled)\n    self._config = config\n    self.instance_name = instance_name\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.interfaces.Datastore.get_connection","title":"<code>get_connection()</code>  <code>abstractmethod</code>","text":"<p>Get the connection object for this datastore instance. Each datastore type must manage a common abstraction for a connection which clients will use to connect to the datastore.</p> Source code in <code>oper8/x/datastores/interfaces.py</code> <pre><code>@abc.abstractmethod\ndef get_connection(self) -&gt; DatastoreConnectionBase:\n    \"\"\"Get the connection object for this datastore instance. Each datastore\n    type must manage a common abstraction for a connection which clients\n    will use to connect to the datastore.\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.postgres","title":"<code>postgres</code>","text":"<p>Common postgres module exposed imports</p>"},{"location":"API%20References/#oper8.x.datastores.postgres.connection","title":"<code>connection</code>","text":"<p>The common Connection type for a postgres instance</p>"},{"location":"API%20References/#oper8.x.datastores.postgres.connection.PostgresConnection","title":"<code>PostgresConnection</code>","text":"<p>               Bases: <code>DatastoreConnectionBase</code></p> <p>A connection for postgres defines the client operations and utilities needed to configure a microservice to interact with a single postgres instance. The key pieces of information are:</p> <ul> <li> <p>General config:</p> <ul> <li>hostname: The hostname where the database can be reached</li> <li>port: The port the database service is listening on</li> </ul> </li> <li> <p>Auth:</p> <ul> <li>auth_secret_name: The in-cluster name for the secret holding the     username and password</li> <li>auth_secret_username_field: The field within the auth secret that     holds the username</li> <li>auth_secret_password_field: The field within the auth secret that     holds the password</li> </ul> </li> <li> <p>TLS:</p> <ul> <li>tls_cert: The content of the TLS cert if tls is enabled</li> <li>tls_secret_name: The in-cluster name for the secret holding the TLS     creds if tls is enabled</li> <li>tls_secret_cert_field: The field within the tls secret that holds the     cert</li> </ul> </li> </ul> Source code in <code>oper8/x/datastores/postgres/connection.py</code> <pre><code>class PostgresConnection(DatastoreConnectionBase):\n    \"\"\"A connection for postgres defines the client operations and utilities\n    needed to configure a microservice to interact with a single postgres\n    instance. The key pieces of information are:\n\n    * General config:\n        * hostname: The hostname where the database can be reached\n        * port: The port the database service is listening on\n\n    * Auth:\n        * auth_secret_name: The in-cluster name for the secret holding the\n            username and password\n        * auth_secret_username_field: The field within the auth secret that\n            holds the username\n        * auth_secret_password_field: The field within the auth secret that\n            holds the password\n\n    * TLS:\n        * tls_cert: The content of the TLS cert if tls is enabled\n        * tls_secret_name: The in-cluster name for the secret holding the TLS\n            creds if tls is enabled\n        * tls_secret_cert_field: The field within the tls secret that holds the\n            cert\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        hostname: str,\n        port: int,\n        auth_secret_name: str,\n        auth_secret_username_field: str,\n        auth_secret_password_field: str,\n        tls_secret_name: Optional[str] = None,\n        tls_secret_cert_field: Optional[str] = None,\n        auth_username: Optional[str] = None,\n        auth_password: Optional[str] = None,\n        tls_cert: Optional[str] = None,\n    ):\n        \"\"\"Construct with all of the crucial information pieces\"\"\"\n        super().__init__(session)\n\n        # Save internal values\n        self._hostname = hostname\n        self._port = port\n        self._auth_secret_name = auth_secret_name\n        self._auth_secret_username_field = auth_secret_username_field\n        self._auth_secret_password_field = auth_secret_password_field\n        self._tls_secret_name = tls_secret_name\n        self._tls_secret_cert_field = tls_secret_cert_field\n\n        # The secret content may be populated or not, depending on whether this\n        # Connection is being created by the component or a CR config. If not\n        # populated now, they will be lazily populated on client request.\n        self._auth_username = auth_username\n        self._auth_password = auth_password\n        self._tls_cert = tls_cert\n\n        # Ensure that the TLS arguments are provided in a reasonable way. The\n        # cert may be omitted\n        tls_args = {tls_secret_name, tls_secret_cert_field}\n        assert (\n            tls_args == {None} or None not in tls_args\n        ), \"All TLS arguments must be provided if tls is enabled\"\n        self._tls_enabled = tls_args != {None}\n        assert (\n            self._tls_enabled or self._tls_cert is None\n        ), \"Cannot give a tls cert value when tls is disabled\"\n\n    ## Properties ##############################################################\n\n    @property\n    def hostname(self) -&gt; str:\n        return self._hostname\n\n    @property\n    def port(self) -&gt; int:\n        return self._port\n\n    @property\n    def auth_secret_name(self) -&gt; str:\n        return self._auth_secret_name\n\n    @property\n    def auth_secret_username_field(self) -&gt; str:\n        return self._auth_secret_username_field\n\n    @property\n    def auth_secret_password_field(self) -&gt; str:\n        return self._auth_secret_password_field\n\n    @property\n    def tls_enabled(self) -&gt; bool:\n        return self._tls_enabled\n\n    @property\n    def tls_secret_name(self) -&gt; str:\n        return self._tls_secret_name\n\n    @property\n    def tls_secret_cert_field(self) -&gt; str:\n        return self._tls_secret_cert_field\n\n    ## Interface ###############################################################\n\n    _DICT_FIELDS = [\n        \"hostname\",\n        \"port\",\n        \"auth_secret_name\",\n        \"auth_secret_username_field\",\n        \"auth_secret_password_field\",\n        \"tls_secret_name\",\n        \"tls_secret_cert_field\",\n    ]\n\n    _PROVIDED_DICT_FIELDS = [\n        \"uri_secret\",\n        \"uri_secret_hostname_field\",\n        \"uri_secret_port_field\",\n    ]\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Return the dict representation of the object for the CR\"\"\"\n        return {field: getattr(self, f\"_{field}\") for field in self._DICT_FIELDS}\n\n    @classmethod\n    def from_dict(cls, session: Session, config_dict: dict) -&gt; \"PostgresConnection\":\n        kwargs = {\"session\": session}\n        config_dict = common.camelcase_to_snake_case(config_dict)\n        uri_secret = config_dict.get(\"uri_secret\", {})\n        uri_hostname_field = config_dict.get(\"uri_secret_hostname_field\", {})\n        uri_port_field = config_dict.get(\"uri_secret_port_field\", {})\n\n        # First pull provided hostname/port secret if available and fill in\n        # hostname/port fields into config_dict\n        if uri_secret and uri_hostname_field and uri_port_field:\n            # If we have provided host/port credentials, we need to extract them\n            # and place these values in our config dict\n            success, secret_content = session.get_object_current_state(\n                \"Secret\", uri_secret\n            )\n            assert_cluster(success, f\"Fetching connection secret [{uri_secret}] failed\")\n            assert_precondition(\n                secret_content,\n                f\"Missing expected Secret/{uri_secret} holding [hostname] and [port]\",\n            )\n            assert \"data\" in secret_content, \"Got a secret without 'data'?\"\n            secret_content = secret_content.get(\"data\", {})\n            hostname_raw = secret_content.get(uri_hostname_field)\n            port_raw = secret_content.get(uri_port_field)\n            assert_config(\n                None not in [hostname_raw, port_raw],\n                f\"Failed to find hostname/port in uri secret [{uri_secret}]\",\n            )\n            hostname = common.b64_secret_decode(hostname_raw)\n            port = common.b64_secret_decode(port_raw)\n\n            config_dict[\"hostname\"], config_dict[\"port\"] = hostname, int(port)\n\n        for field in cls._DICT_FIELDS:\n            if field not in config_dict:\n                raise ValueError(f\"Missing required connection element [{field}]\")\n\n            # Set the kwargs (using None in place of empty strings)\n            kwargs[field] = config_dict[field] or None\n        return cls(**kwargs)\n\n    ## Client Utilities ########################################################\n\n    def get_ssl_mode(self) -&gt; str:\n        \"\"\"Get Postgres SSL mode to operate in\n\n        Returns:\n            ssl_mode: str\n                \"require\" (tls enabled) or \"disable\" (tls disabled)\n        \"\"\"\n        return \"require\" if self.tls_enabled else \"disable\"\n\n    def get_auth_username_password(self) -&gt; Tuple[str, str]:\n        \"\"\"Get the current username/password pair from the auth secret if\n        available\n\n        Returns:\n            username: str or None\n                The plain-text username for the instance or None if not\n                available\n            password: str or None\n                The plain-text password for the instance or None if not\n                available\n        \"\"\"\n        # If not already known, fetch from the cluster\n        if None in [self._auth_username, self._auth_password]:\n            secret_content = self._fetch_secret_data(self._auth_secret_name) or {}\n            username = secret_content.get(self._auth_secret_username_field)\n            password = secret_content.get(self._auth_secret_password_field)\n            if None in [username, password]:\n                log.debug2(\n                    \"Failed to find username/password in auth secret [%s]\",\n                    self._auth_secret_name,\n                )\n                return None, None\n            self._auth_username = username\n            self._auth_password = password\n        return self._auth_username, self._auth_password\n\n    def get_tls_secret_volume_mounts(\n        self,\n        mount_path: str = DEFAULT_TLS_VOLUME_MOUNT_PATH,\n        volume_name: str = DEFAULT_TLS_VOLUME_NAME,\n    ) -&gt; List[dict]:\n        \"\"\"Get the list of volumeMounts entries needed to support TLS for a\n        client. If TLS is not enabled, this will be an empty list.\n\n        Args:\n            mount_path: str\n                A path where the tls entries should be mounted\n            volume_name: str\n                The name of the volume within the pod spec\n\n        Returns:\n            volume_mounts: List[dict]\n                A list of dict entries for the volume mounts which can be used\n                to extend other volume lists\n        \"\"\"\n        if self._tls_enabled:\n            return [dict(name=volume_name, mountPath=mount_path)]\n        return []\n\n    def get_tls_secret_volumes(\n        self,\n        cert_mount_path: Optional[str] = None,\n        volume_name: str = DEFAULT_TLS_VOLUME_NAME,\n    ) -&gt; List[dict]:\n        \"\"\"Get the list of dict entries needed to support TLS for a\n        client. If TLS is not enabled, this will be an empty list.\n\n        Args:\n            cert_mount_path: Optional[str]\n                The name of the file that the ca cert should be mounted to\n            volume_name: str\n                The name of the volume within the pod spec\n\n        Returns:\n            volumes: List[dict]\n                A list of dict Volume entries which can be used to extend other\n                volume lists\n        \"\"\"\n        if self._tls_enabled:\n            cert_mount_path = cert_mount_path or self._tls_secret_cert_field\n            return [\n                dict(\n                    name=volume_name,\n                    secret=dict(\n                        defaultMode=common.mount_mode(440),\n                        secretName=self._tls_secret_name,\n                        items=[\n                            dict(key=self._tls_secret_cert_field, path=cert_mount_path)\n                        ],\n                    ),\n                )\n            ]\n        return []\n\n    def get_tls_cert(self) -&gt; Optional[str]:\n        \"\"\"Get the current TLS certificate for a client connection if TLS is\n        enabled\n\n        If TLS is enabled, but the cert is not found, this function will raise\n        an AssertionError\n\n        Returns:\n            tls_cert: str or None\n                PEM encoded cert string (not base64-encoded) if found, otherwise\n                None\n        \"\"\"\n        if self._tls_enabled:\n            if self._tls_cert is None:\n                secret_data = self._fetch_secret_data(self._tls_secret_name) or {}\n                self._tls_cert = secret_data.get(self._tls_secret_cert_field)\n                assert_precondition(\n                    self._tls_cert is not None, \"Failed to find TLS cert\"\n                )\n            return self._tls_cert\n\n        return None\n</code></pre> <code>__init__(session, hostname, port, auth_secret_name, auth_secret_username_field, auth_secret_password_field, tls_secret_name=None, tls_secret_cert_field=None, auth_username=None, auth_password=None, tls_cert=None)</code> <p>Construct with all of the crucial information pieces</p> Source code in <code>oper8/x/datastores/postgres/connection.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    hostname: str,\n    port: int,\n    auth_secret_name: str,\n    auth_secret_username_field: str,\n    auth_secret_password_field: str,\n    tls_secret_name: Optional[str] = None,\n    tls_secret_cert_field: Optional[str] = None,\n    auth_username: Optional[str] = None,\n    auth_password: Optional[str] = None,\n    tls_cert: Optional[str] = None,\n):\n    \"\"\"Construct with all of the crucial information pieces\"\"\"\n    super().__init__(session)\n\n    # Save internal values\n    self._hostname = hostname\n    self._port = port\n    self._auth_secret_name = auth_secret_name\n    self._auth_secret_username_field = auth_secret_username_field\n    self._auth_secret_password_field = auth_secret_password_field\n    self._tls_secret_name = tls_secret_name\n    self._tls_secret_cert_field = tls_secret_cert_field\n\n    # The secret content may be populated or not, depending on whether this\n    # Connection is being created by the component or a CR config. If not\n    # populated now, they will be lazily populated on client request.\n    self._auth_username = auth_username\n    self._auth_password = auth_password\n    self._tls_cert = tls_cert\n\n    # Ensure that the TLS arguments are provided in a reasonable way. The\n    # cert may be omitted\n    tls_args = {tls_secret_name, tls_secret_cert_field}\n    assert (\n        tls_args == {None} or None not in tls_args\n    ), \"All TLS arguments must be provided if tls is enabled\"\n    self._tls_enabled = tls_args != {None}\n    assert (\n        self._tls_enabled or self._tls_cert is None\n    ), \"Cannot give a tls cert value when tls is disabled\"\n</code></pre> <code>get_auth_username_password()</code> <p>Get the current username/password pair from the auth secret if available</p> <p>Returns:</p> Name Type Description <code>username</code> <code>str</code> <p>str or None The plain-text username for the instance or None if not available</p> <code>password</code> <code>str</code> <p>str or None The plain-text password for the instance or None if not available</p> Source code in <code>oper8/x/datastores/postgres/connection.py</code> <pre><code>def get_auth_username_password(self) -&gt; Tuple[str, str]:\n    \"\"\"Get the current username/password pair from the auth secret if\n    available\n\n    Returns:\n        username: str or None\n            The plain-text username for the instance or None if not\n            available\n        password: str or None\n            The plain-text password for the instance or None if not\n            available\n    \"\"\"\n    # If not already known, fetch from the cluster\n    if None in [self._auth_username, self._auth_password]:\n        secret_content = self._fetch_secret_data(self._auth_secret_name) or {}\n        username = secret_content.get(self._auth_secret_username_field)\n        password = secret_content.get(self._auth_secret_password_field)\n        if None in [username, password]:\n            log.debug2(\n                \"Failed to find username/password in auth secret [%s]\",\n                self._auth_secret_name,\n            )\n            return None, None\n        self._auth_username = username\n        self._auth_password = password\n    return self._auth_username, self._auth_password\n</code></pre> <code>get_ssl_mode()</code> <p>Get Postgres SSL mode to operate in</p> <p>Returns:</p> Name Type Description <code>ssl_mode</code> <code>str</code> <p>str \"require\" (tls enabled) or \"disable\" (tls disabled)</p> Source code in <code>oper8/x/datastores/postgres/connection.py</code> <pre><code>def get_ssl_mode(self) -&gt; str:\n    \"\"\"Get Postgres SSL mode to operate in\n\n    Returns:\n        ssl_mode: str\n            \"require\" (tls enabled) or \"disable\" (tls disabled)\n    \"\"\"\n    return \"require\" if self.tls_enabled else \"disable\"\n</code></pre> <code>get_tls_cert()</code> <p>Get the current TLS certificate for a client connection if TLS is enabled</p> <p>If TLS is enabled, but the cert is not found, this function will raise an AssertionError</p> <p>Returns:</p> Name Type Description <code>tls_cert</code> <code>Optional[str]</code> <p>str or None PEM encoded cert string (not base64-encoded) if found, otherwise None</p> Source code in <code>oper8/x/datastores/postgres/connection.py</code> <pre><code>def get_tls_cert(self) -&gt; Optional[str]:\n    \"\"\"Get the current TLS certificate for a client connection if TLS is\n    enabled\n\n    If TLS is enabled, but the cert is not found, this function will raise\n    an AssertionError\n\n    Returns:\n        tls_cert: str or None\n            PEM encoded cert string (not base64-encoded) if found, otherwise\n            None\n    \"\"\"\n    if self._tls_enabled:\n        if self._tls_cert is None:\n            secret_data = self._fetch_secret_data(self._tls_secret_name) or {}\n            self._tls_cert = secret_data.get(self._tls_secret_cert_field)\n            assert_precondition(\n                self._tls_cert is not None, \"Failed to find TLS cert\"\n            )\n        return self._tls_cert\n\n    return None\n</code></pre> <code>get_tls_secret_volume_mounts(mount_path=DEFAULT_TLS_VOLUME_MOUNT_PATH, volume_name=DEFAULT_TLS_VOLUME_NAME)</code> <p>Get the list of volumeMounts entries needed to support TLS for a client. If TLS is not enabled, this will be an empty list.</p> <p>Parameters:</p> Name Type Description Default <code>mount_path</code> <code>str</code> <p>str A path where the tls entries should be mounted</p> <code>DEFAULT_TLS_VOLUME_MOUNT_PATH</code> <code>volume_name</code> <code>str</code> <p>str The name of the volume within the pod spec</p> <code>DEFAULT_TLS_VOLUME_NAME</code> <p>Returns:</p> Name Type Description <code>volume_mounts</code> <code>List[dict]</code> <p>List[dict] A list of dict entries for the volume mounts which can be used to extend other volume lists</p> Source code in <code>oper8/x/datastores/postgres/connection.py</code> <pre><code>def get_tls_secret_volume_mounts(\n    self,\n    mount_path: str = DEFAULT_TLS_VOLUME_MOUNT_PATH,\n    volume_name: str = DEFAULT_TLS_VOLUME_NAME,\n) -&gt; List[dict]:\n    \"\"\"Get the list of volumeMounts entries needed to support TLS for a\n    client. If TLS is not enabled, this will be an empty list.\n\n    Args:\n        mount_path: str\n            A path where the tls entries should be mounted\n        volume_name: str\n            The name of the volume within the pod spec\n\n    Returns:\n        volume_mounts: List[dict]\n            A list of dict entries for the volume mounts which can be used\n            to extend other volume lists\n    \"\"\"\n    if self._tls_enabled:\n        return [dict(name=volume_name, mountPath=mount_path)]\n    return []\n</code></pre> <code>get_tls_secret_volumes(cert_mount_path=None, volume_name=DEFAULT_TLS_VOLUME_NAME)</code> <p>Get the list of dict entries needed to support TLS for a client. If TLS is not enabled, this will be an empty list.</p> <p>Parameters:</p> Name Type Description Default <code>cert_mount_path</code> <code>Optional[str]</code> <p>Optional[str] The name of the file that the ca cert should be mounted to</p> <code>None</code> <code>volume_name</code> <code>str</code> <p>str The name of the volume within the pod spec</p> <code>DEFAULT_TLS_VOLUME_NAME</code> <p>Returns:</p> Name Type Description <code>volumes</code> <code>List[dict]</code> <p>List[dict] A list of dict Volume entries which can be used to extend other volume lists</p> Source code in <code>oper8/x/datastores/postgres/connection.py</code> <pre><code>def get_tls_secret_volumes(\n    self,\n    cert_mount_path: Optional[str] = None,\n    volume_name: str = DEFAULT_TLS_VOLUME_NAME,\n) -&gt; List[dict]:\n    \"\"\"Get the list of dict entries needed to support TLS for a\n    client. If TLS is not enabled, this will be an empty list.\n\n    Args:\n        cert_mount_path: Optional[str]\n            The name of the file that the ca cert should be mounted to\n        volume_name: str\n            The name of the volume within the pod spec\n\n    Returns:\n        volumes: List[dict]\n            A list of dict Volume entries which can be used to extend other\n            volume lists\n    \"\"\"\n    if self._tls_enabled:\n        cert_mount_path = cert_mount_path or self._tls_secret_cert_field\n        return [\n            dict(\n                name=volume_name,\n                secret=dict(\n                    defaultMode=common.mount_mode(440),\n                    secretName=self._tls_secret_name,\n                    items=[\n                        dict(key=self._tls_secret_cert_field, path=cert_mount_path)\n                    ],\n                ),\n            )\n        ]\n    return []\n</code></pre> <code>to_dict()</code> <p>Return the dict representation of the object for the CR</p> Source code in <code>oper8/x/datastores/postgres/connection.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Return the dict representation of the object for the CR\"\"\"\n    return {field: getattr(self, f\"_{field}\") for field in self._DICT_FIELDS}\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.postgres.factory","title":"<code>factory</code>","text":"<p>Postgres instance factory</p>"},{"location":"API%20References/#oper8.x.datastores.postgres.factory.PostgresFactory","title":"<code>PostgresFactory</code>","text":"<p>               Bases: <code>DatastoreSingletonFactoryBase</code></p> <p>The common factory that will manage instances of Postgres for each deploy</p> Source code in <code>oper8/x/datastores/postgres/factory.py</code> <pre><code>class PostgresFactory(DatastoreSingletonFactoryBase):\n    \"\"\"The common factory that will manage instances of Postgres for each deploy\"\"\"\n\n    DATASTORE_TYPE = \"postgres\"\n    CONNECTION_TYPE = PostgresConnection\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.postgres.interfaces","title":"<code>interfaces</code>","text":"<p>Base class interface for a Postgres component</p>"},{"location":"API%20References/#oper8.x.datastores.postgres.interfaces.IPostgresComponent","title":"<code>IPostgresComponent</code>","text":"<p>               Bases: <code>Datastore</code></p> <p>A postgres chart provides access to a single running Postgres cluster</p> Source code in <code>oper8/x/datastores/postgres/interfaces.py</code> <pre><code>@component(COMPONENT_NAME)\nclass IPostgresComponent(Datastore):\n    \"\"\"A postgres chart provides access to a single running Postgres cluster\"\"\"\n\n    ## Shared Utilities ########################################################\n\n    def tls_enabled(self) -&gt; bool:\n        \"\"\"Return whether TLS is enabled or not\n        Returns:\n            bool: True (TLS enabled), False (TLS disabled)\n        \"\"\"\n        return self.config.get(\"tls\", {}).get(\"enabled\", True)\n</code></pre> <code>tls_enabled()</code> <p>Return whether TLS is enabled or not Returns:     bool: True (TLS enabled), False (TLS disabled)</p> Source code in <code>oper8/x/datastores/postgres/interfaces.py</code> <pre><code>def tls_enabled(self) -&gt; bool:\n    \"\"\"Return whether TLS is enabled or not\n    Returns:\n        bool: True (TLS enabled), False (TLS disabled)\n    \"\"\"\n    return self.config.get(\"tls\", {}).get(\"enabled\", True)\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.redis","title":"<code>redis</code>","text":"<p>Top level imports for the Redis datastore type</p>"},{"location":"API%20References/#oper8.x.datastores.redis.connection","title":"<code>connection</code>","text":"<p>The common connection type for a Redis instance</p>"},{"location":"API%20References/#oper8.x.datastores.redis.connection.RedisConnection","title":"<code>RedisConnection</code>","text":"<p>               Bases: <code>DatastoreConnectionBase</code></p> <p>A RedisConnection holds the core connection information for a named Redis instance, regardless of what IRedisComponent implements it. The key pieces of information are:</p> <ul> <li> <p>General config</p> <ul> <li>hostname: The hostname where the instance can be reached</li> <li>port: The port where the instance is listening</li> </ul> </li> <li> <p>Auth</p> <ul> <li>auth_secret_name: The in-cluster name for the secret holding the     username and password</li> <li>auth_secret_username_field: The field within the auth secret that     holds the username.</li> <li>auth_secret_password_field: The field within the auth secret that     holds the password</li> </ul> </li> <li> <p>TLS:</p> <ul> <li>tls_cert: The content of the TLS cert if tls is enabled</li> <li>tls_secret_name: The in-cluster name for the secret holding the TLS     creds if tls is enabled</li> <li>tls_secret_cert_field: The field within the tls secret that holds the     cert</li> </ul> </li> </ul> Source code in <code>oper8/x/datastores/redis/connection.py</code> <pre><code>class RedisConnection(DatastoreConnectionBase):\n    \"\"\"\n    A RedisConnection holds the core connection information for a named Redis\n    instance, regardless of what IRedisComponent implements it. The key pieces\n    of information are:\n\n    * General config\n        * hostname: The hostname where the instance can be reached\n        * port: The port where the instance is listening\n\n    * Auth\n        * auth_secret_name: The in-cluster name for the secret holding the\n            username and password\n        * auth_secret_username_field: The field within the auth secret that\n            holds the username.\n        * auth_secret_password_field: The field within the auth secret that\n            holds the password\n\n    * TLS:\n        * tls_cert: The content of the TLS cert if tls is enabled\n        * tls_secret_name: The in-cluster name for the secret holding the TLS\n            creds if tls is enabled\n        * tls_secret_cert_field: The field within the tls secret that holds the\n            cert\n    \"\"\"\n\n    def __init__(\n        self,\n        session: Session,\n        hostname: str,\n        port: int,\n        auth_secret_name: str,\n        auth_secret_password_field: str,\n        auth_secret_username_field: str,\n        tls_secret_name: Optional[str] = None,\n        tls_secret_cert_field: Optional[str] = None,\n        auth_username: Optional[str] = None,\n        auth_password: Optional[str] = None,\n        tls_cert: Optional[str] = None,\n    ):\n        super().__init__(session)\n\n        # These fields must be passed in directly\n        self._hostname = hostname\n        self._port = port\n        self._auth_secret_name = auth_secret_name\n        self._auth_secret_username_field = auth_secret_username_field\n        self._auth_secret_password_field = auth_secret_password_field\n        self._tls_secret_name = tls_secret_name\n        self._tls_secret_cert_field = tls_secret_cert_field\n\n        # The secret content may be populated or not, depending on whether this\n        # Connection is being created by the component or a CR config. If not\n        # populated now, they will be lazily populated on client request.\n        self._auth_username = auth_username\n        self._auth_password = auth_password\n        self._tls_cert = tls_cert\n\n        # Ensure that the TLS arguments are provided in a reasonable way. The\n        # cert may be omitted\n        tls_args = {tls_secret_name, tls_secret_cert_field}\n        assert (\n            tls_args == {None} or None not in tls_args\n        ), \"All TLS arguments must be provided if tls is enabled\"\n        self._tls_enabled = tls_args != {None}\n        assert (\n            self._tls_enabled or self._tls_cert is None\n        ), \"Cannot give a tls cert value when tls is disabled\"\n\n        # Schema is deduced based on the presence of the tls arguments\n        self._schema = \"redis\" if tls_secret_name is None else \"rediss\"\n\n    ## Properties ##############################################################\n\n    @property\n    def schema(self) -&gt; str:\n        \"\"\"The schema (redis or rediss)\"\"\"\n        return self._schema\n\n    @property\n    def hostname(self) -&gt; str:\n        \"\"\"The hostname (without schema)\"\"\"\n        return self._hostname\n\n    @property\n    def port(self) -&gt; int:\n        \"\"\"The numeric port\"\"\"\n        return self._port\n\n    @property\n    def auth_secret_name(self) -&gt; str:\n        \"\"\"Secret name containing the username_key and password_key\"\"\"\n        return self._auth_secret_name\n\n    @property\n    def auth_secret_username_field(self) -&gt; str:\n        \"\"\"Field in the auth secret containing the username\"\"\"\n        return self._auth_secret_username_field\n\n    @property\n    def auth_secret_password_field(self) -&gt; str:\n        \"\"\"Field in the auth secret containing the password\"\"\"\n        return self._auth_secret_password_field\n\n    @property\n    def tls_secret_name(self) -&gt; str:\n        \"\"\"The name of the secret holding the tls certificate (for mounting)\"\"\"\n        return self._tls_secret_name\n\n    @property\n    def tls_secret_cert_field(self) -&gt; str:\n        \"\"\"The field within the tls secret that holds the CA cert\"\"\"\n        return self._tls_secret_cert_field\n\n    @property\n    def tls_enabled(self) -&gt; bool:\n        return self._tls_enabled\n\n    ## Interface ###############################################################\n\n    _DICT_FIELDS = [\n        \"hostname\",\n        \"port\",\n        \"auth_secret_name\",\n        \"auth_secret_password_field\",\n        \"auth_secret_username_field\",\n        \"tls_secret_name\",\n        \"tls_secret_cert_field\",\n    ]\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Return the dict representation of the object for the CR\"\"\"\n        return {field: getattr(self, f\"_{field}\") for field in self._DICT_FIELDS}\n\n    @classmethod\n    def from_dict(cls, session: Session, config_dict: dict) -&gt; \"RedisConnection\":\n        kwargs = {\"session\": session}\n        config_dict = common.camelcase_to_snake_case(config_dict)\n        uri_secret = config_dict.get(\"uri_secret\")\n        uri_hostname_field = config_dict.get(\"uri_secret_hostname_field\")\n        uri_port_field = config_dict.get(\"uri_secret_port_field\")\n\n        # First pull provided hostname/port secret if available and fill in\n        # hostname/port fields into config_dict\n        if uri_secret and uri_hostname_field and uri_port_field:\n            # If we have provided host/port credentials, we need to extract them\n            # and place these values in our config dict\n            success, secret_content = session.get_object_current_state(\n                \"Secret\", uri_secret\n            )\n            assert_cluster(success, f\"Fetching connection secret [{uri_secret}] failed\")\n            assert \"data\" in secret_content, \"Got a secret without 'data'?\"\n            secret_content = secret_content.get(\"data\")\n            assert_precondition(\n                secret_content,\n                f\"Missing expected Secret/{uri_secret} holding [hostname] and [port]\",\n            )\n            hostname = common.b64_secret_decode(secret_content.get(uri_hostname_field))\n            port = common.b64_secret_decode(secret_content.get(uri_port_field))\n            if None in [hostname, port]:\n                log.debug2(\n                    \"Failed to find hostname/port in uri secret [%s]\",\n                    uri_secret,\n                )\n\n            try:\n                port = int(port)\n            except ValueError as err:\n                raise ConfigError(f\"Invalid non-int port: {port}\") from err\n            config_dict[\"hostname\"], config_dict[\"port\"] = hostname, port\n\n        for field in cls._DICT_FIELDS:\n            if field not in config_dict:\n                raise ValueError(f\"Missing required connection element [{field}]\")\n\n            # Set the kwargs (using None in place of empty strings)\n            kwargs[field] = config_dict[field] or None\n\n        return cls(**kwargs)\n\n    ## Client Utilities ########################################################\n\n    def get_auth_username_password(self) -&gt; Tuple[Optional[str], Optional[str]]:\n        \"\"\"Get the current username_key/password_key pair from the auth secret if\n        available\n\n        Returns:\n            username:  str or None\n                The plain-text username (not encoded) if available\n            password:  str or None\n                The plain-text password (not encoded) if available\n        \"\"\"\n        if None in [self._auth_username, self._auth_password]:\n            secret_content = self._fetch_secret_data(self._auth_secret_name) or {}\n            log.debug4(\"Auth secret content: %s\", secret_content)\n            log.debug3(\n                \"Looking for [%s/%s]\",\n                self._auth_secret_username_field,\n                self._auth_secret_password_field,\n            )\n            username = secret_content.get(self._auth_secret_username_field)\n            password = secret_content.get(self._auth_secret_password_field)\n            # username not required as expect username only used when using ACL\n            # redis-cli also does support username in URI to be used for that\n            # CITE: https://redis.io/commands/auth\n            if None in [username, password]:\n                log.debug2(\n                    \"Failed to find username/password in auth secret [%s]\",\n                    self._auth_secret_name,\n                )\n                return None, None\n            self._auth_username = username\n            self._auth_password = password\n        return self._auth_username, self._auth_password\n\n    def get_tls_cert(self) -&gt; Optional[str]:\n        \"\"\"Get the current TLS certificate for a client connection if TLS is\n        enabled\n\n        If TLS is enabled, but the cert is not found, this function will raise\n        an AssertionError\n\n        Returns:\n            tls_cert: str or None\n                PEM encoded cert string (not base64-encoded) if found, otherwise\n                None\n        \"\"\"\n        if self._tls_enabled:\n            if self._tls_cert is None:\n                secret_data = self._fetch_secret_data(self._tls_secret_name)\n                if secret_data is not None:\n                    self._tls_cert = secret_data.get(self._tls_secret_cert_field)\n            return self._tls_cert\n\n        return None\n\n    def get_connection_string(self) -&gt; str:\n        \"\"\"Get the formatted Redis connection string to connect to the instance\n\n        Returns:\n            connection_string:  str\n                The formatted connection string\n        \"\"\"\n        username_key, password_key = self.get_auth_username_password()\n        assert_precondition(\n            None not in [username_key, password_key],\n            \"No auth keys available for Redis connection string\",\n        )\n        # NOTE: username/password required and needs to change if ever need to\n        # support the rediss://&lt;host&gt;:&lt;port&gt; format without username/password\n        # CITE: https://redis.io/topics/rediscli\n        return \"{}://{}:{}@{}:{}\".format(\n            self._schema, username_key, password_key, self._hostname, self._port\n        )\n</code></pre> <code>auth_secret_name</code> <code>property</code> <p>Secret name containing the username_key and password_key</p> <code>auth_secret_password_field</code> <code>property</code> <p>Field in the auth secret containing the password</p> <code>auth_secret_username_field</code> <code>property</code> <p>Field in the auth secret containing the username</p> <code>hostname</code> <code>property</code> <p>The hostname (without schema)</p> <code>port</code> <code>property</code> <p>The numeric port</p> <code>schema</code> <code>property</code> <p>The schema (redis or rediss)</p> <code>tls_secret_cert_field</code> <code>property</code> <p>The field within the tls secret that holds the CA cert</p> <code>tls_secret_name</code> <code>property</code> <p>The name of the secret holding the tls certificate (for mounting)</p> <code>get_auth_username_password()</code> <p>Get the current username_key/password_key pair from the auth secret if available</p> <p>Returns:</p> Name Type Description <code>username</code> <code>Optional[str]</code> <p>str or None The plain-text username (not encoded) if available</p> <code>password</code> <code>Optional[str]</code> <p>str or None The plain-text password (not encoded) if available</p> Source code in <code>oper8/x/datastores/redis/connection.py</code> <pre><code>def get_auth_username_password(self) -&gt; Tuple[Optional[str], Optional[str]]:\n    \"\"\"Get the current username_key/password_key pair from the auth secret if\n    available\n\n    Returns:\n        username:  str or None\n            The plain-text username (not encoded) if available\n        password:  str or None\n            The plain-text password (not encoded) if available\n    \"\"\"\n    if None in [self._auth_username, self._auth_password]:\n        secret_content = self._fetch_secret_data(self._auth_secret_name) or {}\n        log.debug4(\"Auth secret content: %s\", secret_content)\n        log.debug3(\n            \"Looking for [%s/%s]\",\n            self._auth_secret_username_field,\n            self._auth_secret_password_field,\n        )\n        username = secret_content.get(self._auth_secret_username_field)\n        password = secret_content.get(self._auth_secret_password_field)\n        # username not required as expect username only used when using ACL\n        # redis-cli also does support username in URI to be used for that\n        # CITE: https://redis.io/commands/auth\n        if None in [username, password]:\n            log.debug2(\n                \"Failed to find username/password in auth secret [%s]\",\n                self._auth_secret_name,\n            )\n            return None, None\n        self._auth_username = username\n        self._auth_password = password\n    return self._auth_username, self._auth_password\n</code></pre> <code>get_connection_string()</code> <p>Get the formatted Redis connection string to connect to the instance</p> <p>Returns:</p> Name Type Description <code>connection_string</code> <code>str</code> <p>str The formatted connection string</p> Source code in <code>oper8/x/datastores/redis/connection.py</code> <pre><code>def get_connection_string(self) -&gt; str:\n    \"\"\"Get the formatted Redis connection string to connect to the instance\n\n    Returns:\n        connection_string:  str\n            The formatted connection string\n    \"\"\"\n    username_key, password_key = self.get_auth_username_password()\n    assert_precondition(\n        None not in [username_key, password_key],\n        \"No auth keys available for Redis connection string\",\n    )\n    # NOTE: username/password required and needs to change if ever need to\n    # support the rediss://&lt;host&gt;:&lt;port&gt; format without username/password\n    # CITE: https://redis.io/topics/rediscli\n    return \"{}://{}:{}@{}:{}\".format(\n        self._schema, username_key, password_key, self._hostname, self._port\n    )\n</code></pre> <code>get_tls_cert()</code> <p>Get the current TLS certificate for a client connection if TLS is enabled</p> <p>If TLS is enabled, but the cert is not found, this function will raise an AssertionError</p> <p>Returns:</p> Name Type Description <code>tls_cert</code> <code>Optional[str]</code> <p>str or None PEM encoded cert string (not base64-encoded) if found, otherwise None</p> Source code in <code>oper8/x/datastores/redis/connection.py</code> <pre><code>def get_tls_cert(self) -&gt; Optional[str]:\n    \"\"\"Get the current TLS certificate for a client connection if TLS is\n    enabled\n\n    If TLS is enabled, but the cert is not found, this function will raise\n    an AssertionError\n\n    Returns:\n        tls_cert: str or None\n            PEM encoded cert string (not base64-encoded) if found, otherwise\n            None\n    \"\"\"\n    if self._tls_enabled:\n        if self._tls_cert is None:\n            secret_data = self._fetch_secret_data(self._tls_secret_name)\n            if secret_data is not None:\n                self._tls_cert = secret_data.get(self._tls_secret_cert_field)\n        return self._tls_cert\n\n    return None\n</code></pre> <code>to_dict()</code> <p>Return the dict representation of the object for the CR</p> Source code in <code>oper8/x/datastores/redis/connection.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Return the dict representation of the object for the CR\"\"\"\n    return {field: getattr(self, f\"_{field}\") for field in self._DICT_FIELDS}\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.redis.factory","title":"<code>factory</code>","text":"<p>Redis instance factory</p>"},{"location":"API%20References/#oper8.x.datastores.redis.factory.RedisFactory","title":"<code>RedisFactory</code>","text":"<p>               Bases: <code>DatastoreSingletonFactoryBase</code></p> <p>The common factory that will manage instances of Redis</p> Source code in <code>oper8/x/datastores/redis/factory.py</code> <pre><code>class RedisFactory(DatastoreSingletonFactoryBase):\n    \"\"\"The common factory that will manage instances of Redis\"\"\"\n\n    DATASTORE_TYPE = \"redis\"\n    CONNECTION_TYPE = RedisConnection\n</code></pre>"},{"location":"API%20References/#oper8.x.datastores.redis.interfaces","title":"<code>interfaces</code>","text":"<p>Base class interface for a Redis component</p>"},{"location":"API%20References/#oper8.x.datastores.redis.interfaces.IRedisComponent","title":"<code>IRedisComponent</code>","text":"<p>               Bases: <code>Datastore</code></p> <p>A redis chart provides access to a redis instance</p> Source code in <code>oper8/x/datastores/redis/interfaces.py</code> <pre><code>@component(COMPONENT_NAME)\nclass IRedisComponent(Datastore):\n    \"\"\"A redis chart provides access to a redis instance\"\"\"\n\n    ## Parent Interface ########################################################\n\n    def get_connection(self) -&gt; RedisConnection:\n        \"\"\"Get the connection object for this instance\"\"\"\n        return RedisConnection(\n            session=self.session,\n            hostname=self._get_hostname(),\n            port=self._get_port(),\n            auth_secret_name=self._get_auth_secret_name(),\n            auth_secret_username_field=self._get_auth_secret_username_field(),\n            auth_secret_password_field=self._get_auth_secret_password_field(),\n            tls_secret_name=self._get_tls_secret_name(),\n            tls_secret_cert_field=self._get_tls_secret_cert_field(),\n            auth_username=self._get_auth_username(),\n            auth_password=self._get_auth_password(),\n            tls_cert=self._get_tls_cert(),\n        )\n\n    ## Abstract Interface ######################################################\n    #\n    # This is the interface that needs to be implemented by a child in order to\n    # provide the common information that a client will use.\n    ##\n\n    ##################\n    ## General Info ##\n    ##################\n\n    @abstractmethod\n    def _get_hostname(self) -&gt; str:\n        \"\"\"Gets the hotsname for the connection. Can be IP address as well.\n\n        Returns:\n            hostname:  str\n                The hostname (without schema) for the service\n        \"\"\"\n\n    @abstractmethod\n    def _get_port(self) -&gt; int:\n        \"\"\"Gets the port where the service is listening\n\n        Returns:\n            port:  int\n                The port where the service is listening\n        \"\"\"\n\n    ###############\n    ## Auth Info ##\n    ###############\n\n    @abstractmethod\n    def _get_auth_secret_name(self) -&gt; str:\n        \"\"\"Get the Auth secret name with any scoping applied\n\n        Returns:\n            auth_secret_name:  str\n                The name of the secret containing the auth secret\n        \"\"\"\n\n    @abstractmethod\n    def _get_auth_secret_username_field(self) -&gt; Optional[str]:\n        \"\"\"Get the field form within the auth secret that contains the\n        username\n\n        Returns:\n            username_key_field:  str\n                The field within the auth secret that contains the username\n        \"\"\"\n\n    @abstractmethod\n    def _get_auth_secret_password_field(self) -&gt; str:\n        \"\"\"Get the field form within the auth secret that contains the\n        password for the user\n\n        Returns:\n            password_key_field:  str\n                The field within the auth secret that contains the password_key\n        \"\"\"\n\n    @abstractmethod\n    def _get_auth_username(self) -&gt; Optional[str]:\n        \"\"\"Get the un-encoded content of the username if available in-memory.\n        Components which proxy an external secret don't need to fetch this\n        content from the cluster.\n\n        Returns:\n            username:  Optional[str]\n                The content of the username if known\n        \"\"\"\n\n    @abstractmethod\n    def _get_auth_password(self) -&gt; Optional[str]:\n        \"\"\"Get the un-encoded content of the password if available in-memory.\n        Components which proxy an external secret don't need to fetch this\n        content from the cluster.\n\n        Returns:\n            password:  Optional[str]\n                The content of the password if known\n        \"\"\"\n\n    ##############\n    ## TLS Info ##\n    ##############\n\n    @abstractmethod\n    def _get_tls_secret_name(self) -&gt; Optional[str]:\n        \"\"\"Get the TLS secret name with any scoping applied if tls is enabled\n\n        Returns:\n            tls_secret_name:  Optional[str]\n                If tls is enabled, returns the name of the secret, otherwise\n                None\n        \"\"\"\n\n    @abstractmethod\n    def _get_tls_secret_cert_field(self) -&gt; Optional[str]:\n        \"\"\"Get the field from within the tls secret that contains the CA\n        certificate a client would need to use to connect\n\n        Returns:\n            cert_field:  Optional[str]\n                The field within the tls secret where the CA certificate lives\n        \"\"\"\n\n    @abstractmethod\n    def _get_tls_cert(self) -&gt; Optional[str]:\n        \"\"\"Get the un-encoded content of the TLS cert if TLS is enabled and\n        available in-memory. Components which proxy an external secret don't\n        need to fetch this content from the cluster.\n\n        Returns:\n            cert_content:  Optional[str]\n                The content of the cert if tls is enabled\n        \"\"\"\n</code></pre> <code>get_connection()</code> <p>Get the connection object for this instance</p> Source code in <code>oper8/x/datastores/redis/interfaces.py</code> <pre><code>def get_connection(self) -&gt; RedisConnection:\n    \"\"\"Get the connection object for this instance\"\"\"\n    return RedisConnection(\n        session=self.session,\n        hostname=self._get_hostname(),\n        port=self._get_port(),\n        auth_secret_name=self._get_auth_secret_name(),\n        auth_secret_username_field=self._get_auth_secret_username_field(),\n        auth_secret_password_field=self._get_auth_secret_password_field(),\n        tls_secret_name=self._get_tls_secret_name(),\n        tls_secret_cert_field=self._get_tls_secret_cert_field(),\n        auth_username=self._get_auth_username(),\n        auth_password=self._get_auth_password(),\n        tls_cert=self._get_tls_cert(),\n    )\n</code></pre>"},{"location":"API%20References/#oper8.x.oper8x_component","title":"<code>oper8x_component</code>","text":"<p>This class provides a base class with shared functionality that all concrete components can use.</p>"},{"location":"API%20References/#oper8.x.oper8x_component.Oper8xComponent","title":"<code>Oper8xComponent</code>","text":"<p>               Bases: <code>Component</code></p> <p>The Oper8xComponent provides common config-based utilities on top of the core oper8.Component base class. It can be used as a drop-in replacement.</p> Source code in <code>oper8/x/oper8x_component.py</code> <pre><code>class Oper8xComponent(Component):\n    \"\"\"The Oper8xComponent provides common config-based utilities on top of the\n    core oper8.Component base class. It can be used as a drop-in replacement.\n    \"\"\"\n\n    def __init__(self, session: Session, disabled: bool = False):\n        \"\"\"Construct with a member to access the session in implementations\n\n        Args:\n            session:  Session\n                The session for the current deployment\n            disabled:  bool\n                Whether or not this component is disabled in the current\n                configuration\n        \"\"\"\n        super().__init__(session=session, disabled=disabled)\n        self._session = session\n\n    @property\n    def session(self):\n        return self._session\n\n    ## Interface Overrides #####################################################\n\n    def deploy(self, session: Session) -&gt; bool:\n        \"\"\"Override the base Component's implementation of deploy to insert the\n        dependency hash annotation. See NOTE in deps_annotation for explanation\n        of why deploy is used instead of update_object_definition.\n\n        Args:\n            session:  Session\n                The session for the current deployment\n\n        Returns:\n            success:  bool\n                True on successful application of the resource to the cluster\n        \"\"\"\n        for obj in self.managed_objects:\n            obj.definition = deps_annotation.add_deps_annotation(\n                self, session, obj.definition\n            )\n        return super().deploy(session)\n\n    def update_object_definition(\n        self,\n        session: Session,\n        internal_name: str,\n        resource_definition: dict,\n    ) -&gt; dict:\n        \"\"\"For components assigned to different namespaces, ensure that the\n        target namespace is set\n\n        Args:\n            session:  Session\n                The session for this deploy\n            internal_name:  str\n                The internal name of the object to update\n            resource_definition:  dict\n                The dict representation of the resource to modify\n\n        Returns:\n            resource_definition:  dict\n                The dict representation of the resource with any modifications\n                applied\n        \"\"\"\n\n        # Call the base implementation\n        resource_definition = super().update_object_definition(\n            session,\n            internal_name,\n            resource_definition,\n        )\n\n        # Inject namespace override for this component if given\n        namespace_override = session.config.get(self.name, {}).get(\"namespace\")\n        if namespace_override is not None:\n            log.debug2(\"Namespace  override for %s: %s\", self, namespace_override)\n            metadata = resource_definition.get(\"metadata\")\n            assert isinstance(metadata, dict), \"Resource metadata is not a dict!\"\n            metadata[\"namespace\"] = namespace_override\n\n        return resource_definition\n\n    ## Shared Utilities ########################################################\n\n    def get_cluster_name(self, resource_name: str) -&gt; str:\n        \"\"\"Get the name for a given resource with any instance scoping applied\n\n        Args:\n            resource_name:  str\n                The unscoped name of a kubernetes resource\n\n        Returns:\n            resource_cluster_name:  str\n                The name that the resource will use in the cluster\n        \"\"\"\n        return common.get_resource_cluster_name(\n            resource_name=resource_name,\n            component=self.name,\n            session=self.session,\n        )\n\n    def get_replicas(self, force: bool = False) -&gt; Union[int, None]:\n        \"\"\"Get the replica count for this component based on the current\n        deploy's t-shirt size and the state of the instance-size label. A\n        replica count is only returned if there is not an existing replica count\n        in the cluster for this deployment, the t-shirt size has changed, or\n        the force flag is True.\n\n        Args:\n            force: bool\n                If True, the state of the cluster will not be checked\n\n        Returns:\n            replicas:  Union[int, None]\n                If replicas should be set for this deployment, the integer count\n                will be returned, otherwise None is returned.\n        \"\"\"\n        return common.get_replicas(\n            session=self.session,\n            component_name=self.name,\n            unscoped_name=self.name,\n            force=force,\n        )\n</code></pre>"},{"location":"API%20References/#oper8.x.oper8x_component.Oper8xComponent.__init__","title":"<code>__init__(session, disabled=False)</code>","text":"<p>Construct with a member to access the session in implementations</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The session for the current deployment</p> required <code>disabled</code> <code>bool</code> <p>bool Whether or not this component is disabled in the current configuration</p> <code>False</code> Source code in <code>oper8/x/oper8x_component.py</code> <pre><code>def __init__(self, session: Session, disabled: bool = False):\n    \"\"\"Construct with a member to access the session in implementations\n\n    Args:\n        session:  Session\n            The session for the current deployment\n        disabled:  bool\n            Whether or not this component is disabled in the current\n            configuration\n    \"\"\"\n    super().__init__(session=session, disabled=disabled)\n    self._session = session\n</code></pre>"},{"location":"API%20References/#oper8.x.oper8x_component.Oper8xComponent.deploy","title":"<code>deploy(session)</code>","text":"<p>Override the base Component's implementation of deploy to insert the dependency hash annotation. See NOTE in deps_annotation for explanation of why deploy is used instead of update_object_definition.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The session for the current deployment</p> required <p>Returns:</p> Name Type Description <code>success</code> <code>bool</code> <p>bool True on successful application of the resource to the cluster</p> Source code in <code>oper8/x/oper8x_component.py</code> <pre><code>def deploy(self, session: Session) -&gt; bool:\n    \"\"\"Override the base Component's implementation of deploy to insert the\n    dependency hash annotation. See NOTE in deps_annotation for explanation\n    of why deploy is used instead of update_object_definition.\n\n    Args:\n        session:  Session\n            The session for the current deployment\n\n    Returns:\n        success:  bool\n            True on successful application of the resource to the cluster\n    \"\"\"\n    for obj in self.managed_objects:\n        obj.definition = deps_annotation.add_deps_annotation(\n            self, session, obj.definition\n        )\n    return super().deploy(session)\n</code></pre>"},{"location":"API%20References/#oper8.x.oper8x_component.Oper8xComponent.get_cluster_name","title":"<code>get_cluster_name(resource_name)</code>","text":"<p>Get the name for a given resource with any instance scoping applied</p> <p>Parameters:</p> Name Type Description Default <code>resource_name</code> <code>str</code> <p>str The unscoped name of a kubernetes resource</p> required <p>Returns:</p> Name Type Description <code>resource_cluster_name</code> <code>str</code> <p>str The name that the resource will use in the cluster</p> Source code in <code>oper8/x/oper8x_component.py</code> <pre><code>def get_cluster_name(self, resource_name: str) -&gt; str:\n    \"\"\"Get the name for a given resource with any instance scoping applied\n\n    Args:\n        resource_name:  str\n            The unscoped name of a kubernetes resource\n\n    Returns:\n        resource_cluster_name:  str\n            The name that the resource will use in the cluster\n    \"\"\"\n    return common.get_resource_cluster_name(\n        resource_name=resource_name,\n        component=self.name,\n        session=self.session,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.x.oper8x_component.Oper8xComponent.get_replicas","title":"<code>get_replicas(force=False)</code>","text":"<p>Get the replica count for this component based on the current deploy's t-shirt size and the state of the instance-size label. A replica count is only returned if there is not an existing replica count in the cluster for this deployment, the t-shirt size has changed, or the force flag is True.</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <code>bool</code> <p>bool If True, the state of the cluster will not be checked</p> <code>False</code> <p>Returns:</p> Name Type Description <code>replicas</code> <code>Union[int, None]</code> <p>Union[int, None] If replicas should be set for this deployment, the integer count will be returned, otherwise None is returned.</p> Source code in <code>oper8/x/oper8x_component.py</code> <pre><code>def get_replicas(self, force: bool = False) -&gt; Union[int, None]:\n    \"\"\"Get the replica count for this component based on the current\n    deploy's t-shirt size and the state of the instance-size label. A\n    replica count is only returned if there is not an existing replica count\n    in the cluster for this deployment, the t-shirt size has changed, or\n    the force flag is True.\n\n    Args:\n        force: bool\n            If True, the state of the cluster will not be checked\n\n    Returns:\n        replicas:  Union[int, None]\n            If replicas should be set for this deployment, the integer count\n            will be returned, otherwise None is returned.\n    \"\"\"\n    return common.get_replicas(\n        session=self.session,\n        component_name=self.name,\n        unscoped_name=self.name,\n        force=force,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.x.oper8x_component.Oper8xComponent.update_object_definition","title":"<code>update_object_definition(session, internal_name, resource_definition)</code>","text":"<p>For components assigned to different namespaces, ensure that the target namespace is set</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The session for this deploy</p> required <code>internal_name</code> <code>str</code> <p>str The internal name of the object to update</p> required <code>resource_definition</code> <code>dict</code> <p>dict The dict representation of the resource to modify</p> required <p>Returns:</p> Name Type Description <code>resource_definition</code> <code>dict</code> <p>dict The dict representation of the resource with any modifications applied</p> Source code in <code>oper8/x/oper8x_component.py</code> <pre><code>def update_object_definition(\n    self,\n    session: Session,\n    internal_name: str,\n    resource_definition: dict,\n) -&gt; dict:\n    \"\"\"For components assigned to different namespaces, ensure that the\n    target namespace is set\n\n    Args:\n        session:  Session\n            The session for this deploy\n        internal_name:  str\n            The internal name of the object to update\n        resource_definition:  dict\n            The dict representation of the resource to modify\n\n    Returns:\n        resource_definition:  dict\n            The dict representation of the resource with any modifications\n            applied\n    \"\"\"\n\n    # Call the base implementation\n    resource_definition = super().update_object_definition(\n        session,\n        internal_name,\n        resource_definition,\n    )\n\n    # Inject namespace override for this component if given\n    namespace_override = session.config.get(self.name, {}).get(\"namespace\")\n    if namespace_override is not None:\n        log.debug2(\"Namespace  override for %s: %s\", self, namespace_override)\n        metadata = resource_definition.get(\"metadata\")\n        assert isinstance(metadata, dict), \"Resource metadata is not a dict!\"\n        metadata[\"namespace\"] = namespace_override\n\n    return resource_definition\n</code></pre>"},{"location":"API%20References/#oper8.x.utils","title":"<code>utils</code>","text":"<p>Common utilities for reused components</p>"},{"location":"API%20References/#oper8.x.utils.abc_static","title":"<code>abc_static</code>","text":"<p>This module adds metaclass support for declaring an interface with @abstractmethod methods that MUST be implemented as @classmethod or @staticmethod</p>"},{"location":"API%20References/#oper8.x.utils.abc_static.ABCStatic","title":"<code>ABCStatic</code>","text":"<p>An ABCStatic class is a child of abc.ABC which has support for enforcing methods which combine @classmethod and @abstractmethod</p> Source code in <code>oper8/x/utils/abc_static.py</code> <pre><code>class ABCStatic(metaclass=ABCStaticMeta):\n    \"\"\"An ABCStatic class is a child of abc.ABC which has support for enforcing\n    methods which combine @classmethod and @abstractmethod\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.abc_static.ABCStaticMeta","title":"<code>ABCStaticMeta</code>","text":"<p>               Bases: <code>ABCMeta</code></p> <p>The StaticABCMeta class is a metaclass that enforces implementations of base class functions marked as both @abstractmethod and @classmethod. Methods with this signature MUST be implemented with the @classmethod or @staticmethod decorator in derived classes.</p> Source code in <code>oper8/x/utils/abc_static.py</code> <pre><code>class ABCStaticMeta(abc.ABCMeta):\n    \"\"\"The StaticABCMeta class is a metaclass that enforces implementations of\n    base class functions marked as both @abstractmethod and @classmethod.\n    Methods with this signature MUST be implemented with the @classmethod or\n    @staticmethod decorator in derived classes.\n    \"\"\"\n\n    def __init__(cls, name, bases, dct):\n        # Find abstract class methods that have not been implemented at all\n        attrs = {name: getattr(cls, name) for name in dir(cls)}\n        cls.__abstract_class_methods__ = [\n            name\n            for name, attr in attrs.items()\n            if inspect.ismethod(attr) and getattr(attr, \"__isabstractmethod__\", False)\n        ]\n\n        # For any abstract class methods that have not been implemented,\n        # overwrite them to raise NotImplementedError if called\n        for method_name in cls.__abstract_class_methods__:\n\n            def not_implemented(*_, x=method_name, **__):\n                raise NotImplementedError(f\"Cannot invoke abstract class method {x}\")\n\n            not_implemented.__original_signature__ = inspect.signature(\n                getattr(cls, method_name)\n            )\n            setattr(cls, method_name, not_implemented)\n\n        # Look for abstract class methods of parents\n        base_abstract_class_methods = {\n            method_name: getattr(base, method_name)\n            for base in bases\n            for method_name in getattr(base, \"__abstract_class_methods__\", [])\n            if method_name not in cls.__abstract_class_methods__\n        }\n\n        # If any parent abstract class methods have been implemented as instance\n        # methods, raise an import-time exception\n        for method_name, base_method in base_abstract_class_methods.items():\n            # A local implementation is valid if it is a bound method (\n            # implemented as a @classmethod) or it is a function with a\n            # signature that exactly matches the signature of the base class\n            # (implemented as @staticmethod).\n            this_method = getattr(cls, method_name)\n            is_classmethod = inspect.ismethod(this_method)\n            original_signature = getattr(base_method, \"__original_signature__\", None)\n            is_staticmethod = inspect.isfunction(this_method) and inspect.signature(\n                this_method\n            ) in [original_signature, inspect.signature(base_method)]\n            if not (is_classmethod or is_staticmethod):\n                raise NotImplementedError(\n                    f\"The method [{method_name}] is an @classmethod @abstractmethod. \"\n                    f\"{cls} implements it as an instance method\"\n                )\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.common","title":"<code>common</code>","text":"<p>Shared utilities accessible to all components</p>"},{"location":"API%20References/#oper8.x.utils.common.from_string_or_number","title":"<code>from_string_or_number(value)</code>","text":"<p>Handle strings or numbers for fields that can be either string or numeric</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[int, str, float]</code> <p>Union[str, int, float] Quantity type that can be in numeric or string form (e.g. resources)</p> required <p>Returns:</p> Name Type Description <code>formatted_value</code> <code>Union[int, str, float]</code> <p>Union[str, int, float] The value formatted as the correct type</p> Source code in <code>oper8/x/utils/common.py</code> <pre><code>def from_string_or_number(value: Union[int, str, float]) -&gt; Union[int, str, float]:\n    \"\"\"Handle strings or numbers for fields that can be either string or numeric\n\n    Args:\n        value: Union[str, int, float]\n            Quantity type that can be in numeric or string form (e.g. resources)\n\n    Returns:\n        formatted_value: Union[str, int, float]\n            The value formatted as the correct type\n    \"\"\"\n    # By default no conversion is needed\n    formatted_value = value\n\n    # If it's a string, try converting it to an int, then a float\n    if isinstance(value, str):\n        for target_type in [int, float]:\n            try:\n                formatted_value = target_type(value)\n                break\n            except ValueError:\n                pass\n\n    return formatted_value\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.common.get_deploy_labels","title":"<code>get_deploy_labels(session, base_labels=None)</code>","text":"<p>Get labels for a Deployment resource on top of the standard base labels</p> Source code in <code>oper8/x/utils/common.py</code> <pre><code>def get_deploy_labels(session, base_labels=None):\n    \"\"\"Get labels for a Deployment resource on top of the standard base labels\"\"\"\n    # Shallow copy is fine here since labels are one-level deep and only strings\n    deploy_labels = copy.copy(base_labels or {})\n    deploy_labels[\"instance-size\"] = session.spec.size\n    return deploy_labels\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.common.get_labels","title":"<code>get_labels(cluster_name, session, component_name=None)</code>","text":"<p>Common utility for fetching the set of metadata.labels for a given resource. Args:     cluster_name:  str         The name of the resource as it will be applied to the cluster         including any scoping applied by get_resource_cluster_name     session:  DeploySession         The session for the current deployment     component_name:  str         The name of the component that manages this resource.         NOTE: This argument is optional for backwards compatibility,             but should always be provided to ensure accurate labels! Returns:     labels:  Dict[str, str]         The full set of labels to use for the given resource</p> Source code in <code>oper8/x/utils/common.py</code> <pre><code>def get_labels(\n    cluster_name: str,\n    session: Session,\n    component_name: Optional[str] = None,\n) -&gt; Dict[str, str]:\n    \"\"\"Common utility for fetching the set of metadata.labels for a given resource.\n    Args:\n        cluster_name:  str\n            The name of the resource as it will be applied to the cluster\n            including any scoping applied by get_resource_cluster_name\n        session:  DeploySession\n            The session for the current deployment\n        component_name:  str\n            The name of the component that manages this resource.\n            NOTE: This argument is optional for backwards compatibility,\n                but should always be provided to ensure accurate labels!\n    Returns:\n        labels:  Dict[str, str]\n            The full set of labels to use for the given resource\n    \"\"\"\n    labels = {\n        \"app\": cluster_name,\n        \"app.kubernetes.io/managed-by\": \"Oper8\",\n        \"app.kubernetes.io/instance\": session.name,\n    }\n    if component_name:\n        labels[\"component\"] = component_name\n    if slot_name := get_slot_name(component_name, session):\n        labels[\"slot\"] = slot_name\n\n    # Add user-specified labels from the CR's spec.labels field\n    user_labels = session.spec.labels or {}\n    labels.update(user_labels)\n\n    return labels\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.common.get_replicas","title":"<code>get_replicas(session, component_name, unscoped_name, force=False, replicas_override=None)</code>","text":"<p>Get the replica count for the given resource.</p> <p>This function consolidates logic for getting replicas for all components in the application. It allows replicas to be conditionally set only when needed to avoid thrashing with HPAs.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current deploy session</p> required <code>component_name</code> <code>str</code> <p>str The name of the component to get replicas for</p> required <code>unscoped_name</code> <code>str</code> <p>str The external name of the deployment without scoping</p> required <code>force</code> <code>bool</code> <p>bool If True, the state of the cluster will not be checked</p> <code>False</code> <code>replicas_override</code> <code>Union[int, None]</code> <p>int or None An override value to use in place of the normal config-based value</p> <code>None</code> <p>Returns:</p> Name Type Description <code>replicas</code> <code>Union[int, None]</code> <p>int or None If replicas should not be set for this resource, None is returned, otherwise the number of replicas is returned based on the t-shirt size for the instance.</p> Source code in <code>oper8/x/utils/common.py</code> <pre><code>def get_replicas(\n    session: Session,\n    component_name: str,\n    unscoped_name: str,\n    force: bool = False,\n    replicas_override: Union[int, None] = None,\n) -&gt; Union[int, None]:\n    \"\"\"\n    Get the replica count for the given resource.\n\n    This function consolidates logic for getting replicas for all components in\n    the application. It allows replicas to be conditionally set only when needed\n    to avoid thrashing with HPAs.\n\n    Args:\n        session: Session\n            The current deploy session\n        component_name: str\n            The name of the component to get replicas for\n        unscoped_name: str\n            The external name of the deployment without scoping\n        force: bool\n            If True, the state of the cluster will not be checked\n        replicas_override: int or None\n            An override value to use in place of the normal config-based value\n\n    Returns:\n        replicas: int or None\n            If replicas should not be set for this resource, None is returned,\n            otherwise the number of replicas is returned based on the t-shirt\n            size for the instance.\n    \"\"\"\n\n    # Fetch the current state of the deployment\n    if not force:\n        name = get_resource_cluster_name(\n            resource_name=unscoped_name,\n            component=component_name,\n            session=session,\n        )\n        success, content = session.get_object_current_state(\n            kind=\"Deployment\",\n            name=name,\n            api_version=\"apps/v1\",\n        )\n        assert success, f\"Failed to look up state for [{name}]\"\n\n        # Check the current content to see if this is a t-shirt size change\n        if content is not None:\n            # Fetch the current replica count. We'll reuse this if there's no\n            # reason to change\n            replicas = content.get(\"spec\", {}).get(\"replicas\")\n\n            # If we found replicas, check for t-shirt size change\n            if replicas is None:\n                log.debug(\"No replicas found for [%s]. Using config.\".name)\n            else:\n                assert isinstance(replicas, int), \"Replicas is not an int!\"\n                current_size = session.spec.size\n                deployed_size = (\n                    content.get(\"metadata\", {}).get(\"labels\", {}).get(\"instance-size\")\n                )\n                if replicas == 0 and not session.spec.get(\"backup\", {}).get(\n                    \"offlineQuiesce\", False\n                ):\n                    log.debug(\n                        \"Found [%s] with size [%s] and offlineQuiesce off. Need \"\n                        \"to scale up from [%s] replicas.\",\n                        name,\n                        current_size,\n                        replicas,\n                    )\n                elif current_size == deployed_size:\n                    log.debug(\n                        \"Found [%s] with size [%s]. Not changing replicas from [%s].\",\n                        name,\n                        current_size,\n                        replicas,\n                    )\n                    return replicas\n                else:\n                    log.debug(\n                        \"Found t-shirt size change for [%s] from [%s -&gt; %s]\",\n                        name,\n                        deployed_size,\n                        current_size,\n                    )\n\n    # Look up the replicas based on the t-shirt size\n    size = session.spec.size\n    replica_map = session.config.get(\"replicas\", {}).get(size, {})\n    replicas = replicas_override or replica_map.get(component_name)\n    log.debug3(\"Replica map for [%s]: %s\", size, replica_map)\n    assert_config(\n        replicas is not None,\n        f\"No replicas for [{component_name}] available for size [{size}]\",\n    )\n    return replicas\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.common.get_resource_cluster_name","title":"<code>get_resource_cluster_name(resource_name, component, session)</code>","text":"<p>Common helper function to get the name a given kubernetes resource should use when deployed to the cluster.</p> <p>Parameters:</p> Name Type Description Default <code>resource_name</code> <code>str</code> <p>str The raw name for the resource (e.g. sireg-secret)</p> required <code>component</code> <code>Component</code> <p>Union[Component, str] The component (or component name) that owns this resource</p> required <code>session</code> <code>Session</code> <p>Session The session for the current reconciliation deploy</p> required <p>Returns:</p> Name Type Description <code>resource_cluster_name</code> <p>str The resource name with appropriate scoping and truncation added</p> Source code in <code>oper8/x/utils/common.py</code> <pre><code>def get_resource_cluster_name(\n    resource_name: str,\n    component: Component,\n    session: Session,\n):\n    \"\"\"Common helper function to get the name a given kubernetes resource should\n    use when deployed to the cluster.\n\n    Args:\n        resource_name:  str\n            The raw name for the resource (e.g. sireg-secret)\n        component:  Union[Component, str]\n            The component (or component name) that owns this resource\n        session:  Session\n            The session for the current reconciliation deploy\n\n    Returns:\n        resource_cluster_name:  str\n            The resource name with appropriate scoping and truncation added\n    \"\"\"\n    if is_global(component, session):\n        log.debug2(\n            \"Applying global name logic to [%s] for component [%s]\",\n            resource_name,\n            component,\n        )\n        return session.get_truncated_name(resource_name)\n    return session.get_scoped_name(resource_name)\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.common.get_slot_name","title":"<code>get_slot_name(component, session)</code>","text":"<p>Get the slot name for the given component in the current deployment</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>Union[Component, str]</code> <p>Union[Component, str] The component to fetch the slot name for</p> required <code>session</code> <code>Session</code> <p>DeploySession The session for the current deployment</p> required <p>Returns:</p> Name Type Description <code>slot_name</code> <code>str</code> <p>str The string name of the slot where the given component will live for this deployment. For global components, the static global slot name is returned</p> Source code in <code>oper8/x/utils/common.py</code> <pre><code>def get_slot_name(component: Union[Component, str], session: Session) -&gt; str:\n    \"\"\"Get the slot name for the given component in the current deployment\n\n    Args:\n        component:  Union[Component, str]\n            The component to fetch the slot name for\n        session:  DeploySession\n            The session for the current deployment\n\n    Returns:\n        slot_name:  str\n            The string name of the slot where the given component will live for\n            this deployment. For global components, the static global slot name\n            is returned\n    \"\"\"\n    if not is_global(component, session):\n        return session.name\n    return constants.GLOBAL_SLOT\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.common.is_global","title":"<code>is_global(component, session)</code>","text":"<p>Determine if the given component is global in this deployment</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>Union[Component, str]</code> <p>Union[Component, str] The component to fetch the slot name for</p> required <code>session</code> <code>Session</code> <p>Session The session for the current deployment</p> required <p>Returns:</p> Name Type Description <code>is_global</code> <code>bool</code> <p>bool True if the given component is global!</p> Source code in <code>oper8/x/utils/common.py</code> <pre><code>def is_global(component: Union[Component, str], session: Session) -&gt; bool:\n    \"\"\"Determine if the given component is global in this deployment\n\n    Args:\n        component:  Union[Component, str]\n            The component to fetch the slot name for\n        session:  Session\n            The session for the current deployment\n\n    Returns:\n        is_global:  bool\n            True if the given component is global!\n    \"\"\"\n    component_name = component if isinstance(component, str) else component.name\n    return session.config.get(component_name, {}).get(constants.GLOBAL_SLOT, False)\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.common.metadata_defaults","title":"<code>metadata_defaults(cluster_name, session, **kwargs)</code>","text":"<p>This function will create the metadata object given the external name for a resource. The external name should be created using common.get_resource_external_name. These functions are separate because the external name is often needed independently, so it will be pre-computed at the start of most components.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_name</code> <code>str</code> <p>str The fully scoped and truncated name that the resource will use in the cluster (metadata.name)</p> required <code>session</code> <code>Session</code> <p>DeploySession The session for the current reconciliation deploy</p> required <p>Returns:</p> Name Type Description <code>metadata</code> <code>dict</code> <p>dict The constructed metadata dict</p> Source code in <code>oper8/x/utils/common.py</code> <pre><code>def metadata_defaults(\n    cluster_name: str,\n    session: Session,\n    **kwargs,\n) -&gt; dict:\n    \"\"\"This function will create the metadata object given the external name for\n    a resource. The external name should be created using\n    common.get_resource_external_name. These functions are separate because the\n    external name is often needed independently, so it will be pre-computed at\n    the start of most components.\n\n    Args:\n        cluster_name:  str\n            The fully scoped and truncated name that the resource will use in\n            the cluster (metadata.name)\n        session:  DeploySession\n            The session for the current reconciliation deploy\n\n    Returns:\n        metadata:  dict\n            The constructed metadata dict\n    \"\"\"\n    # NOTE: For the time being, there are no defaults injected here, but we will\n    #   retain the abstraction function so that we can add defaulting\n    #   functionality (e.g. multi-namespace deployments) without touching every\n    #   file.\n    return {\"name\": cluster_name, **kwargs}\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.common.mount_mode","title":"<code>mount_mode(octal_val)</code>","text":"<p>This helper gets the decimal version of an octal representation of file permissions used for a volume mount.</p> <p>Parameters:</p> Name Type Description Default <code>octal_val</code> <p>int or str The number as octal (e.g. 755 or \"0755\")</p> required <p>Returns:</p> Name Type Description <code>decimal_val</code> <p>int The decimal integer value corresponding to the given octal value which can be used in VolumeMount's default_mode field</p> Source code in <code>oper8/x/utils/common.py</code> <pre><code>def mount_mode(octal_val):\n    \"\"\"This helper gets the decimal version of an octal representation of file\n    permissions used for a volume mount.\n\n    Args:\n        octal_val:  int or str\n            The number as octal (e.g. 755 or \"0755\")\n\n    Returns:\n        decimal_val:  int\n            The decimal integer value corresponding to the given octal value\n            which can be used in VolumeMount's default_mode field\n    \"\"\"\n    return int(str(octal_val), 8)\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.constants","title":"<code>constants</code>","text":"<p>Shared constants across the various oper8.x tools</p>"},{"location":"API%20References/#oper8.x.utils.deps_annotation","title":"<code>deps_annotation</code>","text":"<p>This module holds shared functionality for adding dependency annotations to all resources that need them.</p> <p>A dependency annotation on a Pod encodes a unique hash of the set of data-resources that the Pod depends on. For example, if a Pod mounds a Secret and a ConfigMap, the dependency annotation will hold a unique hash of the data content of these secrets. The role of the dependency annotation is to force a rollover when upstream data-resources change their content so that the content is guaranteed to be picked up by the consuming Pod.</p>"},{"location":"API%20References/#oper8.x.utils.deps_annotation.add_deps_annotation","title":"<code>add_deps_annotation(component, session, resource_definition)</code>","text":"<p>Add the dependency hash annotation to any pods found in the given object</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>Component</code> <p>Component The component that this resource belongs to</p> required <code>session</code> <code>Session</code> <p>Session The session for this deploy</p> required <code>resource_definition</code> <code>dict</code> <p>dict The dict representation of the resource to modify</p> required <p>Returns:</p> Name Type Description <code>resource_definition</code> <code>dict</code> <p>dict The dict representation of the resource with any modifications applied</p> Source code in <code>oper8/x/utils/deps_annotation.py</code> <pre><code>@alog.logged_function(log.debug)\ndef add_deps_annotation(\n    component: Component,\n    session: Session,\n    resource_definition: dict,\n) -&gt; dict:\n    \"\"\"Add the dependency hash annotation to any pods found in the given object\n\n    Args:\n        component:  Component\n            The component that this resource belongs to\n        session:  Session\n            The session for this deploy\n        resource_definition:  dict\n            The dict representation of the resource to modify\n\n    Returns:\n        resource_definition:  dict\n            The dict representation of the resource with any modifications\n            applied\n    \"\"\"\n    resource_name = \"{}/{}\".format(\n        resource_definition.get(\"kind\"),\n        resource_definition.get(\"metadata\", {}).get(\"name\"),\n    )\n\n    # Look for any/all pod annotations\n    pod = _find_pod(resource_definition)\n    if pod is not None:\n        log.debug2(\"Found Pod for [%s]\", resource_name)\n        log.debug4(pod)\n\n        # Traverse through and look for anything that looks like a secret or\n        # configmap reference\n        deps_map = _find_pod_data_deps(pod)\n        log.debug3(\"Deps Map: %s\", deps_map)\n        if deps_map:\n            # Go through each dependency and determine if it needs to be fetched\n            # of if it's part of the owning component\n            deps_list = []\n            for dep_kind, dep_names in deps_map.items():\n                for dep_name in dep_names:\n                    # Look for this object in the objects managed by this\n                    # component.\n                    #\n                    # NOTE: This will only be the components which have been\n                    #   declared earlier in the chart or have explicitly been\n                    #   marked as upstreams of this object.\n                    found_in_component = False\n                    for obj in component.managed_objects:\n                        log.debug4(\"Checking %s/%s\", obj.kind, obj.name)\n                        if obj.kind == dep_kind and obj.name == dep_name:\n                            log.debug3(\n                                \"Found intra-chart dependency of %s: %s\",\n                                resource_name,\n                                obj,\n                            )\n                            deps_list.append(obj.definition)\n                            found_in_component = True\n                            break\n\n                    # If not found in the component, add it as a lookup\n                    if not found_in_component:\n                        log.debug3(\n                            \"Found extra-chart dependency of %s: %s/%s\",\n                            resource_name,\n                            dep_kind,\n                            dep_name,\n                        )\n                        deps_list.append((dep_kind, dep_name))\n\n            # Add the annotation with the full list\n            md = pod.setdefault(\"metadata\", {})\n            annos = md.setdefault(\"annotations\", {})\n            md[\"annotations\"] = merge_configs(\n                annos, get_deps_annotation(session, deps_list, resource_name)\n            )\n\n    log.debug4(\"Updated Definition of [%s]: %s\", resource_name, resource_definition)\n    return resource_definition\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.deps_annotation.get_deps_annotation","title":"<code>get_deps_annotation(session, dependencies, resource_name='', namespace=_SESSION_NAMESPACE)</code>","text":"<p>Get a dict holding an annotation key/value pair representing the unique content hash of all given dependencies. This can be used to force pods to roll over when a dependency such as a ConfigMap or Secret changes its content. This function supports two ways of fetching dependency content:</p> <ol> <li>Dict representation of the object</li> <li>Tuple of the scoped (kind, name) for the object</li> </ol> <p>Additionally, this function holds special logic for ConfigMap and Secret dependencies, but can handle arbitrary kinds. For kinds without special logic, the full dict representation is used to compute the hash.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current session</p> required <code>dependencies</code> <code>List[Union[dict, Tuple[str, str]]]</code> <p>list(dict or str or cdk8s.ApiObject) An ordered list of dependencies to compute the content hash from</p> required <code>resource_name</code> <code>str</code> <p>str A string name for the resource (used for logging)</p> <code>''</code> <code>namespace</code> <code>Optional[str]</code> <p>Optional[str] Namespace where the dependencies live. Defaults to session.namespace</p> <code>_SESSION_NAMESPACE</code> <p>Returns:</p> Name Type Description <code>deps_annotation</code> <code>dict</code> <p>dict A dict representation of the key/value pair used to hold the content hash for the given set of dependencies</p> Source code in <code>oper8/x/utils/deps_annotation.py</code> <pre><code>def get_deps_annotation(\n    session: Session,\n    dependencies: List[Union[dict, Tuple[str, str]]],\n    resource_name: str = \"\",\n    namespace: Optional[str] = _SESSION_NAMESPACE,\n) -&gt; dict:\n    \"\"\"Get a dict holding an annotation key/value pair representing the unique\n    content hash of all given dependencies. This can be used to force pods to\n    roll over when a dependency such as a ConfigMap or Secret changes its\n    content. This function supports two ways of fetching dependency content:\n\n    1. Dict representation of the object\n    2. Tuple of the scoped (kind, name) for the object\n\n    Additionally, this function holds special logic for ConfigMap and Secret\n    dependencies, but can handle arbitrary kinds. For kinds without special\n    logic, the full dict representation is used to compute the hash.\n\n    Args:\n        session:  Session\n            The current session\n        dependencies:  list(dict or str or cdk8s.ApiObject)\n            An ordered list of dependencies to compute the content hash from\n        resource_name:  str\n            A string name for the resource (used for logging)\n        namespace:  Optional[str]\n            Namespace where the dependencies live. Defaults to session.namespace\n\n    Returns:\n        deps_annotation:  dict\n            A dict representation of the key/value pair used to hold the content\n            hash for the given set of dependencies\n    \"\"\"\n    content_hash = hashlib.sha1()\n    namespace = namespace if namespace != _SESSION_NAMESPACE else session.namespace\n    for dep in dependencies:\n        # Get the dict representation depending on what type this is\n        if isinstance(dep, tuple):\n            log.debug3(\"[%s] Handling tuple dependency: %s\", resource_name, dep)\n            assert len(dep) == 2, f\"Invalid dependency tuple given: {dep}\"\n            kind, name = dep\n            success, dep_dict = session.get_object_current_state(\n                name=name,\n                kind=kind,\n                namespace=namespace,\n            )\n            assert success, f\"Failed to fetch current state of {kind}/{name}\"\n\n            # There are several reasons that the upstream dependency would not\n            # be found, some legitimate and some not:\n            #\n            # 1. The dependency is not managed by this operator and this is a\n            #   dry run. This can't be solved since we don't have control over\n            #   the state of the cluster in dry run.\n            #\n            # 2. The dependency is part of a cyclic dependency between\n            #   Components. While a sign of something bad, this is ultimately\n            #   something that needs to be solved by decoupling the Component\n            #   dependencies.\n            #\n            # 3. The upstream is an undeclared chart dependency. This is an\n            #   easily fixed bug in the component by adding the necessary\n            #   add_dependency() calls.\n            #\n            # 4. The upstream is part of an undeclared component dependency.\n            #   This is an easily fixed bug in the parent Application by adding\n            #   the missing add_component_dependency() calls.\n            #\n            # Since some of these are things that should be quickly fixed, but\n            # some are signs of larger systemic problems, we warn and move on.\n            # For (1), these external dependencies should be present in the\n            # cluster. For the rest, once the deploy completes for the coupled\n            # components, the resources will show up and the next reconcile will\n            # cause the hash to change to what it should be.\n            if dep_dict is None:\n                log.warning(\n                    \"Working around missing external data dependency for [%s]: %s/%s\",\n                    resource_name,\n                    kind,\n                    name,\n                )\n                continue\n        else:\n            log.debug3(\n                \"[%s] Handling dict dependency: %s\",\n                resource_name,\n                dep.get(\"metadata\", {}).get(\"name\"),\n            )\n            assert isinstance(dep, dict), f\"Unknown dependency type: {type(dep)}\"\n            dep_dict = dep\n\n        # The hash should be unique to the name and kind\n        kind = dep_dict.get(\"kind\", \"\")\n        name = dep_dict.get(\"metadata\", {}).get(\"name\", \"\")\n        content_hash.update(kind.encode(\"utf-8\"))\n        content_hash.update(name.encode(\"utf-8\"))\n\n        # Compute the data hash based on any kind-specific logic\n        if kind in [\"Secret\", \"ConfigMap\"]:\n            log.debug2(\"Getting data hash for dep of kind %s\", kind)\n            data_dict = dep_dict.get(\"data\", {})\n        else:\n            log.debug2(\"Getting full hash for dep of kind %s\", kind)\n            data_dict = dep_dict\n        log.debug4(\"Data Dict: %s\", data_dict)\n\n        # Add to the overall hash\n        content_hash.update(json.dumps(data_dict, sort_keys=True).encode(\"utf-8\"))\n\n    # Return the annotation dict\n    final_hash = content_hash.hexdigest()\n    log.debug2(\"[%s] Final Hash: %s\", resource_name, final_hash)\n    return {DEPS_ANNOTATION: final_hash}\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.tls","title":"<code>tls</code>","text":"<p>Shared utilities for managing TLS keys and certs</p>"},{"location":"API%20References/#oper8.x.utils.tls.generate_ca_cert","title":"<code>generate_ca_cert(key, encode=True)</code>","text":"<p>Generate a Certificate Authority certificate based on a private key</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>RSAPrivateKey The private key that will pair with this CA cert</p> required <code>encode</code> <p>bool Base64 encode the output pem bytes</p> <code>True</code> <p>Returns:</p> Name Type Description <code>ca</code> <p>str The PEM encoded string for this CA cert</p> Source code in <code>oper8/x/utils/tls.py</code> <pre><code>def generate_ca_cert(key, encode=True):\n    \"\"\"Generate a Certificate Authority certificate based on a private key\n\n    Args:\n        key:  RSAPrivateKey\n            The private key that will pair with this CA cert\n        encode:  bool\n            Base64 encode the output pem bytes\n\n    Returns:\n        ca:  str\n            The PEM encoded string for this CA cert\n    \"\"\"\n\n    # Create self-signed CA\n    # The specifics of the extensions that are required for the CA were gleaned\n    # from the etcd operator example found here:\n    # https://github.com/openshift/etcd-ha-operator/blob/master/roles/tls_certs/templates/ca_crt_conf.j2\n    log.debug(\"Creating CA\")\n    subject = get_subject()\n    ca = (\n        x509.CertificateBuilder()\n        .subject_name(subject)\n        .issuer_name(subject)\n        .public_key(key.public_key())\n        .serial_number(x509.random_serial_number())\n        .not_valid_before(datetime.datetime.utcnow())\n        .not_valid_after(\n            # Our certificate will be valid for 10000 days\n            datetime.datetime.utcnow()\n            + datetime.timedelta(days=10000)\n        )\n        .add_extension(\n            # X509v3 Basic Constraints: critical\n            #     CA:TRUE\n            x509.BasicConstraints(ca=True, path_length=None),\n            critical=True,\n        )\n        .add_extension(\n            # X509v3 Key Usage: critical\n            #     Digital Signature, Key Encipherment, Certificate Sign\n            x509.KeyUsage(\n                digital_signature=True,\n                content_commitment=False,\n                key_encipherment=True,\n                data_encipherment=False,\n                key_agreement=False,\n                key_cert_sign=True,\n                crl_sign=False,\n                encipher_only=False,\n                decipher_only=False,\n            ),\n            critical=True,\n        )\n        .sign(key, hashes.SHA256(), default_backend())\n    )\n\n    cert_pem = ca.public_bytes(Encoding.PEM)\n    return (base64.b64encode(cert_pem) if encode else cert_pem).decode(\"utf-8\")\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.tls.generate_derived_key_cert_pair","title":"<code>generate_derived_key_cert_pair(ca_key, san_list, encode=True, key_cert_sign=False)</code>","text":"<p>Generate a certificate for use in encrypting TLS traffic, derived from a common key</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>RSAPrivateKey The private key that will pair with this CA cert</p> required <code>san_list</code> <p>list(str) List of strings to use for the Subject Alternate Name</p> required <code>encode</code> <p>bool Whether or not to base64 encode the output pem strings</p> <code>True</code> <code>key_cert_sign</code> <p>bool Whether or not to set the key_cert_sign usage bit in the generated certificate. This may be needed when the derived key/cert will be used as an intermediate CA or expected to act as a self-signed CA. Reference: https://ldapwiki.com/wiki/KeyUsage</p> <code>False</code> <p>Returns:</p> Name Type Description <code>key_pem</code> <p>str The pem-encoded key (base64 encoded if encode set)</p> <code>crt_pem</code> <p>str The pem-encoded cert (base64 encoded if encode set)</p> Source code in <code>oper8/x/utils/tls.py</code> <pre><code>def generate_derived_key_cert_pair(ca_key, san_list, encode=True, key_cert_sign=False):\n    \"\"\"Generate a certificate for use in encrypting TLS traffic, derived from\n    a common key\n\n    Args:\n        key:  RSAPrivateKey\n            The private key that will pair with this CA cert\n        san_list:  list(str)\n            List of strings to use for the Subject Alternate Name\n        encode:  bool\n            Whether or not to base64 encode the output pem strings\n        key_cert_sign:  bool\n            Whether or not to set the key_cert_sign usage bit in the generated certificate.\n            This may be needed when the derived key/cert will be used as an intermediate CA\n            or expected to act as a self-signed CA.\n            Reference: https://ldapwiki.com/wiki/KeyUsage\n\n    Returns:\n        key_pem:  str\n            The pem-encoded key (base64 encoded if encode set)\n        crt_pem:  str\n            The pem-encoded cert (base64 encoded if encode set)\n    \"\"\"\n\n    # Create a new private key for the server\n    key, key_pem = generate_key(encode=encode)\n\n    # Create the server certificate as if using a CSR. The final key will be\n    # signed by the CA private key, but will have the public key from the\n    # server's key.\n    #\n    # NOTE: It is not legal to use an identical Common Name for both the CA and\n    #   the derived certificate. With openssl 1.1.1k, this results in an invalid\n    #   certificate that fails with \"self signed certificate.\"\n    #   CITE: https://stackoverflow.com/a/19738223\n    cert = (\n        x509.CertificateBuilder()\n        .subject_name(get_subject(f\"{DEFAULT_COMMON_NAME}.server\"))\n        .issuer_name(get_subject())\n        .public_key(key.public_key())\n        .serial_number(x509.random_serial_number())\n        .not_valid_before(datetime.datetime.utcnow())\n        .not_valid_after(\n            # Our certificate will be valid for 10000 days\n            datetime.datetime.utcnow()\n            + datetime.timedelta(days=10000)\n        )\n        .add_extension(\n            x509.SubjectAlternativeName([x509.DNSName(san) for san in san_list]),\n            critical=False,\n        )\n        .add_extension(\n            # X509v3 Key Usage: critical\n            #     Digital Signature, Key Encipherment\n            x509.KeyUsage(\n                digital_signature=True,\n                content_commitment=False,\n                key_encipherment=True,\n                data_encipherment=False,\n                key_agreement=False,\n                key_cert_sign=key_cert_sign,\n                crl_sign=False,\n                encipher_only=False,\n                decipher_only=False,\n            ),\n            critical=True,\n        )\n        .add_extension(\n            # X509v3 Extended Key Usage:\n            #     TLS Web Client Authentication, TLS Web Server Authentication\n            x509.ExtendedKeyUsage(\n                [ExtendedKeyUsageOID.CLIENT_AUTH, ExtendedKeyUsageOID.SERVER_AUTH]\n            ),\n            critical=False,\n        )\n        .sign(ca_key, hashes.SHA256(), default_backend())\n    )\n\n    crt_pem = cert.public_bytes(Encoding.PEM)\n    return (key_pem, (base64.b64encode(crt_pem) if encode else crt_pem).decode(\"utf-8\"))\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.tls.generate_key","title":"<code>generate_key(encode=True)</code>","text":"<p>Generate a new RSA key for use when generating TLS components</p> <p>Parameters:</p> Name Type Description Default <code>encode</code> <p>bool Base64 encode the output pem bytes</p> <code>True</code> <p>Returns:</p> Name Type Description <code>key</code> <p>RSAPrivateKey The key object that can be used to sign certificates</p> <code>key_pem</code> <p>str The PEM encoded string for the key</p> Source code in <code>oper8/x/utils/tls.py</code> <pre><code>def generate_key(encode=True):\n    \"\"\"Generate a new RSA key for use when generating TLS components\n\n    Args:\n        encode:  bool\n            Base64 encode the output pem bytes\n\n    Returns:\n        key:  RSAPrivateKey\n            The key object that can be used to sign certificates\n        key_pem:  str\n            The PEM encoded string for the key\n    \"\"\"\n    key = rsa.generate_private_key(\n        public_exponent=65537, key_size=2048, backend=default_backend()\n    )\n    key_pem = key.private_bytes(\n        Encoding.PEM,\n        PrivateFormat.PKCS8,\n        encryption_algorithm=serialization.NoEncryption(),\n    )\n    return (key, (base64.b64encode(key_pem) if encode else key_pem).decode(\"utf-8\"))\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.tls.get_subject","title":"<code>get_subject(common_name=DEFAULT_COMMON_NAME)</code>","text":"<p>Get the subject object used when creating self-signed certificates. This will be consistent across all components, but will be tailored to the domain of the cluster.</p> <p>Parameters:</p> Name Type Description Default <code>common_name</code> <code>str</code> <p>str The Common Name to use for this subject</p> <code>DEFAULT_COMMON_NAME</code> <p>Returns:</p> Name Type Description <code>subject</code> <code>Name</code> <p>x509.Name The full subect object to use when constructing certificates</p> Source code in <code>oper8/x/utils/tls.py</code> <pre><code>def get_subject(common_name: str = DEFAULT_COMMON_NAME) -&gt; x509.Name:\n    \"\"\"Get the subject object used when creating self-signed certificates. This\n    will be consistent across all components, but will be tailored to the domain\n    of the cluster.\n\n    Args:\n        common_name:  str\n            The Common Name to use for this subject\n\n    Returns:\n        subject:  x509.Name\n            The full subect object to use when constructing certificates\n    \"\"\"\n    return x509.Name(\n        [\n            x509.NameAttribute(NameOID.COMMON_NAME, common_name),\n        ]\n    )\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.tls.parse_private_key_pem","title":"<code>parse_private_key_pem(key_pem)</code>","text":"<p>Parse the content of a pem-encoded private key file into an RSAPrivateKey</p> <p>Parameters:</p> Name Type Description Default <code>key_pem</code> <p>str The pem-encoded key (not base64 encoded)</p> required <p>Returns:</p> Name Type Description <code>key</code> <p>RSAPrivateKey The parsed key object which can be used for signing certs</p> Source code in <code>oper8/x/utils/tls.py</code> <pre><code>def parse_private_key_pem(key_pem):\n    \"\"\"Parse the content of a pem-encoded private key file into an RSAPrivateKey\n\n    Args:\n        key_pem:  str\n            The pem-encoded key (not base64 encoded)\n\n    Returns:\n        key:  RSAPrivateKey\n            The parsed key object which can be used for signing certs\n    \"\"\"\n    return serialization.load_pem_private_key(key_pem.encode(\"utf-8\"), None)\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.tls.parse_public_key_pem_from_cert","title":"<code>parse_public_key_pem_from_cert(cert_pem)</code>","text":"<p>Extract the pem-encoded public key from a pem-encoded</p> <p>Parameters:</p> Name Type Description Default <code>cert_pem</code> <p>str The pem-encoded certificate (not base64 encoded)</p> required <p>Returns:</p> Name Type Description <code>key</code> <p>RSAPrivateKey The parsed key object which can be used for signing certs</p> Source code in <code>oper8/x/utils/tls.py</code> <pre><code>def parse_public_key_pem_from_cert(cert_pem):\n    \"\"\"Extract the pem-encoded public key from a pem-encoded\n\n    Args:\n        cert_pem:  str\n            The pem-encoded certificate (not base64 encoded)\n\n    Returns:\n        key:  RSAPrivateKey\n            The parsed key object which can be used for signing certs\n    \"\"\"\n    return (\n        x509.load_pem_x509_certificate(cert_pem.encode(\"utf-8\"))\n        .public_key()\n        .public_bytes(\n            serialization.Encoding.PEM, serialization.PublicFormat.SubjectPublicKeyInfo\n        )\n        .decode(\"utf-8\")\n    )\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.tls_context","title":"<code>tls_context</code>","text":"<p>Common tls_context module setup</p>"},{"location":"API%20References/#oper8.x.utils.tls_context.factory","title":"<code>factory</code>","text":"<p>This module implements a factory for TlsContext implementations</p>"},{"location":"API%20References/#oper8.x.utils.tls_context.factory.get_tls_context","title":"<code>get_tls_context(session, config_overrides=None)</code>","text":"<p>Get an instance of the configured implementation of the tls context</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current deploy session</p> required <code>config_overrides</code> <code>Optional[dict]</code> <p>Optional[dict] Optional runtime config values. These will overwrite any values pulled from the session.config</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tls_context</code> <code>ITlsContext</code> <p>ITlsContext The constructed instance of the context</p> Source code in <code>oper8/x/utils/tls_context/factory.py</code> <pre><code>def get_tls_context(\n    session: Session,\n    config_overrides: Optional[dict] = None,\n) -&gt; ITlsContext:\n    \"\"\"Get an instance of the configured implementation of the tls context\n\n    Args:\n        session:  Session\n            The current deploy session\n        config_overrides:  Optional[dict]\n            Optional runtime config values. These will overwrite any values\n            pulled from the session.config\n\n    Returns:\n        tls_context:  ITlsContext\n            The constructed instance of the context\n    \"\"\"\n    return _TlsContextSingletonFactory.get_tls_context(\n        session,\n        config_overrides=config_overrides,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.tls_context.factory.register_tls_context_type","title":"<code>register_tls_context_type(context_class)</code>","text":"<p>Register a constructor for a given context implementation type</p> <p>Parameters:</p> Name Type Description Default <code>context_class</code> <code>Type[ITlsContext]</code> <p>Type[ITlsContext] The ITlsContext child class to register</p> required Source code in <code>oper8/x/utils/tls_context/factory.py</code> <pre><code>def register_tls_context_type(context_class: Type[ITlsContext]):\n    \"\"\"Register a constructor for a given context implementation type\n\n    Args:\n        context_class:  Type[ITlsContext]\n            The ITlsContext child class to register\n    \"\"\"\n    _TlsContextSingletonFactory.register(context_class)\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.tls_context.interface","title":"<code>interface</code>","text":"<p>This module defines the interface needed to provide TLS key/cert pairs to a given microservice and fetch a client-side certificate for making calls to a microservice that serves a key/cert pair derived from this context.</p>"},{"location":"API%20References/#oper8.x.utils.tls_context.interface.ITlsContext","title":"<code>ITlsContext</code>","text":"<p>               Bases: <code>ABC</code></p> <p>This interface encapsulates the management of TLS for a running instance of the operand. It encapsulates the following functions:</p> <ul> <li>Manage a CA key/cert pair for signing derived microservice certificates</li> <li>Create derived key/cert pairs for individual microservices</li> </ul> Source code in <code>oper8/x/utils/tls_context/interface.py</code> <pre><code>class ITlsContext(abc.ABC):\n    \"\"\"This interface encapsulates the management of TLS for a running instance\n    of the operand. It encapsulates the following functions:\n\n    * Manage a CA key/cert pair for signing derived microservice certificates\n    * Create derived key/cert pairs for individual microservices\n    \"\"\"\n\n    # The string constant that will be used by derived classes to define the\n    # type label string\n    _TYPE_LABEL_ATTRIBUTE = \"TYPE_LABEL\"\n\n    ## Construction ############################################################\n\n    def __init__(self, session: Session, config: aconfig.Config):\n        \"\"\"Construct with the current session so that member functions do not\n        take it as an argument\n\n        Args:\n            session:  Session\n                The current deploy session\n            config:  aconfig.Config\n                The config for this instance\n        \"\"\"\n        self._session = session\n        self._config = config\n\n    @property\n    def session(self) -&gt; Session:\n        return self._session\n\n    @property\n    def config(self) -&gt; aconfig.Config:\n        return self._config\n\n    ## Interface ###############################################################\n\n    def request_server_key_cert_pair(  # noqa: B027\n        self,\n        server_component: Component,\n        san_hostnames_list: List[str],\n        san_ip_list: List[str] = None,\n        key_name: str = None,\n        intermediate_ca: bool = False,\n    ) -&gt; None:\n        \"\"\"Request creation of the PEM encoded value of the key/cert pair for a\n        given server. This function has to be called from before render_chart is\n        called. I.e., parse_config / Component constructor phase.\n        Implementations of this function will generate the pair (in background)\n        if it has not been already requested.\n\n        Args:\n            server_component:  Component\n                The Component that manages the server. This can be used to add\n                a new Component if needed that will manage the resource for the\n                derived content and configure dependencies.\n            san_hostnames_list:  List[str]\n                The list of Subject Alternate Names (hostnames only)\n            san_ip_list:  List[str]\n                The list of Subject Alternate Names (ip addresses only, IPv4,\n                IPv6)\n            key_name:  str\n                In case component requires multiple certificates. The key_name\n                is used to distinguishes between component cert requests.\n            intermediate_ca:  bool\n                Whether or not to configure the certificate for use as an\n                intermediate CA. This implies setting the key_cert_sign usage\n                bit in the generated cert.\n                Reference: https://ldapwiki.com/wiki/KeyUsage\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_server_key_cert_pair(\n        self,\n        server_component: Component,\n        key_name: str = None,\n        encode: bool = True,\n        existing_key_pem: str = None,\n        existing_cert_pem: str = None,\n    ) -&gt; Tuple[str, str]:\n        \"\"\"Get the PEM encoded value of the key/cert pair for a given server.\n        You have to forst request_server_key_cert_pair in render_config phase,\n         and later in render_chart retrieve generated cert.\n\n        Args:\n            server_component:  Component\n                The Component that manages the server. This can be used to add\n                a new Component if needed that will manage the resource for the\n                derived content and configure dependencies.\n            key_name:  str\n                In case component requires multiple certificates. The key_name\n                is used to distinguies between component cert requests.\n            encode:  bool\n                Whether or not to base64 encode the output pem strings\n            existing_key_pem: str\n                Optionaly, you may provide the (decoded) value of PK/CERK pair.\n                TLS context is free to check the Cert/PK and return this pair or\n                generate new one.\n            existing_cert_pem: str,\n                Optionaly, you may provide the (decoded) value of PK/CERK pair.\n                TLS context is free to check the Cert/PK and return this pair or\n                generate new one.\n        Returns:\n            key_pem:  Optional[str]\n                This is the pem-encoded key content (base64\n                encoded if encode is set)\n            cert_pem:  Optional[str]\n                This is the pem-encoded cert content (base64\n                encoded if encode is set)\n        \"\"\"\n\n    @abc.abstractmethod\n    def get_client_cert(\n        self,\n        client_component: Component,\n        encode: bool = True,\n    ) -&gt; str:\n        \"\"\"Get a cert which can be used by a client to connect to a server which\n        is serving using a key/cert pair signed by the shared CA.\n\n        Args:\n            client_component:  Component\n                The Component that manages the client. This can be used to add\n                a new Component if needed that will manage the resource for the\n                derived content and configure dependencies.\n            encode:  bool\n                Whether or not to base64 encode the output pem strings\n\n        Returns:\n            crt_pem:  Optional[str]\n                The pem-encoded cert (base64 encoded if encode set).\n        \"\"\"\n</code></pre> <code>__init__(session, config)</code> <p>Construct with the current session so that member functions do not take it as an argument</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current deploy session</p> required <code>config</code> <code>Config</code> <p>aconfig.Config The config for this instance</p> required Source code in <code>oper8/x/utils/tls_context/interface.py</code> <pre><code>def __init__(self, session: Session, config: aconfig.Config):\n    \"\"\"Construct with the current session so that member functions do not\n    take it as an argument\n\n    Args:\n        session:  Session\n            The current deploy session\n        config:  aconfig.Config\n            The config for this instance\n    \"\"\"\n    self._session = session\n    self._config = config\n</code></pre> <code>get_client_cert(client_component, encode=True)</code> <code>abstractmethod</code> <p>Get a cert which can be used by a client to connect to a server which is serving using a key/cert pair signed by the shared CA.</p> <p>Parameters:</p> Name Type Description Default <code>client_component</code> <code>Component</code> <p>Component The Component that manages the client. This can be used to add a new Component if needed that will manage the resource for the derived content and configure dependencies.</p> required <code>encode</code> <code>bool</code> <p>bool Whether or not to base64 encode the output pem strings</p> <code>True</code> <p>Returns:</p> Name Type Description <code>crt_pem</code> <code>str</code> <p>Optional[str] The pem-encoded cert (base64 encoded if encode set).</p> Source code in <code>oper8/x/utils/tls_context/interface.py</code> <pre><code>@abc.abstractmethod\ndef get_client_cert(\n    self,\n    client_component: Component,\n    encode: bool = True,\n) -&gt; str:\n    \"\"\"Get a cert which can be used by a client to connect to a server which\n    is serving using a key/cert pair signed by the shared CA.\n\n    Args:\n        client_component:  Component\n            The Component that manages the client. This can be used to add\n            a new Component if needed that will manage the resource for the\n            derived content and configure dependencies.\n        encode:  bool\n            Whether or not to base64 encode the output pem strings\n\n    Returns:\n        crt_pem:  Optional[str]\n            The pem-encoded cert (base64 encoded if encode set).\n    \"\"\"\n</code></pre> <code>get_server_key_cert_pair(server_component, key_name=None, encode=True, existing_key_pem=None, existing_cert_pem=None)</code> <code>abstractmethod</code> <p>Get the PEM encoded value of the key/cert pair for a given server. You have to forst request_server_key_cert_pair in render_config phase,  and later in render_chart retrieve generated cert.</p> <p>Parameters:</p> Name Type Description Default <code>server_component</code> <code>Component</code> <p>Component The Component that manages the server. This can be used to add a new Component if needed that will manage the resource for the derived content and configure dependencies.</p> required <code>key_name</code> <code>str</code> <p>str In case component requires multiple certificates. The key_name is used to distinguies between component cert requests.</p> <code>None</code> <code>encode</code> <code>bool</code> <p>bool Whether or not to base64 encode the output pem strings</p> <code>True</code> <code>existing_key_pem</code> <code>str</code> <p>str Optionaly, you may provide the (decoded) value of PK/CERK pair. TLS context is free to check the Cert/PK and return this pair or generate new one.</p> <code>None</code> <code>existing_cert_pem</code> <code>str</code> <p>str, Optionaly, you may provide the (decoded) value of PK/CERK pair. TLS context is free to check the Cert/PK and return this pair or generate new one.</p> <code>None</code> <p>Returns:     key_pem:  Optional[str]         This is the pem-encoded key content (base64         encoded if encode is set)     cert_pem:  Optional[str]         This is the pem-encoded cert content (base64         encoded if encode is set)</p> Source code in <code>oper8/x/utils/tls_context/interface.py</code> <pre><code>@abc.abstractmethod\ndef get_server_key_cert_pair(\n    self,\n    server_component: Component,\n    key_name: str = None,\n    encode: bool = True,\n    existing_key_pem: str = None,\n    existing_cert_pem: str = None,\n) -&gt; Tuple[str, str]:\n    \"\"\"Get the PEM encoded value of the key/cert pair for a given server.\n    You have to forst request_server_key_cert_pair in render_config phase,\n     and later in render_chart retrieve generated cert.\n\n    Args:\n        server_component:  Component\n            The Component that manages the server. This can be used to add\n            a new Component if needed that will manage the resource for the\n            derived content and configure dependencies.\n        key_name:  str\n            In case component requires multiple certificates. The key_name\n            is used to distinguies between component cert requests.\n        encode:  bool\n            Whether or not to base64 encode the output pem strings\n        existing_key_pem: str\n            Optionaly, you may provide the (decoded) value of PK/CERK pair.\n            TLS context is free to check the Cert/PK and return this pair or\n            generate new one.\n        existing_cert_pem: str,\n            Optionaly, you may provide the (decoded) value of PK/CERK pair.\n            TLS context is free to check the Cert/PK and return this pair or\n            generate new one.\n    Returns:\n        key_pem:  Optional[str]\n            This is the pem-encoded key content (base64\n            encoded if encode is set)\n        cert_pem:  Optional[str]\n            This is the pem-encoded cert content (base64\n            encoded if encode is set)\n    \"\"\"\n</code></pre> <code>request_server_key_cert_pair(server_component, san_hostnames_list, san_ip_list=None, key_name=None, intermediate_ca=False)</code> <p>Request creation of the PEM encoded value of the key/cert pair for a given server. This function has to be called from before render_chart is called. I.e., parse_config / Component constructor phase. Implementations of this function will generate the pair (in background) if it has not been already requested.</p> <p>Parameters:</p> Name Type Description Default <code>server_component</code> <code>Component</code> <p>Component The Component that manages the server. This can be used to add a new Component if needed that will manage the resource for the derived content and configure dependencies.</p> required <code>san_hostnames_list</code> <code>List[str]</code> <p>List[str] The list of Subject Alternate Names (hostnames only)</p> required <code>san_ip_list</code> <code>List[str]</code> <p>List[str] The list of Subject Alternate Names (ip addresses only, IPv4, IPv6)</p> <code>None</code> <code>key_name</code> <code>str</code> <p>str In case component requires multiple certificates. The key_name is used to distinguishes between component cert requests.</p> <code>None</code> <code>intermediate_ca</code> <code>bool</code> <p>bool Whether or not to configure the certificate for use as an intermediate CA. This implies setting the key_cert_sign usage bit in the generated cert. Reference: https://ldapwiki.com/wiki/KeyUsage</p> <code>False</code> Source code in <code>oper8/x/utils/tls_context/interface.py</code> <pre><code>def request_server_key_cert_pair(  # noqa: B027\n    self,\n    server_component: Component,\n    san_hostnames_list: List[str],\n    san_ip_list: List[str] = None,\n    key_name: str = None,\n    intermediate_ca: bool = False,\n) -&gt; None:\n    \"\"\"Request creation of the PEM encoded value of the key/cert pair for a\n    given server. This function has to be called from before render_chart is\n    called. I.e., parse_config / Component constructor phase.\n    Implementations of this function will generate the pair (in background)\n    if it has not been already requested.\n\n    Args:\n        server_component:  Component\n            The Component that manages the server. This can be used to add\n            a new Component if needed that will manage the resource for the\n            derived content and configure dependencies.\n        san_hostnames_list:  List[str]\n            The list of Subject Alternate Names (hostnames only)\n        san_ip_list:  List[str]\n            The list of Subject Alternate Names (ip addresses only, IPv4,\n            IPv6)\n        key_name:  str\n            In case component requires multiple certificates. The key_name\n            is used to distinguishes between component cert requests.\n        intermediate_ca:  bool\n            Whether or not to configure the certificate for use as an\n            intermediate CA. This implies setting the key_cert_sign usage\n            bit in the generated cert.\n            Reference: https://ldapwiki.com/wiki/KeyUsage\n    \"\"\"\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.tls_context.internal","title":"<code>internal</code>","text":"<p>This implementation of the ITlsContext uses internal code to manage the tls context</p>"},{"location":"API%20References/#oper8.x.utils.tls_context.internal.InternalCaComponent","title":"<code>InternalCaComponent</code>","text":"<p>               Bases: <code>Oper8xComponent</code></p> <p>This Component will manage a single secret containing a CA key/cert pair</p> Source code in <code>oper8/x/utils/tls_context/internal.py</code> <pre><code>class InternalCaComponent(Oper8xComponent):\n    \"\"\"This Component will manage a single secret containing a CA key/cert pair\"\"\"\n\n    CA_SECRET_NAME = \"infra-tls-ca\"\n    CA_KEY_FILENAME = \"key.ca.pem\"\n    CA_CRT_FILENAME = \"crt.ca.pem\"\n\n    name = \"internal-tls\"\n\n    ## Component Interface #####################################################\n\n    def __init__(\n        self,\n        session: Session,\n        *args,\n        labels: Optional[dict] = None,\n        **kwargs,\n    ):\n        \"\"\"Construct the parent Component and set up internal data holders\"\"\"\n        super().__init__(*args, session=session, **kwargs)\n        self._ca_key_pem = None\n        self._ca_crt_pem = None\n\n        # Pull labels from config or use defaults\n        self._labels = labels\n\n    def build_chart(self, *args, **kwargs):\n        \"\"\"Implement delayed chart construction in build_chart\"\"\"\n\n        # Make sure the data values are populated\n        self._initialize_data()\n\n        # Get the labels to use for the secret\n        secret_cluster_name = self._get_secret_name()\n        labels = self._labels\n        if labels is None:\n            labels = common.get_labels(\n                cluster_name=secret_cluster_name,\n                session=self.session,\n                component_name=self.CA_SECRET_NAME,\n            )\n        log.debug(\"Creating internal CA secret: %s\", secret_cluster_name)\n        self.add_resource(\n            name=self.CA_SECRET_NAME,\n            obj=dict(\n                kind=\"Secret\",\n                apiVersion=\"v1\",\n                metadata=common.metadata_defaults(\n                    session=self.session,\n                    cluster_name=secret_cluster_name,\n                    labels=labels,\n                ),\n                data={\n                    self.CA_KEY_FILENAME: common.b64_secret(self._ca_key_pem),\n                    self.CA_CRT_FILENAME: common.b64_secret(self._ca_crt_pem),\n                },\n            ),\n        )\n\n    ## Public Utilities ########################################################\n\n    def get_ca_key_cert(self) -&gt; Tuple[str, str]:\n        \"\"\"Get the pem-encoded CA key cert pair\n\n        Returns:\n            ca_key_pem:  str\n                The pem-encoded (not base64 encoded) secret key\n            ca_crt_pem:  str\n                The pem-encoded (not base64 encoded) secret cert\n        \"\"\"\n        self._initialize_data()\n        return self._ca_key_pem, self._ca_crt_pem\n\n    ## Implementation Details ##################################################\n\n    def _get_secret_name(self) -&gt; str:\n        \"\"\"Get the CA secret name with any scoping applied\"\"\"\n        return self.get_cluster_name(self.CA_SECRET_NAME)\n\n    def _initialize_data(self):\n        \"\"\"Initialize the data if needed\"\"\"\n\n        # If this is the first time, actually do the init\n        if None in [self._ca_crt_pem, self._ca_key_pem]:\n            secret_cluster_name = self._get_secret_name()\n            log.debug2(\"Cluster TLS Secret Name: %s\", secret_cluster_name)\n            success, content = self.session.get_object_current_state(\n                kind=\"Secret\",\n                name=secret_cluster_name,\n            )\n            assert_cluster(\n                success, f\"Failed to check cluster for [{secret_cluster_name}]\"\n            )\n            if content is not None:\n                # Extract the pem strings\n                key_pem = content.get(\"data\", {}).get(self.CA_KEY_FILENAME)\n                crt_pem = content.get(\"data\", {}).get(self.CA_CRT_FILENAME)\n                if None in [key_pem, crt_pem]:\n                    log.warning(\n                        \"Found CA secret [%s] but content is invalid!\",\n                        secret_cluster_name,\n                    )\n                    self._generate()\n                else:\n                    log.debug(\"Found valid CA secret content.\")\n                    self._ca_key_pem = common.b64_secret_decode(key_pem)\n                    self._ca_crt_pem = common.b64_secret_decode(crt_pem)\n            else:\n                log.debug2(\"No existing CA secret found. Generating.\")\n                self._generate()\n\n    def _generate(self):\n        \"\"\"Generate a new CA\"\"\"\n        key, self._ca_key_pem = tls.generate_key(encode=False)\n        self._ca_crt_pem = tls.generate_ca_cert(key, encode=False)\n</code></pre> <code>__init__(session, *args, labels=None, **kwargs)</code> <p>Construct the parent Component and set up internal data holders</p> Source code in <code>oper8/x/utils/tls_context/internal.py</code> <pre><code>def __init__(\n    self,\n    session: Session,\n    *args,\n    labels: Optional[dict] = None,\n    **kwargs,\n):\n    \"\"\"Construct the parent Component and set up internal data holders\"\"\"\n    super().__init__(*args, session=session, **kwargs)\n    self._ca_key_pem = None\n    self._ca_crt_pem = None\n\n    # Pull labels from config or use defaults\n    self._labels = labels\n</code></pre> <code>build_chart(*args, **kwargs)</code> <p>Implement delayed chart construction in build_chart</p> Source code in <code>oper8/x/utils/tls_context/internal.py</code> <pre><code>def build_chart(self, *args, **kwargs):\n    \"\"\"Implement delayed chart construction in build_chart\"\"\"\n\n    # Make sure the data values are populated\n    self._initialize_data()\n\n    # Get the labels to use for the secret\n    secret_cluster_name = self._get_secret_name()\n    labels = self._labels\n    if labels is None:\n        labels = common.get_labels(\n            cluster_name=secret_cluster_name,\n            session=self.session,\n            component_name=self.CA_SECRET_NAME,\n        )\n    log.debug(\"Creating internal CA secret: %s\", secret_cluster_name)\n    self.add_resource(\n        name=self.CA_SECRET_NAME,\n        obj=dict(\n            kind=\"Secret\",\n            apiVersion=\"v1\",\n            metadata=common.metadata_defaults(\n                session=self.session,\n                cluster_name=secret_cluster_name,\n                labels=labels,\n            ),\n            data={\n                self.CA_KEY_FILENAME: common.b64_secret(self._ca_key_pem),\n                self.CA_CRT_FILENAME: common.b64_secret(self._ca_crt_pem),\n            },\n        ),\n    )\n</code></pre> <code>get_ca_key_cert()</code> <p>Get the pem-encoded CA key cert pair</p> <p>Returns:</p> Name Type Description <code>ca_key_pem</code> <code>str</code> <p>str The pem-encoded (not base64 encoded) secret key</p> <code>ca_crt_pem</code> <code>str</code> <p>str The pem-encoded (not base64 encoded) secret cert</p> Source code in <code>oper8/x/utils/tls_context/internal.py</code> <pre><code>def get_ca_key_cert(self) -&gt; Tuple[str, str]:\n    \"\"\"Get the pem-encoded CA key cert pair\n\n    Returns:\n        ca_key_pem:  str\n            The pem-encoded (not base64 encoded) secret key\n        ca_crt_pem:  str\n            The pem-encoded (not base64 encoded) secret cert\n    \"\"\"\n    self._initialize_data()\n    return self._ca_key_pem, self._ca_crt_pem\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.tls_context.internal.InternalTlsContext","title":"<code>InternalTlsContext</code>","text":"<p>               Bases: <code>ITlsContext</code></p> Source code in <code>oper8/x/utils/tls_context/internal.py</code> <pre><code>class InternalTlsContext(ITlsContext):\n    __doc__ = __doc__\n\n    TYPE_LABEL = \"internal\"\n\n    def __init__(self, session: Session, *args, **kwargs):\n        \"\"\"At construct time, this instance will add a Component to the session\n        which will manage the CA secret\n        \"\"\"\n        super().__init__(session, *args, **kwargs)\n\n        # Add the CA Component if it is not already present in the session.\n        # There is a pretty nasty condition here when running in standalone mode\n        # where the factory may attempt to recreate the singleton instance after\n        # a subsystem has overwritten it, so we need to check and see if there\n        # is a matching component in the session already\n        pre_existing_component = [\n            comp\n            for comp in session.get_components()\n            if comp.name == InternalCaComponent.name\n        ]\n        if pre_existing_component:\n            self._component = pre_existing_component[0]\n        else:\n            self._component = InternalCaComponent(\n                session=session, labels=self.config.labels\n            )\n\n        # Keep track of pairs for each server so that they are only generated\n        # once\n        self._server_pairs = {}\n\n    ## Interface ###############################################################\n    def request_server_key_cert_pair(\n        self,\n        server_component: Component,\n        san_hostnames_list: List[str],\n        san_ip_list: List[str] = None,\n        key_name: str = None,\n        intermediate_ca: bool = False,\n    ) -&gt; None:\n        \"\"\"Request creation of the PEM encoded value of the key/cert pair for a\n        given server. This function has to be called from before render_chart is\n        called. I.e., parse_config / Component constructor phase.\n        Implementations of this function will generate the pair (in background)\n        if it has not been already requested.\n\n        Args:\n            server_component:  Component\n                The Component that manages the server. This can be used to add\n                a new Component if needed that will manage the resource for the\n                derived content and configure dependencies.\n            san_list:  List[str]\n                The list of Subject Alternate Names\n            key_name:  str\n                In case component requires multiple certificates. The key_name\n                is used to distinguish between component cert requests.\n            intermediate_ca:  bool\n                Whether or not to configure the certificate for use as an\n                intermediate CA. This implies setting the key_cert_sign usage\n                bit in the generated cert.\n                Reference: https://ldapwiki.com/wiki/KeyUsage\n        \"\"\"\n        cache_key = server_component.name + (\n            \"-\" + key_name if key_name is not None else \"\"\n        )\n\n        if cache_key in self._server_pairs:\n            log.warning(\n                \"Certificate server key/cert pair for %s has been already \"\n                \"requested. Ignoring this request.\",\n                cache_key,\n            )\n            return\n\n        log.debug(\"Generating server key/cert pair for %s\", cache_key)\n\n        # Mark the server component as dependent on the internal component. This\n        # is not strictly necessary since values are consumed by value, but it\n        # makes sequential sense.\n        self.session.add_component_dependency(server_component, self._component)\n\n        # Get the CA's private key\n        ca_key = tls.parse_private_key_pem(self._component.get_ca_key_cert()[0])\n\n        # Generate the derived pair\n        san_list = (san_hostnames_list or []) + (san_ip_list or [])\n        self._server_pairs[cache_key] = tls.generate_derived_key_cert_pair(\n            ca_key=ca_key,\n            san_list=san_list,\n            encode=False,\n            key_cert_sign=intermediate_ca,\n        )\n\n    def get_server_key_cert_pair(\n        self,\n        server_component: Component,\n        key_name: str = None,\n        encode: bool = True,\n        existing_key_pem: str = None,\n        existing_cert_pem: str = None,\n    ) -&gt; Tuple[str, str]:\n        \"\"\"This function derives a server key/cert pair from the CA key/cert\n        managed by the internal component.\n\n        Args:\n            server_component:  Component\n                The Component that manages the server. This can be used to add\n                a new Component if needed that will manage the resource for the\n                derived content and configure dependencies.\n            key_name:  str\n                In case component requires multiple certificates. The key_name\n                is used to distinguish between component cert requests.\n            encode:  bool\n                Whether or not to base64 encode the output pem strings\n            existing_key_pem: str\n                If both existing key/cert are specified, then they are returned\n                immediately without any checks\n            existing_cert_pem: str\n                If both existing key/cert are specified, then they are returned\n                immediately without any checks\n         Returns:\n            key_pem:  str\n                This is the pem-encoded key content (base64\n                encoded if encode is set)\n            cert_pem:  str\n                This is the pem-encoded cert content (base64\n                encoded if encode is set)\n        \"\"\"\n        log.debug2(\"Getting server key/cert pair for %s\", server_component)\n        cache_key = server_component.name + (\n            \"-\" + key_name if key_name is not None else \"\"\n        )\n\n        assert (\n            cache_key in self._server_pairs\n        ), f\"Trying to obtain certificate {key_name} which was not previouly requested\"\n        if existing_key_pem is not None and existing_cert_pem is not None:\n            key_pem = existing_key_pem\n            cert_pem = existing_cert_pem\n        else:\n            # Return the stored pair for this server\n            (key_pem, cert_pem) = self._server_pairs[cache_key]\n        if encode:\n            return (common.b64_secret(key_pem), common.b64_secret(cert_pem))\n        return (key_pem, cert_pem)\n\n    def get_client_cert(\n        self,\n        client_component: Component,\n        encode: bool = True,\n    ) -&gt; str:\n        \"\"\"Get the CA's public cert\n\n        Args:\n            client_component:  Component\n                The Component that manages the client. This implementation does\n                not need the component.\n            encode:  bool\n                Whether or not to base64 encode the output pem strings\n\n        Returns:\n            crt_pem:  Optional[str]\n               The pem-encoded cert (base64 encoded if encode set)\n        \"\"\"\n        log.debug2(\"Getting client cert for %s\", client_component)\n        _, ca_crt = self._component.get_ca_key_cert()\n        if encode:\n            return common.b64_secret(ca_crt)\n        return ca_crt\n</code></pre> <code>__init__(session, *args, **kwargs)</code> <p>At construct time, this instance will add a Component to the session which will manage the CA secret</p> Source code in <code>oper8/x/utils/tls_context/internal.py</code> <pre><code>def __init__(self, session: Session, *args, **kwargs):\n    \"\"\"At construct time, this instance will add a Component to the session\n    which will manage the CA secret\n    \"\"\"\n    super().__init__(session, *args, **kwargs)\n\n    # Add the CA Component if it is not already present in the session.\n    # There is a pretty nasty condition here when running in standalone mode\n    # where the factory may attempt to recreate the singleton instance after\n    # a subsystem has overwritten it, so we need to check and see if there\n    # is a matching component in the session already\n    pre_existing_component = [\n        comp\n        for comp in session.get_components()\n        if comp.name == InternalCaComponent.name\n    ]\n    if pre_existing_component:\n        self._component = pre_existing_component[0]\n    else:\n        self._component = InternalCaComponent(\n            session=session, labels=self.config.labels\n        )\n\n    # Keep track of pairs for each server so that they are only generated\n    # once\n    self._server_pairs = {}\n</code></pre> <code>get_client_cert(client_component, encode=True)</code> <p>Get the CA's public cert</p> <p>Parameters:</p> Name Type Description Default <code>client_component</code> <code>Component</code> <p>Component The Component that manages the client. This implementation does not need the component.</p> required <code>encode</code> <code>bool</code> <p>bool Whether or not to base64 encode the output pem strings</p> <code>True</code> <p>Returns:</p> Name Type Description <code>crt_pem</code> <code>str</code> <p>Optional[str] The pem-encoded cert (base64 encoded if encode set)</p> Source code in <code>oper8/x/utils/tls_context/internal.py</code> <pre><code>def get_client_cert(\n    self,\n    client_component: Component,\n    encode: bool = True,\n) -&gt; str:\n    \"\"\"Get the CA's public cert\n\n    Args:\n        client_component:  Component\n            The Component that manages the client. This implementation does\n            not need the component.\n        encode:  bool\n            Whether or not to base64 encode the output pem strings\n\n    Returns:\n        crt_pem:  Optional[str]\n           The pem-encoded cert (base64 encoded if encode set)\n    \"\"\"\n    log.debug2(\"Getting client cert for %s\", client_component)\n    _, ca_crt = self._component.get_ca_key_cert()\n    if encode:\n        return common.b64_secret(ca_crt)\n    return ca_crt\n</code></pre> <code>get_server_key_cert_pair(server_component, key_name=None, encode=True, existing_key_pem=None, existing_cert_pem=None)</code> <p>This function derives a server key/cert pair from the CA key/cert managed by the internal component.</p> <p>Parameters:</p> Name Type Description Default <code>server_component</code> <code>Component</code> <p>Component The Component that manages the server. This can be used to add a new Component if needed that will manage the resource for the derived content and configure dependencies.</p> required <code>key_name</code> <code>str</code> <p>str In case component requires multiple certificates. The key_name is used to distinguish between component cert requests.</p> <code>None</code> <code>encode</code> <code>bool</code> <p>bool Whether or not to base64 encode the output pem strings</p> <code>True</code> <code>existing_key_pem</code> <code>str</code> <p>str If both existing key/cert are specified, then they are returned immediately without any checks</p> <code>None</code> <code>existing_cert_pem</code> <code>str</code> <p>str If both existing key/cert are specified, then they are returned immediately without any checks</p> <code>None</code> <p>Returns:     key_pem:  str         This is the pem-encoded key content (base64         encoded if encode is set)     cert_pem:  str         This is the pem-encoded cert content (base64         encoded if encode is set)</p> Source code in <code>oper8/x/utils/tls_context/internal.py</code> <pre><code>def get_server_key_cert_pair(\n    self,\n    server_component: Component,\n    key_name: str = None,\n    encode: bool = True,\n    existing_key_pem: str = None,\n    existing_cert_pem: str = None,\n) -&gt; Tuple[str, str]:\n    \"\"\"This function derives a server key/cert pair from the CA key/cert\n    managed by the internal component.\n\n    Args:\n        server_component:  Component\n            The Component that manages the server. This can be used to add\n            a new Component if needed that will manage the resource for the\n            derived content and configure dependencies.\n        key_name:  str\n            In case component requires multiple certificates. The key_name\n            is used to distinguish between component cert requests.\n        encode:  bool\n            Whether or not to base64 encode the output pem strings\n        existing_key_pem: str\n            If both existing key/cert are specified, then they are returned\n            immediately without any checks\n        existing_cert_pem: str\n            If both existing key/cert are specified, then they are returned\n            immediately without any checks\n     Returns:\n        key_pem:  str\n            This is the pem-encoded key content (base64\n            encoded if encode is set)\n        cert_pem:  str\n            This is the pem-encoded cert content (base64\n            encoded if encode is set)\n    \"\"\"\n    log.debug2(\"Getting server key/cert pair for %s\", server_component)\n    cache_key = server_component.name + (\n        \"-\" + key_name if key_name is not None else \"\"\n    )\n\n    assert (\n        cache_key in self._server_pairs\n    ), f\"Trying to obtain certificate {key_name} which was not previouly requested\"\n    if existing_key_pem is not None and existing_cert_pem is not None:\n        key_pem = existing_key_pem\n        cert_pem = existing_cert_pem\n    else:\n        # Return the stored pair for this server\n        (key_pem, cert_pem) = self._server_pairs[cache_key]\n    if encode:\n        return (common.b64_secret(key_pem), common.b64_secret(cert_pem))\n    return (key_pem, cert_pem)\n</code></pre> <code>request_server_key_cert_pair(server_component, san_hostnames_list, san_ip_list=None, key_name=None, intermediate_ca=False)</code> <p>Request creation of the PEM encoded value of the key/cert pair for a given server. This function has to be called from before render_chart is called. I.e., parse_config / Component constructor phase. Implementations of this function will generate the pair (in background) if it has not been already requested.</p> <p>Parameters:</p> Name Type Description Default <code>server_component</code> <code>Component</code> <p>Component The Component that manages the server. This can be used to add a new Component if needed that will manage the resource for the derived content and configure dependencies.</p> required <code>san_list</code> <p>List[str] The list of Subject Alternate Names</p> required <code>key_name</code> <code>str</code> <p>str In case component requires multiple certificates. The key_name is used to distinguish between component cert requests.</p> <code>None</code> <code>intermediate_ca</code> <code>bool</code> <p>bool Whether or not to configure the certificate for use as an intermediate CA. This implies setting the key_cert_sign usage bit in the generated cert. Reference: https://ldapwiki.com/wiki/KeyUsage</p> <code>False</code> Source code in <code>oper8/x/utils/tls_context/internal.py</code> <pre><code>def request_server_key_cert_pair(\n    self,\n    server_component: Component,\n    san_hostnames_list: List[str],\n    san_ip_list: List[str] = None,\n    key_name: str = None,\n    intermediate_ca: bool = False,\n) -&gt; None:\n    \"\"\"Request creation of the PEM encoded value of the key/cert pair for a\n    given server. This function has to be called from before render_chart is\n    called. I.e., parse_config / Component constructor phase.\n    Implementations of this function will generate the pair (in background)\n    if it has not been already requested.\n\n    Args:\n        server_component:  Component\n            The Component that manages the server. This can be used to add\n            a new Component if needed that will manage the resource for the\n            derived content and configure dependencies.\n        san_list:  List[str]\n            The list of Subject Alternate Names\n        key_name:  str\n            In case component requires multiple certificates. The key_name\n            is used to distinguish between component cert requests.\n        intermediate_ca:  bool\n            Whether or not to configure the certificate for use as an\n            intermediate CA. This implies setting the key_cert_sign usage\n            bit in the generated cert.\n            Reference: https://ldapwiki.com/wiki/KeyUsage\n    \"\"\"\n    cache_key = server_component.name + (\n        \"-\" + key_name if key_name is not None else \"\"\n    )\n\n    if cache_key in self._server_pairs:\n        log.warning(\n            \"Certificate server key/cert pair for %s has been already \"\n            \"requested. Ignoring this request.\",\n            cache_key,\n        )\n        return\n\n    log.debug(\"Generating server key/cert pair for %s\", cache_key)\n\n    # Mark the server component as dependent on the internal component. This\n    # is not strictly necessary since values are consumed by value, but it\n    # makes sequential sense.\n    self.session.add_component_dependency(server_component, self._component)\n\n    # Get the CA's private key\n    ca_key = tls.parse_private_key_pem(self._component.get_ca_key_cert()[0])\n\n    # Generate the derived pair\n    san_list = (san_hostnames_list or []) + (san_ip_list or [])\n    self._server_pairs[cache_key] = tls.generate_derived_key_cert_pair(\n        ca_key=ca_key,\n        san_list=san_list,\n        encode=False,\n        key_cert_sign=intermediate_ca,\n    )\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.tls_context.public","title":"<code>public</code>","text":"<p>This file holds functions that should be used outside of this module by components, subsystems, and applications that need access to the TLS context functionality.</p>"},{"location":"API%20References/#oper8.x.utils.tls_context.public.get_client_cert","title":"<code>get_client_cert(session, *args, **kwargs)</code>","text":"<p>Get the CA's public cert</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current deploy session</p> required Passthrough Args <p>See ITlsContext.get_client_cert</p> <p>Returns:</p> Name Type Description <code>crt_pem</code> <code>str</code> <p>Optional[str]                The pem-encoded cert (base64 encoded if encode set),</p> Source code in <code>oper8/x/utils/tls_context/public.py</code> <pre><code>def get_client_cert(\n    session: Session,\n    *args,\n    **kwargs,\n) -&gt; str:\n    \"\"\"Get the CA's public cert\n\n    Args:\n        session:  Session\n            The current deploy session\n\n    Passthrough Args:\n        See ITlsContext.get_client_cert\n\n    Returns:\n        crt_pem:  Optional[str]\n                           The pem-encoded cert (base64 encoded if encode set),\n    \"\"\"\n    return get_tls_context(session).get_client_cert(*args, **kwargs)\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.tls_context.public.get_server_key_cert_pair","title":"<code>get_server_key_cert_pair(session, *args, **kwargs)</code>","text":"<p>Get the previously requested PEM encoded value of the key/cert pair for a given server. Implementations will retrieveh the pair if it does not exist and will fetch its content if it does. If the content is not available, the assertion is triggered.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current deploy session</p> required Passthrough Args <p>See ITlsContext.get_server_key_cert_pair</p> <p>Returns:</p> Name Type Description <code>key_pem</code> <code>str</code> <p>str This is the pem-encoded key content (base64 encoded if encode is set)</p> <code>cert_pem</code> <code>str</code> <p>str This is the pem-encoded cert content (base64 encoded if encode is set)</p> Source code in <code>oper8/x/utils/tls_context/public.py</code> <pre><code>def get_server_key_cert_pair(\n    session: Session,\n    *args,\n    **kwargs,\n) -&gt; Tuple[str, str]:\n    \"\"\"Get the previously requested PEM encoded value of the key/cert pair for a\n    given server. Implementations will retrieveh the pair if it does not exist\n    and will fetch its content if it does. If the content is not available, the\n    assertion is triggered.\n\n    Args:\n        session:  Session\n            The current deploy session\n\n    Passthrough Args:\n        See ITlsContext.get_server_key_cert_pair\n\n    Returns:\n        key_pem:  str\n            This is the pem-encoded key content (base64 encoded if\n            encode is set)\n        cert_pem:  str\n            This is the pem-encoded cert content (base64 encoded\n            if encode is set)\n    \"\"\"\n    return get_tls_context(session).get_server_key_cert_pair(*args, **kwargs)\n</code></pre>"},{"location":"API%20References/#oper8.x.utils.tls_context.public.request_server_key_cert_pair","title":"<code>request_server_key_cert_pair(session, *args, **kwargs)</code>","text":"<p>Request creation of the PEM encoded value of the key/cert pair for a given server. This function has to be called from before render_chart is called. I.e., parse_config / Component constructor phase. Implementations of this function will generate the pair (in background) if it has not been already requested.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Session</code> <p>Session The current deploy session</p> required Passthrough Args <p>See ITlsContext.request_server_key_cert_pair</p> Source code in <code>oper8/x/utils/tls_context/public.py</code> <pre><code>def request_server_key_cert_pair(\n    session: Session,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Request creation of the PEM encoded value of the key/cert pair for a\n    given server. This function has to be called from before render_chart is\n    called. I.e., parse_config / Component constructor phase. Implementations of\n    this function will generate the pair (in background) if it has not been\n    already requested.\n\n    Args:\n        session:  Session\n            The current deploy session\n\n    Passthrough Args:\n        See ITlsContext.request_server_key_cert_pair\n    \"\"\"\n    return get_tls_context(session).request_server_key_cert_pair(*args, **kwargs)\n</code></pre>"},{"location":"architecture/","title":"Architecture","text":"<p>The oper8 architecture is focused on allowing users to implement exactly what their operator needs while making the operator logic \"just work\" behind the scenes.</p>"},{"location":"architecture/#definitions","title":"Definitions","text":"<ul> <li> <p><code>watch</code>: The <code>watch</code> operation is the binding of specific event handling logic to a <code>group/version/kind</code>.</p> </li> <li> <p><code>reconciliation</code>: The event handling in the <code>operator</code> pattern is referred to as a <code>reconciliation</code>. This is because the primary job of an <code>operator</code> is to reconcile the current state of the cluster against the desired state declared in the Custom Resource instance (<code>CR</code>).</p> </li> <li> <p><code>apply</code>: In order to manage dependent resources, an <code>operator</code> needs to make changes in the cluster. The <code>apply</code> operation takes an in-memory representation of an object and applies that change to the cluster.</p> </li> <li> <p><code>get</code>: An <code>operator</code> often needs to fetch the current state of a resource from the cluster. The <code>get</code> operation fetches this state at the current time.</p> </li> <li> <p><code>disable</code>: Depending on configuration values, an <code>operator</code> may need to ensure that a given resource does not exist in the cluster. The <code>disable</code> checks for a resource and deletes it if found.</p> </li> <li> <p><code>verify</code>: In order to understand the state of a given CR instance, an <code>operator</code> must be able to verify the current state of its managed resources. In <code>oper8</code>, this is done with type-specific logic for most standard kubernetes objects, and simply by checking for the object's presence for all other types.</p> </li> </ul>"},{"location":"architecture/#abstractions","title":"Abstractions","text":""},{"location":"architecture/#session","title":"<code>Session</code>","text":"<p>A <code>Session</code> contains the state of the current reconciliation (1 <code>Session</code> == 1 reconciliation). A <code>Session</code> is shared across all related objects which associate with the same reconciliation. For instance, users can implement a <code>controller</code> method which get some <code>component</code> status, and then use that information to modify other <code>component</code> by leveraging a <code>session</code>. <code>Session</code> also serves as a gateway to use <code>DeployManager</code> which <code>get</code> or <code>set</code> objects inside of the k8s cluster.</p>"},{"location":"architecture/#deploymanager","title":"<code>DeployManager</code>","text":"<p><code>DeployManager</code> module implements an abstraction around <code>apply</code>, <code>get</code> and <code>disable</code> operations (Similar to <code>kubectl apply</code>). It is accessible via <code>Session</code> in all user implementation.</p>"},{"location":"architecture/#watchmanager","title":"<code>WatchManager</code>","text":"<p>A <code>WatchManager</code> module implements an abstract interface for regstering <code>Controllers</code> as reconciliation event listeners via <code>watch</code> operation. In general, users do not need to interact with this abstraction outside of invoking <code>oper8</code> as a <code>__main__</code> module with <code>--dry_run</code>.</p>"},{"location":"architecture/#component","title":"<code>Component</code>","text":"<p>A <code>Component</code> is an atomic grouping of raw <code>kubernetes</code> resources that serves a single purpose. For instance, a standard microservice will consist of a <code>Deployment</code>, a <code>Secret</code> and a <code>Service</code>. These would all be grouped into a single <code>Component</code> as creating any of them in isolation would not make sense.</p>"},{"location":"architecture/#controller","title":"<code>Controller</code>","text":"<p>A <code>Controller</code> is the core object type that manges the mapping from a CR instance to the set of <code>Component</code>. A <code>Controller</code> is bound to exactly one CR by specifying <code>group/version/kind</code>, and define associated <code>Component(s)</code>. A <code>Controller</code> performs <code>reconciliation</code> when the corresponding <code>watch</code> event is triggered.</p>"},{"location":"architecture/#reconciliation-overview","title":"Reconciliation overview","text":"<p>The <code>reconcile</code> entry point is <code>Controller.run_reconcile()</code> which then triggers the following steps in high level:</p> <ol> <li>[Setup] Construct The Session: Set up the immutable <code>Session</code> object that will be passed through the rest of the rollout</li> <li>[Setup] Construct the session dependency DAG (Directed acyclic graph) based on components dependencies: User defined components described in <code>Controller.setup_components()</code> and <code>Session.add_component_dependency()</code> are in the end converted into DAG so that <code>oper8</code> can deploy them from upstream components.</li> <li>[Phase1] Run the <code>Component.deploy()</code> in DAG: In dependency order, invoke <code>deploy</code> on each <code>Component</code>, halting if any <code>Component</code> terminates with an unsuccessful state. This may be caused by an expected error (prerequisite resource not found in the cluster), or an unexpected error (unstable cluster).</li> <li>[Phase2] Run user defined <code>Controller.after_deploy()</code> or <code>Controller.after_deploy_unsuccessful()</code>: User-defined <code>Controllers</code> may define custom logic to run after the <code>deploy</code> DAG has completed successfully, but before the <code>verify</code> DAG runs. Note this is not one component level like <code>Component.deploy()</code>. This runs after all components deployments are finished.</li> <li>[Phase3] Run the <code>Component.verify()</code> in DAG: The deployed components from <code>Phase2</code> are verified per component in accordance with the dependency order of the DAG. User can customize verification logic for each component using the <code>Component.verify()</code> function.</li> <li>[Phase4] Run user defined <code>Controller.after_verify()</code> or <code>Controller.after_verify_unsuccessful()</code>: User-defined <code>Controllers</code> may define custom logic to run after all components verify DAG is completed. This is primarily useful for custom readiness checks that requires the entire application to be ready.</li> </ol>"},{"location":"architecture/#typical-development-flow","title":"Typical Development Flow","text":"<p>In short, a typical development flow with <code>oper8</code> are the following:</p> <p>First, conceptualize your CR and related k8s resources into oper8 abstractions.</p> <ul> <li>What is your main CR?</li> <li>What k8s resources your CR needs? How they can be grouped as <code>Component</code>?</li> <li>What are the dependency relationships between each <code>components</code>?</li> <li>What kind of reconciliation logic you need for the CR?</li> </ul> <p>Then, implement your application with <code>oper8</code>.</p> <p>Typically, user defines <code>Components</code> and a <code>Controller</code> for the target CR. Then use the <code>session</code> with <code>component</code> or <code>controller</code> to get/set k8s objects in the cluster and use it to customize <code>reconciliation</code> logic.</p> <p>The user rarely need to modify <code>WatchManager</code> or <code>DeployManager</code>.</p>"},{"location":"session_dependency_graph/","title":"Session Dependency Graph","text":"<p>Oper8 creates a session dependency graph based on component dependencies to determine the correct deployment order. This page explains how to log the generated graph at runtime and how to visualize it using the provided debug script during development. For the overall system architecture, please refer to the architecture page.</p>"},{"location":"session_dependency_graph/#log-session-dependency-graph","title":"Log session dependency graph","text":"<p>The debug log feature is available with v0.1.34 or higher.</p> <p>To log the session dependency graph, set the log level to <code>debug3</code> or <code>debug4</code>. Once enabled, the controller log will include a string representation of the session dependency graph.</p> <pre><code>2025-07-23T06:29:35.564320 [CTRLR:DBG3:xxx]     Session dependency DAG: Graph({compA:[],compB:[compA],compC:[compA],compD:[compC],compE:[compC,compF],compF:[compA]})\n</code></pre>"},{"location":"session_dependency_graph/#visualize-session-dependency-graph","title":"Visualize session dependency graph","text":"<p>You can also visualize the graph. First, save the session graph string into a text file (i.e. <code>scripts/graph/session_dependency_graph.txt</code>). Then run the following command to start a local server that renders the graph as an interactive HTML page at http://127.0.0.1:8050.</p> <p>If you would like to customize the visualization, refer to the <code>scripts/graph/create_session_dependency_graph.py</code> script.</p> <pre><code>\u276f tox -e graph -- --oper8-graph-path=scripts/graph/session_dependency_graph.txt\n...\n * Running on http://127.0.0.1:8050\n</code></pre> <p></p>"}]}